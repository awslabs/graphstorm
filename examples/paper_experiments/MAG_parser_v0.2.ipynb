{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('raw_data/mag_venues.txt', 'r')\n",
    "venues = set()\n",
    "for line in f.readlines():\n",
    "    venue = json.loads(line)\n",
    "    venues.add(venue['id'])\n",
    "print(f'There are {len(venues)} venues')\n",
    "venues = list(venues)\n",
    "venues.sort()\n",
    "df = pd.DataFrame({'id': venues})\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'processed_data/mag/mag_venue_id.parquet')\n",
    "print(\"venue data type:\", type(venues[0]))\n",
    "venues = set(venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('raw_data/mag_affiliations.txt', 'r')\n",
    "affs = list()\n",
    "aff_names = list()\n",
    "for line in f.readlines():\n",
    "    aff = json.loads(line)\n",
    "    affs.append(aff['id'])\n",
    "    aff_names.append(aff['NormalizedName'])\n",
    "print(f'There are {len(affs)} affiliations')\n",
    "df = pd.DataFrame({'id': affs, 'name': aff_names})\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'processed_data/mag/mag_affs.parquet')\n",
    "\n",
    "df = pd.DataFrame({'id': affs})\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'processed_data/mag/mag_aff_id.parquet')\n",
    "print('Afflication data type:', type(affs[0]))\n",
    "affs = set(affs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get affiliation of authors and save the author-affiliation pairs in the parquet files. Not all authors have known affiliations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_authors = []\n",
    "for i in range(5):\n",
    "    f = open(f'raw_data/mag_authors_{i}.txt', 'r')\n",
    "    authors = []\n",
    "    affs = []\n",
    "    num_authors = 0\n",
    "    for line in f.readlines():\n",
    "        num_authors += 1\n",
    "        author = json.loads(line)\n",
    "        full_authors.append(author['id'])\n",
    "        if 'last_known_aff_id' in author:\n",
    "            authors.append(author['id'])\n",
    "            affs.append(int(author['last_known_aff_id']))\n",
    "    df = pd.DataFrame({'author': authors, 'affiliation': affs})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'processed_data/mag/mag_author2aff_{i}.parquet')\n",
    "    print(f'There are {num_authors} authors and {len(authors)} of them have affiliations')\n",
    "    \n",
    "df = pd.DataFrame({'id': full_authors})\n",
    "print('author ID data type:', type(full_authors[0]))\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'processed_data/mag/mag_author_id.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paper_file(i):\n",
    "    f = open(f'raw_data/mag_papers_{i}.txt', 'r')\n",
    "    num_paper_venue = 0\n",
    "    venue_ids = set()\n",
    "    fos_set = set()\n",
    "\n",
    "    paper_ids = []\n",
    "    titles = []\n",
    "    years = []\n",
    "    paper2author = ([], [], [])\n",
    "    paper2venue = ([], [])\n",
    "    paper2fos = ([], [], [])\n",
    "    paper2paper = ([], [])\n",
    "    for line in f.readlines():\n",
    "        paper = json.loads(line)\n",
    "        if 'id' not in paper or 'title' not in paper or 'year' not in paper or 'authors' not in paper or len(paper['authors']) == 0:\n",
    "            continue\n",
    "        paper_ids.append(paper['id'])\n",
    "        titles.append(paper['title'])\n",
    "        years.append(paper['year'])\n",
    "        for order, author in enumerate(paper['authors']):\n",
    "            paper2author[0].append(paper['id'])\n",
    "            paper2author[1].append(author['id'])\n",
    "            paper2author[2].append(order)\n",
    "        if 'venue' in paper:\n",
    "            num_paper_venue += 1\n",
    "            venue = paper['venue']\n",
    "            if 'id' in venue:\n",
    "                venue_ids.add(venue['id'])\n",
    "                paper2venue[0].append(paper['id'])\n",
    "                paper2venue[1].append(venue['id'])\n",
    "                assert venue['id'] in venues\n",
    "        # If a paper has field of study.\n",
    "        if 'fos' in paper:\n",
    "            for fos in paper['fos']:\n",
    "                if fos['w'] > 0:\n",
    "                    fos_set.add(fos['name'])\n",
    "                    paper2fos[0].append(paper['id'])\n",
    "                    paper2fos[1].append(fos['name'])\n",
    "                    paper2fos[2].append(fos['w'])\n",
    "        if 'references' in paper:\n",
    "            for ref in paper['references']:\n",
    "                paper2paper[0].append(paper['id'])\n",
    "                paper2paper[1].append(ref)\n",
    "\n",
    "    df = pd.DataFrame({'paper': paper_ids, 'title': titles, 'year': years})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'processed_data/mag/mag_papers_{i}.parquet')\n",
    "    print(f'There are {len(paper_ids)} papers in file {i}', flush=True)\n",
    "\n",
    "    df = pd.DataFrame({'paper': paper2author[0], 'author': paper2author[1], 'order': paper2author[2]})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'processed_data/mag/mag_paper2author_{i}.parquet')\n",
    "    print(f'There are {len(paper2author[0])} paper-author pairs', flush=True)\n",
    "    \n",
    "    df = pd.DataFrame({'paper': paper2venue[0], 'venue': paper2venue[1]})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'processed_data/mag/mag_paper2venue_{i}.parquet')\n",
    "    print(f'There are {len(paper2venue[0])} paper-venue pairs', flush=True)\n",
    "    \n",
    "    df = pd.DataFrame({'paper': paper2fos[0], 'fos': paper2fos[1], 'w': paper2fos[2]})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'processed_data/mag/mag_paper2fos_{i}.parquet')\n",
    "    print(f'There are {len(paper2fos[0])} paper-fos pairs', flush=True)\n",
    "\n",
    "    df = pd.DataFrame({'src_paper': paper2paper[0], 'dst_paper': paper2paper[1]})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'processed_data/mag/mag_paper2paper_{i}.parquet')\n",
    "    print(f'There are {len(paper2paper[0])} paper citations', flush=True)\n",
    "    \n",
    "    return num_paper_venue, venue_ids, fos_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphstorm.gconstruct.utils import multiprocessing_data_read\n",
    "data = multiprocessing_data_read([i for i in range(51)], num_processes=8, user_parser=parse_paper_file)\n",
    "\n",
    "num_paper_venue = 0\n",
    "venue_ids = set()\n",
    "fos_set = set()\n",
    "for i in data:\n",
    "    num_paper_venue += data[i][0]\n",
    "    venue_ids.update(data[i][1])\n",
    "    fos_set.update(data[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {num_paper_venue} papers that have venues.')\n",
    "print(f'There are {len(venue_ids)} venues with IDs')\n",
    "print(f'There are {len(fos_set)} topic fields')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id': np.array(list(fos_set))})\n",
    "table = pa.Table.from_pandas(df)\n",
    "print('fos data type:', type(list(fos_set)[0]))\n",
    "pq.write_table(table, 'processed_data/mag/mag_fos.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates venues as labels of paper nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_papers = []\n",
    "venues = []\n",
    "for i in range(51):\n",
    "    table = pd.read_parquet(f'processed_data/mag/mag_paper2venue_{i}.parquet')\n",
    "    v_papers.append(table['paper'].to_numpy())\n",
    "    venues.append(table['venue'].to_numpy())\n",
    "v_papers = np.concatenate(v_papers)\n",
    "venues = np.concatenate(venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all unique venues and only keep the venues that have a large number publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_venues, v_cnts = np.unique(venues, return_counts=True)\n",
    "common_venue_map = {venue: i for i, venue in enumerate(uniq_venues[v_cnts > 10000].tolist())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only collect the papers published in the popular venues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_papers1 = []\n",
    "venues1 = []\n",
    "for pid, v in zip(v_papers, venues):\n",
    "    if v in common_venue_map:\n",
    "        v_papers1.append(pid)\n",
    "        venues1.append(common_venue_map[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2v_map = {paper: venue for paper, venue in zip(v_papers1, venues1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the paper node files with venues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_labels = 0\n",
    "for i in range(51):\n",
    "    table = pd.read_parquet(f'processed_data/mag/mag_papers_{i}.parquet')\n",
    "    venues = []\n",
    "    for paper in table['paper'].to_numpy():\n",
    "        if paper in p2v_map:\n",
    "            venues.append(p2v_map[paper])\n",
    "            num_labels += 1\n",
    "        else:\n",
    "            venues.append(math.nan)\n",
    "    df = pd.DataFrame({'paper': table['paper'], 'title': table['title'],\n",
    "                       'year': table['year'], 'venue': np.array(venues)})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'processed_data/mag/mag_papers_with_labels_{i}.parquet')\n",
    "    print(f'There are {len(venues)} papers in file {i}', flush=True)\n",
    "    print(f'There are {num_labels} labels so far.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
