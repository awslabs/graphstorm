{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from graphstorm.gconstruct.file_io import read_data_parquet, write_data_hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel, BertConfig\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to generate tokens of text data and compute their BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tokens(strs, tokenizer, max_seq_len):\n",
    "    tokens = []\n",
    "    att_masks = []\n",
    "    type_ids = []\n",
    "    for s in strs:\n",
    "        t = tokenizer(s, max_length=max_seq_len,\n",
    "                      truncation=True, padding='max_length', return_tensors='pt')\n",
    "        tokens.append(t['input_ids'])\n",
    "        att_masks.append(t['attention_mask'])\n",
    "        type_ids.append(t['token_type_ids'])\n",
    "    tokens = th.cat(tokens, dim=0)\n",
    "    att_masks = th.cat(att_masks, dim=0)\n",
    "    type_ids = th.cat(type_ids, dim=0)\n",
    "    return tokens, att_masks, type_ids\n",
    "    \n",
    "def compute_bert_embed(tokens, att_masks, type_ids, lm_model, device, bert_batch_size):\n",
    "    lm_model.eval()\n",
    "    out_embeds = []\n",
    "    lm_model = lm_model.to(device)\n",
    "    with th.no_grad():\n",
    "        tokens_list = th.split(tokens, bert_batch_size)\n",
    "        att_masks_list = th.split(att_masks, bert_batch_size)\n",
    "        token_types_list = th.split(type_ids, bert_batch_size)\n",
    "        for tokens, att_masks, token_types in zip(tokens_list, att_masks_list, token_types_list):\n",
    "            outputs = lm_model(tokens.to(device),\n",
    "                               attention_mask=att_masks.to(device),\n",
    "                               token_type_ids=token_types.to(device))\n",
    "            out_embeds.append(outputs.pooler_output.cpu())\n",
    "        out_embeds = th.cat(out_embeds)\n",
    "    return out_embeds.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the paper nodes and compute their BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_papers(i):\n",
    "    print(f'process file {i}')\n",
    "    papers = read_data_parquet(f'mag_papers_{i}.parquet')\n",
    "    max_seq_len = 128\n",
    "    bert_model = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "    config = BertConfig.from_pretrained(bert_model)\n",
    "    lm_model = BertModel.from_pretrained(bert_model, config=config)\n",
    "    \n",
    "    tokens = compute_tokens(papers['title'], tokenizer, max_seq_len)\n",
    "    gpu = int(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    device = f\"cuda:{gpu}\"\n",
    "    embeds = compute_bert_embed(tokens[0], tokens[1], tokens[2], lm_model, device, 1024)\n",
    "    res = {}\n",
    "    res['paper'] = papers['paper']\n",
    "    res['feat'] = embeds\n",
    "    res['year'] = papers['year']\n",
    "    write_data_hdf5(res, f'mag_papers_bert_{i}.hdf5')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphstorm.gconstruct.utils import multiprocessing_data_read\n",
    "data = multiprocessing_data_read([i for i in range(51)], num_processes=8, user_parser=process_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the fos nodes and compute their BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fos():\n",
    "    fos = read_data_parquet('mag_fos.parquet')\n",
    "    max_seq_len = 16\n",
    "    bert_model = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "    config = BertConfig.from_pretrained(bert_model)\n",
    "    lm_model = BertModel.from_pretrained(bert_model, config=config)\n",
    "    \n",
    "    tokens = compute_tokens(fos['id'], tokenizer, max_seq_len)\n",
    "    device = \"cuda:0\"\n",
    "    embeds = compute_bert_embed(tokens[0], tokens[1], tokens[2], lm_model, device, 1024)\n",
    "    res = {}\n",
    "    res['id'] = fos['id']\n",
    "    res['feat'] = embeds\n",
    "    write_data_hdf5(res, 'mag_fos_bert.hdf5')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_fos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
