{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('mag_venues.txt', 'r')\n",
    "venues = set()\n",
    "for line in f.readlines():\n",
    "    venue = json.loads(line)\n",
    "    venues.add(venue['id'])\n",
    "print(f'There are {len(venues)} venues')\n",
    "venues = list(venues)\n",
    "venues.sort()\n",
    "df = pd.DataFrame({'id': venues})\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'mag_venue_id.parquet')\n",
    "print(\"venue data type:\", type(venues[0]))\n",
    "venues = set(venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('mag_affiliations.txt', 'r')\n",
    "affs = list()\n",
    "aff_names = list()\n",
    "for line in f.readlines():\n",
    "    aff = json.loads(line)\n",
    "    affs.append(aff['id'])\n",
    "    aff_names.append(aff['NormalizedName'])\n",
    "print(f'There are {len(affs)} affiliations')\n",
    "df = pd.DataFrame({'id': affs, 'name': aff_names})\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'mag_affs.parquet')\n",
    "\n",
    "df = pd.DataFrame({'id': affs})\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'mag_aff_id.parquet')\n",
    "print('Afflication data type:', type(affs[0]))\n",
    "affs = set(affs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get affiliation of authors and save the author-affiliation pairs in the parquet files. Not all authors have known affiliations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_authors = []\n",
    "for i in range(5):\n",
    "    f = open(f'mag_authors_{i}.txt', 'r')\n",
    "    authors = []\n",
    "    affs = []\n",
    "    num_authors = 0\n",
    "    for line in f.readlines():\n",
    "        num_authors += 1\n",
    "        author = json.loads(line)\n",
    "        full_authors.append(author['id'])\n",
    "        if 'last_known_aff_id' in author:\n",
    "            authors.append(author['id'])\n",
    "            affs.append(int(author['last_known_aff_id']))\n",
    "            #assert author['last_known_aff_id'] in affs, f\"{author['last_known_aff_id']} does not exist\"\n",
    "            #assert author['id'] not in author2aff\n",
    "    df = pd.DataFrame({'author': authors, 'affiliation': affs})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'mag_author2aff_{i}.parquet')\n",
    "    print(f'There are {num_authors} authors and {len(authors)} of them have affiliations')\n",
    "    \n",
    "df = pd.DataFrame({'id': full_authors})\n",
    "print('author ID data type:', type(full_authors[0]))\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'mag_author_id.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paper_file(i):\n",
    "    f = open(f'mag_papers_{i}.txt', 'r')\n",
    "    num_paper_venue = 0\n",
    "    venue_ids = set()\n",
    "    fos_set = set()\n",
    "\n",
    "    paper_ids = []\n",
    "    titles = []\n",
    "    years = []\n",
    "    paper2author = ([], [], [])\n",
    "    paper2venue = ([], [])\n",
    "    paper2fos = ([], [], [])\n",
    "    paper2paper = ([], [])\n",
    "    for line in f.readlines():\n",
    "        paper = json.loads(line)\n",
    "        if 'id' not in paper or 'title' not in paper or 'year' not in paper or 'authors' not in paper or len(paper['authors']) == 0:\n",
    "            continue\n",
    "        paper_ids.append(paper['id'])\n",
    "        titles.append(paper['title'])\n",
    "        years.append(paper['year'])\n",
    "        for order, author in enumerate(paper['authors']):\n",
    "            paper2author[0].append(paper['id'])\n",
    "            paper2author[1].append(author['id'])\n",
    "            paper2author[2].append(order)\n",
    "        #if len(paper['authors']) == 0:\n",
    "        #    print(paper)\n",
    "        if 'venue' in paper:\n",
    "            num_paper_venue += 1\n",
    "            venue = paper['venue']\n",
    "            if 'id' in venue:\n",
    "                venue_ids.add(venue['id'])\n",
    "                paper2venue[0].append(paper['id'])\n",
    "                paper2venue[1].append(venue['id'])\n",
    "                assert venue['id'] in venues\n",
    "        if 'fos' in paper:\n",
    "            for fos in paper['fos']:\n",
    "                fos_set.add(fos['name'])\n",
    "                paper2fos[0].append(paper['id'])\n",
    "                paper2fos[1].append(fos['name'])\n",
    "                paper2fos[2].append(fos['w'])\n",
    "        if 'references' in paper:\n",
    "            for ref in paper['references']:\n",
    "                paper2paper[0].append(paper['id'])\n",
    "                paper2paper[1].append(ref)\n",
    "\n",
    "    df = pd.DataFrame({'paper': paper_ids, 'title': titles, 'year': years})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'mag_papers_{i}.parquet')\n",
    "    print(f'There are {len(paper_ids)} papers in file {i}', flush=True)\n",
    "\n",
    "    df = pd.DataFrame({'paper': paper2author[0], 'author': paper2author[1], 'order': paper2author[2]})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'mag_paper2author_{i}.parquet')\n",
    "    print(f'There are {len(paper2author[0])} paper-author pairs', flush=True)\n",
    "    \n",
    "    df = pd.DataFrame({'paper': paper2venue[0], 'venue': paper2venue[1]})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'mag_paper2venue_{i}.parquet')\n",
    "    print(f'There are {len(paper2venue[0])} paper-venue pairs', flush=True)\n",
    "    \n",
    "    df = pd.DataFrame({'paper': paper2fos[0], 'fos': paper2fos[1], 'w': paper2fos[2]})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'mag_paper2fos_{i}.parquet')\n",
    "    print(f'There are {len(paper2fos[0])} paper-fos pairs', flush=True)\n",
    "\n",
    "    df = pd.DataFrame({'src_paper': paper2paper[0], 'dst_paper': paper2paper[1]})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'mag_paper2paper_{i}.parquet')\n",
    "    print(f'There are {len(paper2paper[0])} paper citations', flush=True)\n",
    "    \n",
    "    return num_paper_venue, venue_ids, fos_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "import queue\n",
    "import gc\n",
    "\n",
    "def worker_fn(task_queue, res_queue, user_parser):\n",
    "    \"\"\" The worker function in the worker pool\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    task_queue : Queue\n",
    "        The queue that contains all tasks\n",
    "    res_queue : Queue\n",
    "        The queue that contains the processed data. This is used for\n",
    "        communication between the worker processes and the master process.\n",
    "    user_parser : callable\n",
    "        The user-defined function to read and process the data files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            # If the queue is empty, it will raise the Empty exception.\n",
    "            i, in_file = task_queue.get_nowait()\n",
    "            data = user_parser(in_file)\n",
    "            res_queue.put((i, data))\n",
    "            gc.collect()\n",
    "    except queue.Empty:\n",
    "        pass\n",
    "    \n",
    "class WorkerPool:\n",
    "    def __init__(self, name, in_files, num_processes, user_parser):\n",
    "        self.name = name\n",
    "        self.processes = []\n",
    "        manager = multiprocessing.Manager()\n",
    "        self.task_queue = manager.Queue()\n",
    "        self.res_queue = manager.Queue(8)\n",
    "        self.num_files = len(in_files)\n",
    "        for i, in_file in enumerate(in_files):\n",
    "            self.task_queue.put((i, in_file))\n",
    "        for _ in range(num_processes):\n",
    "            proc = Process(target=worker_fn, args=(self.task_queue, self.res_queue, user_parser))\n",
    "            proc.start()\n",
    "            self.processes.append(proc)\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\" Get the processed data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a dict : key is the file index, the value is processed data.\n",
    "        \"\"\"\n",
    "        return_dict = {}\n",
    "        while len(return_dict) < self.num_files:\n",
    "            file_idx, vals= self.res_queue.get()\n",
    "            return_dict[file_idx] = vals\n",
    "            gc.collect()\n",
    "        return return_dict\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\" Stop the process pool.\n",
    "        \"\"\"\n",
    "        for proc in self.processes:\n",
    "            proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = WorkerPool(\"mag\", [i for i in range(51)], num_processes=8, user_parser=parse_paper_file)\n",
    "data = pool.get_data()\n",
    "pool.close()\n",
    "\n",
    "num_paper_venue = 0\n",
    "venue_ids = set()\n",
    "fos_set = set()\n",
    "for i in data:\n",
    "    num_paper_venue += data[i][0]\n",
    "    venue_ids.update(data[i][1])\n",
    "    fos_set.update(data[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {num_paper_venue} papers that have venues.')\n",
    "print(f'There are {len(venue_ids)} venues with IDs')\n",
    "print(f'There are {len(fos_set)} topic fields')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id': np.array(list(fos_set))})\n",
    "table = pa.Table.from_pandas(df)\n",
    "print('fos data type:', type(list(fos_set)[0]))\n",
    "pq.write_table(table, 'mag_fos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
