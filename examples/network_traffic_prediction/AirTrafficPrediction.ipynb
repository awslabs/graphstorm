{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782e11df-01f7-44ed-96dc-a3c6e0c34d56",
   "metadata": {},
   "source": [
    "# Air Transportation Network Prediction Using GraphStorm\n",
    "\n",
    "This notebook provides end-to-end training and inference pipelines for the air transportation network prediction whose data are generated using the `Synthetic_Airport_Traffic_wAirlines.ipynb` and `Air_Traffic_Data_Exp&Gc.ipynb` notebooks. Please run them first to create the synthetic data and store it at the `./gs_1p` folder.\n",
    "\n",
    "For this synthetic air transportation network data, we define the prediction task as a node time series regression, i.e., predicting the total inventory amount.\n",
    "\n",
    "Users may find that the end-to-end training/inference pipelines in this notebook are similar as the [Notebook 1: Use GraphStorm APIs for Building a Node Classification Pipeline](https://graphstorm.readthedocs.io/en/latest/api/notebooks/Notebook_1_NC_Pipeline.html). This notebook, however, has three major differences from the [Notebook 1](https://graphstorm.readthedocs.io/en/latest/api/notebooks/Notebook_1_NC_Pipeline.html) API example, including:\n",
    "\n",
    "1. A customized RGCN model that can use sliding window method to process time series data for the regression task;\n",
    "2. A customized node trainer class that can handle the specific evaluation method for time series data;\n",
    "3. A customized node inferrer class that can handle the autoagressive sliding window method during model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b109b38-24d1-4e45-af3a-0d6f22a2dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup log level in Jupyter Notebook to show running information\n",
    "import logging\n",
    "logging.basicConfig(level=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab68a-8026-4d8b-8aae-de85c27db2fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 0. Initialize the GraphStorm Standalone Environment\n",
    "\n",
    "The first step to call `gs.initialize()` for using GraphStorm Standalone mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd68b1-317d-49b8-8809-596bbd956009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the total days, should be same as data generation\n",
    "NUM_DEMAND_DAYS = 31\n",
    "\n",
    "# initialization\n",
    "import graphstorm as gs\n",
    "gs.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ed0f4-93e1-4789-bef7-d8bd9b5cbf51",
   "metadata": {},
   "source": [
    "### 1. Setup GraphStorm Dataset and DataLoaders\n",
    "\n",
    "Similar as PyTorch model training pipeline, we create a dataset by constructing `gs.dataset.GSgnnData` class. In most cases, users only need to provide the location of the graph description JSON file, which is created in GraphStorm's gconstruct operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03475a99-d20b-4c76-b9ba-eb3f1d7d7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GraphStorm Dataset for the air transportation graph data\n",
    "from graphstorm.dataloading import GSgnnData\n",
    "\n",
    "ml_data = GSgnnData(part_config='./gs_1p/air_traffic.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0bb7e-a16e-4545-92c0-557872e5be5f",
   "metadata": {},
   "source": [
    "Then, we create different `DataLoader`s for training, validation, and testing. As shown below, we allow users to specify different `DataLoader` settings, e.g., `fanout`, `batch_size`, except for a few model-related properties, such as `node_feats`, `edge_feats` and `label_field`.\n",
    "\n",
    "GNN models may only use parts of graph features, therefore, GraphStorm `DataLoader`s allow users to specifies the `node_feats` and `edge_feats` in a format of a dictionary of lists of strings. Keys of the dictionary are node/edge type names, while values are lists of feature name strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e4fa77-a744-4632-91c2-89191a51c74d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define dataloaders for training, validation, and testing\n",
    "nfeats_4_modeling = {'airport':['latitude','longitude', 'inventory_amounts']}\n",
    "efeats_4_modeling = {('airport', 'demand', 'airport'): ['demands'], \\\n",
    "                     ('airport', 'traffic', 'airport'): ['capacity', 'traffics']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8548b-d52f-41ef-8efb-e48a16957c96",
   "metadata": {},
   "source": [
    "In our synthetic air transportation network, we have both node features and edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34593f0d-ead1-4261-b4d7-3ea1be16096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphstorm.dataloading import GSgnnNodeDataLoader\n",
    "\n",
    "fanout=[10, 10]\n",
    "train_dataloader = GSgnnNodeDataLoader(\n",
    "    dataset=ml_data,\n",
    "    target_idx=ml_data.get_node_train_set(ntypes=['airport']),\n",
    "    node_feats=nfeats_4_modeling,\n",
    "    edge_feats=efeats_4_modeling,\n",
    "    label_field='inventory_amounts',\n",
    "    fanout=fanout,\n",
    "    batch_size=64,\n",
    "    train_task=True)\n",
    "val_dataloader = GSgnnNodeDataLoader(\n",
    "    dataset=ml_data,\n",
    "    target_idx=ml_data.get_node_val_set(ntypes=['airport']),\n",
    "    node_feats=nfeats_4_modeling,\n",
    "    edge_feats=efeats_4_modeling,\n",
    "    label_field='inventory_amounts',\n",
    "    fanout=fanout,\n",
    "    batch_size=64,\n",
    "    train_task=False)\n",
    "test_dataloader = GSgnnNodeDataLoader(\n",
    "    dataset=ml_data,\n",
    "    target_idx=ml_data.get_node_test_set(ntypes=['airport']),\n",
    "    node_feats=nfeats_4_modeling,\n",
    "    edge_feats=efeats_4_modeling,\n",
    "    label_field='inventory_amounts',\n",
    "    fanout=fanout,\n",
    "    batch_size=64,\n",
    "    train_task=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776cd5af-7c0a-4021-8921-23d70d67b488",
   "metadata": {},
   "source": [
    "### 2. Create a Customized GraphStorm-compatible RGCN Model for the Time Series Node Regression\n",
    "\n",
    "GraphStorm has a set of GNN component modules that could be freely combined for different tasks. This notebook depends on an RGCN model, `RgcnNRModel4TS`, that extends from the `GSgnnModel` and implements the sliding window method in its `forward()` and `predict()` functions for node regression by using GaphStorm APIs. Users can find the details of the `RgcnNRModel4TS` in the `nr_models.py` file.\n",
    "\n",
    "Because our synthetic air transportation network have both static features and time series features, the customized `RgcnNRModel4TS` need to know which features are static or time series. So the `RgcnNRModel4TS` class asks this information in its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba68c6f-d419-4f39-a3c4-340b256a83c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nr_models import RgcnNRModel4TS\n",
    "\n",
    "# initialize the model with specific time series related arugments\n",
    "ts_nfeat_names = {'airport':['inventory_amounts']}\n",
    "ts_efeat_names = {('airport', 'demand', 'airport'): ['demands'], \\\n",
    "                  ('airport', 'traffic', 'airport'): ['traffics']}\n",
    "\n",
    "# import a customized RGCN model for node time series regression\n",
    "model = RgcnNRModel4TS(g=ml_data.g,\n",
    "                       num_hid_layers=len(fanout),\n",
    "                       node_feat_field=nfeats_4_modeling,\n",
    "                       edge_feat_field=efeats_4_modeling,\n",
    "                       edge_feat_mp_op='add',\n",
    "                       target_ntype='airport',\n",
    "                       ts_nfeat_names=ts_nfeat_names,\n",
    "                       ts_efeat_names=ts_efeat_names,\n",
    "                       hid_size=128,\n",
    "                       ts_size=NUM_DEMAND_DAYS,\n",
    "                       window_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50883e00-ae86-4773-aa81-935fba9e72b6",
   "metadata": {},
   "source": [
    "### 3. Setup a GraphStorm Evaluator\n",
    "\n",
    "To check the performance during model training, GraphStorm relies on a set of built-in `Evaluator`s for different tasks. Here we use a built-in `graphstorm.eval.GSgnnRegressionEvaluator` for the node time series regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d5fca3-f026-460f-97b4-ed63427b1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a regression evaluator for the trainer\n",
    "from graphstorm.eval import GSgnnRegressionEvaluator\n",
    "\n",
    "evaluator = GSgnnRegressionEvaluator(eval_frequency=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb84ae-baff-4d2b-909d-743bb8b9b8b5",
   "metadata": {},
   "source": [
    "### 4. Setup a Trainer and Training\n",
    "\n",
    "For training loop, GraphStorm has different `Trainer`s for specific tasks. GraphStorm's `GSgnnNodePredictionTrainer` is designed to orchestrate dataloaders, models, and evaluators, but cannot handle the sliding window method. So here we extends the `GSgnnNodePredictionTrainer`, replacing its default `node_mini_batch_gnn_predict()` method with a customized method that supports sliding window for model training and validation, and autoregressive sliding window for testing. Users can find the details in the `nr_models.py` file.\n",
    "\n",
    "Then we use the custom trainer's `fit()` function to train our model. During training, users can check the output log, finding training loss, evaluation metrics, and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ce33e-097a-4256-83b4-928bb8f1a856",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nr_models import NodePredictionTrainer4TS\n",
    "\n",
    "# define a trainer, which is our own custom trainer\n",
    "trainer = NodePredictionTrainer4TS(model)\n",
    "\n",
    "trainer.setup_evaluator(evaluator)\n",
    "trainer.setup_device(gs.utils.get_device())\n",
    "\n",
    "# Train the model with the trainer using fit() function\n",
    "trainer.fit(train_loader=train_dataloader,\n",
    "            val_loader=val_dataloader,\n",
    "            test_loader=test_dataloader,\n",
    "            num_epochs=200,\n",
    "            save_model_frequency=1000,\n",
    "            save_model_path='./models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46329dac-dbf3-46ce-b900-4c34973772f5",
   "metadata": {},
   "source": [
    "### 5. Visualize Model Performance History\n",
    "\n",
    "Besides the log, we can examine the model performance on the validation and testing sets by visualizing evalutors' history properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e77bd-9528-43fd-9d66-951332c5ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extract evaluation history of metrics from the evaluator's history property\n",
    "val_metrics, test_metrics = [], []\n",
    "for val_metric, test_metric in trainer.evaluator.history:\n",
    "    val_metrics.append(val_metric['rmse'])\n",
    "    test_metrics.append(test_metric['rmse'])\n",
    "\n",
    "# plot the performance curves\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(val_metrics, label='val rmse')\n",
    "ax.plot(test_metrics, label='test rmse')\n",
    "ax.set(xlabel='Epochs', ylabel='Evaluation RMSE')\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd440b9-f332-4111-9e05-6b1523f8776c",
   "metadata": {},
   "source": [
    "### 6. Inference with a Trained Model\n",
    "\n",
    "GraphStorm automatically saves the best performed model(s) according to the validation values. The path to the best model can be acquired through a trainer's `get_best_model_path` function. We can first find out which checkpoint is the best model and its path. And then restore it by using model's `restore_model()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0e174-16e7-4473-aed3-b96eb6b3d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training, the best model is saved to disk\n",
    "best_model_path = trainer.get_best_model_path()\n",
    "print('Best model path:', best_model_path)\n",
    "\n",
    "# we can restore the model from the saved path using restore_model() function.\n",
    "model.restore_model(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebe0fb-385a-46e7-a171-e280d9e59a10",
   "metadata": {},
   "source": [
    "To do inference, users can either create a new dataloader as the following code does, or reuse one of the dataloaders defined in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e7a2b-5d89-4474-9e7b-6e2a646a0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloader for inference\n",
    "print(f'========================== Do Inference ================================')\n",
    "infer_dataloader = GSgnnNodeDataLoader(\n",
    "    dataset=ml_data,\n",
    "    target_idx=ml_data.get_node_infer_set(ntypes='airport', mask=None),\n",
    "    node_feats=nfeats_4_modeling,\n",
    "    edge_feats=efeats_4_modeling,\n",
    "    label_field='inventory_amounts',\n",
    "    fanout=fanout,\n",
    "    batch_size=64,\n",
    "    train_task=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f271746-c51e-4138-8543-b69b8fbc34bd",
   "metadata": {},
   "source": [
    "GraphStorm provides a set of `Inferrer`s that can perform highly efficient inference for very large graphs. However, GraphStorm's `GSgnnNodePredictionInferrer` cannot handle the sliding window mehtod either. Similarly we extends the `GSgnnNodePredictionInferrer` to specifically implement the autoregressive sliding window method by replacing true time serie values with the predicted ones, and then call the same customized `node_mini_batch_gnn_predict()` method during inference. Users can find the details in the `nr_models.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "270dc5b7-8cb0-4515-abc1-5c2bbca36836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nr_models import NodePredictionInferrer4TS\n",
    "\n",
    "# Run inference on the inference dataset\n",
    "infer = NodePredictionInferrer4TS(model)\n",
    "\n",
    "infer.infer(infer_dataloader,\n",
    "            use_mini_batch_infer=True,\n",
    "            save_embed_path=None,\n",
    "            init_step=0,\n",
    "            infer_steps=7,\n",
    "            node_id_mapping_file='./gs_1p/node_mapping.pt',\n",
    "            save_prediction_path='./predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252cfb9-ac19-463d-86e0-ba24347ab123",
   "metadata": {},
   "source": [
    "Now let's check the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af92cd-dfcb-4ae7-b29d-fe536e1f6ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The prediction results on the inference graph are saved to the folder named after the target_ntype\n",
    "!ls -lh ./predictions/airport/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa9283a-f450-4a70-890b-bae33f2f7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733415bc-640d-4a68-8013-a6015d34f7a4",
   "metadata": {},
   "source": [
    "#### Prediction results exploration\n",
    "\n",
    "The prediction results are two Pytorch tensor files. One is the actual prediction values, and another is the GraphStorm node ID file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb244f-5b17-4f1d-96c2-43afd6f2ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_th = th.load('./predictions/airport/predict-00000.pt')\n",
    "pred_th.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff9ba4-4b6b-4615-a15e-3c68bbc92759",
   "metadata": {},
   "source": [
    "The `predict-00000.pt` file stores the prediction results, which are in a $N \\times 7 $ tensor, where $N$ is the number of all airports, and $7$ is the total steps to predict.\n",
    "\n",
    "As we set the `init_step=0` and `infer_steps=7` in the above `infer()` function, it will start from the step 0 and use values defined in `window_size`, i.e. from step 0 to step 6, to predict the 7th step values, and then repeat this inference operation 7 times to obtain the prediction values from step 7 to step 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b37cb-52df-4915-b3e4-feea89521a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nid_th = th.load('./predictions/airport/predict_nids-00000.pt')\n",
    "pred_nid_th.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7bb31-fd53-493e-93cc-c1cc83afbcd7",
   "metadata": {},
   "source": [
    "The `predict_nids-00000.pt` file stores the GraphStorm node IDs, ranging from 0 to the total number of nodes minus 1. The ID tensor has the same order as the predictions, therefore can be used for raw ID mapping.\n",
    "\n",
    "----\n",
    "#### Node ID mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbdd3b-a68b-47e7-8fd4-0b153b80a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_nid_mapping = pd.read_parquet('./gs_1p/raw_id_mappings/airport/part-00000.parquet')\n",
    "raw_nid_mapping.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6683c26b-a7ea-4f87-b426-ee4a662cc87b",
   "metadata": {},
   "source": [
    "The synthetic airport node table use airport iata codes as node IDs. GraphStorm graph construction command convert them into the integer-based IDs. So we can use all three files to remap the prediction values to the original iata codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dfa4466-c314-488f-ab39-eaee748de0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "    'new': pred_nid_th.numpy(),\n",
    "    'prediction': pred_th.numpy().tolist()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac8a20-b0ed-4c5d-9ad3-910228d9cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_df = pd.merge(raw_nid_mapping, pred_df, on='new')\n",
    "final_pred_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da7d78-20be-4d9d-9f67-8042ec88061d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
