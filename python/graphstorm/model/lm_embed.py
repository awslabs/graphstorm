"""
    Copyright 2023 Contributors

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

    Embedding layer with Language model
    Some node features are generated by a language model, e.g., BERT
"""

import torch as th
import dgl

from .embed import GSNodeInputLayer
from .embed import GSNodeEncoderInputLayer
from .lm_model import init_lm_model
from .lm_model import get_lm_node_feats
from ..utils import get_rank

def update_bert_cache(g, lm_models_info, lm_models, lm_emb_cache, lm_infer_batch_size):
    """ Update the lm_emb_cache using lanaguage models.

    Parameters
    ----------
    lm_models_info: dict
        Language model information
    lm_models: dict
        Language models
    lm_emb_cache: dict
        Language model embedding cache
    lm_infer_batch_size: int
        Language model inference batch size
    """
    for (lm_ntypes, lm_node_feats), lm_model \
        in zip(lm_models_info, lm_models):
        lm_model.eval()
        for ntype in lm_ntypes:
            if get_rank() == 0:
                print('compute bert embedding on node {}'.format(ntype))
            hidden_size = lm_model.feat_size
            if 'bert_emb' not in g.nodes[ntype].data:
                g.nodes[ntype].data['bert_emb'] = \
                    dgl.distributed.DistTensor(
                        (g.number_of_nodes(ntype), hidden_size),
                        name="bert_emb",
                        dtype=th.float32,
                        part_policy=g.get_node_partition_policy(ntype),
                        persistent=True)
            input_emb = g.nodes[ntype].data['bert_emb']
            infer_nodes = dgl.distributed.node_split(
                th.ones((g.number_of_nodes(ntype),), dtype=th.bool),
                partition_book=g.get_partition_book(),
                ntype=ntype, force_even=False)

            node_list = th.split(infer_nodes, lm_infer_batch_size)
            input_ntypes = [ntype]
            for _, input_nodes in enumerate(node_list):
                input_lm_feats = {}
                input_lm_feats[ntype] = {
                    fname: feat[input_nodes] \
                        for fname, feat in lm_node_feats[ntype].items()
                }
                text_embs = lm_model(input_ntypes, input_lm_feats)
                input_emb[input_nodes] = text_embs[ntype].to('cpu')
            th.distributed.barrier()
            lm_emb_cache[ntype] = input_emb
        lm_model.train()

def lm_model_forward(input_nodes, lm_emb_cache, lm_models, lm_models_info, use_cache=False):
    """ Do language model forward pass on input_nodes

    Parameters
    ----------
    input_nodes: dict
        Input nodes for different node types
    lm_emb_cache: dict
        Language model embedding cache for different node types
    lm_models: list
        Language models for different node types.
        lm_models stores a list of different language models.
    lm_models_info: list
        Language model information for different node types.
        lm_models_info stores a list of (lm_ntypes, lm_node_feats) tuples.
        There is a one-one correspondence between the language models stored in
        lm_models and the (lm_ntypes, lm_node_feats) tuples in lm_models_info.
        The lm_ntypes stores the node types that share the same language model
        and the lm_node_feats stores the corresponding language model computation
        related node features (i.e., token ids, attention masks, etc.)
    use_cache: bool
        If true, use the embeddings in lm_emb_cache
    """
    lm_feats = {}

    # Get the device from lm_models
    # The cached BERT embedding should be moved to the same device
    # as lm_models.
    dev = next(lm_models[0].parameters()).device
    if use_cache:
        # No bert training, Get cached LM embedding
        # Note: self.lm_emb_cache is initialized by calling warmup
        for ntype, idx in input_nodes.items():
            if ntype in lm_emb_cache:
                lm_feats[ntype] = lm_emb_cache[ntype][idx].to(dev)
    else:
        # TODO: Release the bert cache properly
        #       This may need support from DistDGL
        # Need bert training
        for (lm_ntypes, lm_node_feats), lm_model \
            in zip(lm_models_info, lm_models):
            input_ntypes = []
            input_lm_feats = {}
            for ntype in lm_ntypes:
                if ntype in input_nodes:
                    input_ntypes.append(ntype)
                    input_lm_feats[ntype] = {
                        fname: feat[input_nodes[ntype]].to(dev) \
                            for fname, feat in lm_node_feats[ntype].items()
                    }

            if len(input_ntypes) > 0:
                lm_feats.update(lm_model(input_ntypes, input_lm_feats))
    return lm_feats

class GSPureLMNodeInputLayer(GSNodeInputLayer):
    """The input embedding layer with language model only for all nodes in a
    heterogeneous graph.

    The input layer only has the language model layer and each node type should
    have text feature. The output dimension will be the same as the output
    dimension of the language model.

    Use GSLMNodeEncoderInputLayer if there are extra node features or a different
    output dimension is required.

    Parameters
    ----------
    g: DistGraph
        The distributed graph
    node_lm_configs:
        A list of language model configurations.
    num_train: int
        Number of trainable texts
    lm_infer_batch_size: int
        Batch size used for computing text embeddings for static lm model
    """
    def __init__(self,
                 g,
                 node_lm_configs,
                 num_train=0,
                 lm_infer_batch_size=16):
        super(GSPureLMNodeInputLayer, self).__init__(g)
        assert node_lm_configs is not None and len(node_lm_configs) > 0, \
            "language model configurations must be provided"

        lm_models = th.nn.ModuleList()
        lm_models_info = []
        node_with_lm_feats = set()
        for lm_config in node_lm_configs:
            lm_model = init_lm_model(lm_config,
                                     num_train=num_train,
                                     lm_infer_batch_size=lm_infer_batch_size)
            # A list of node types sharing the same lm model
            lm_ntypes = lm_config["node_types"]
            lm_node_feats = get_lm_node_feats(g, lm_model, lm_ntypes)
            lm_models.append(lm_model)
            lm_models_info.append((lm_ntypes, lm_node_feats))
            node_with_lm_feats.update(lm_ntypes)
        assert len(node_with_lm_feats) == len(g.ntypes), \
            "Every node type in a graph should have text feature"

        self.num_train = num_train
        self.lm_infer_batch_size = lm_infer_batch_size
        self.use_cache = False
        self.lm_emb_cache = {}

        self.lm_models = lm_models
        self.lm_models_info = lm_models_info
        self._feat_size = self.lm_models[0].feat_size
        for lm_model in self.lm_models:
            assert self.out_dims == lm_model.feat_size, \
                "All Language models should have the same output embedding " \
                "dimension, otherwise please use GSLMNodeEncoderInputLayer " \
                "(--model-encoder-type mlp) instead of GSLMNodeLMInputLayer " \
                "(--model-encoder-type lm)"

    def get_general_dense_parameters(self):
        """ Get dense layers' parameters.

        Returns
        -------
        list of Tensors: the dense parameters
        """
        # There is no dense parameters
        return []

    def get_lm_dense_parameters(self):
        """ get the language model related parameters

        Returns
        -------
        list of Tensors: the language model parameters.
        """
        return self.lm_models.parameters()

    def prepare(self, g):
        # If there is no trainable nodes, freeze Bert layer.
        if self.num_train == 0:
            self.freeze(g)

    def freeze(self, g):
        """ Generate Bert caching if needed
        """
        # The lm_emb_cache is used in following cases:
        # 1) We don't need to fine-tune Bert, i.e., train_nodes == 0.
        #    In this case, we only generate bert lm_emb_cache once before model training.
        #
        # 2) GNN warnup when lm_freeze_epochs > 0 (controlled by trainer)
        #    We generate the bert emb_cache before model training.
        #    In the first lm_freeze_epochs epochs, the number of trainable text
        #    nodes are set to 0 and the lm_emb_cache is not refreshed.
        #
        # 3) if train_nodes > 0, no emb_cache is used unless Case 2.
        update_bert_cache(g,
                          self.lm_models_info,
                          self.lm_models,
                          self.lm_emb_cache,
                          self.lm_infer_batch_size)
        self.use_cache = True

    def require_cache_embed(self):
        """ Whether to cache the embeddings for inference.

        Returns
        -------
        Bool : return True to cache the embeddings for inference.
        """
        return True

    #pylint: disable=keyword-arg-before-vararg
    #pylint: disable=unused-argument
    def forward(self, input_feats, input_nodes):
        """Forward computation

        Parameters
        ----------
        input_feats: dict
            input features, ignored
        input_nodes: dict
            input node ids

        Returns
        -------
        a dict of Tensor: the node embeddings.
        """
        assert isinstance(input_nodes, dict), 'The input node IDs should be in a dict.'

        lm_feats = lm_model_forward(input_nodes,
            self.lm_emb_cache,
            self.lm_models,
            self.lm_models_info,
            use_cache=len(self.lm_emb_cache) > 0 and self.use_cache)

        return lm_feats

    @property
    def out_dims(self):
        return self._feat_size

class GSLMNodeEncoderInputLayer(GSNodeEncoderInputLayer):
    """The input encoder layer with language model for all nodes in a heterogeneous graph.

    The input layer adds language model layer on nodes with textual node features and
    generate LM embeddings using the LM model. The LM embeddings are then treated
    as node features.

    The input layer adds learnable embeddings on nodes if the nodes do not have features.
    It adds a linear layer on nodes with node features and the linear layer
    projects the node features to a specified dimension. A user can add learnable
    embeddings on the nodes with node features. In this case, the input layer
    combines the node features with the learnable embeddings and project them to
    the specified dimension.

    Parameters
    ----------
    g: DistGraph
        The distributed graph
    node_lm_configs:
        A list of language model configurations.
    feat_size : dict of int
        The original feat sizes of each node type
    embed_size : int
        The embedding size
    num_train: int
        Number of trainable texts
    lm_infer_batch_size: int
        Batch size used for computing text embeddings for static lm model
    activation : func
        The activation function
    dropout : float
        The dropout parameter
    use_node_embeddings : bool
        Whether we will use the node embeddings for individual nodes even when node features are
        available.
    """
    def __init__(self,
                 g,
                 node_lm_configs,
                 feat_size,
                 embed_size,
                 num_train=0,
                 lm_infer_batch_size=16,
                 activation=None,
                 dropout=0.0,
                 use_node_embeddings=False):
        assert node_lm_configs is not None and len(node_lm_configs) > 0, \
            "language model configurations must be provided"

        lm_models = th.nn.ModuleList()
        lm_models_info = []
        adjust_feat_size = dict(feat_size)
        for lm_config in node_lm_configs:
            lm_model = init_lm_model(lm_config,
                                     num_train=num_train,
                                     lm_infer_batch_size=lm_infer_batch_size)
            # A list of node types sharing the same lm model
            lm_ntypes = lm_config["node_types"]
            lm_node_feats = get_lm_node_feats(g, lm_model, lm_ntypes)
            lm_models.append(lm_model)
            lm_models_info.append((lm_ntypes, lm_node_feats))

            # Update feature size
            for ntype in lm_ntypes:
                adjust_feat_size[ntype] += lm_model.feat_size
                if get_rank() == 0:
                    print(f'Node {ntype} adds lm {lm_config["lm_type"]} '
                          f'features {feat_size[ntype]}->{adjust_feat_size[ntype]}')

        self.num_train = num_train
        self.lm_infer_batch_size = lm_infer_batch_size
        self.use_cache = False
        self.lm_emb_cache = {}

        super(GSLMNodeEncoderInputLayer, self).__init__(
            g, adjust_feat_size, embed_size,
            activation, dropout, use_node_embeddings)

        self.lm_models = lm_models
        self.lm_models_info = lm_models_info

    def get_general_dense_parameters(self):
        """ Get dense layers' parameters.

        Returns
        -------
        list of Tensors: the dense parameters
        """
        params = list(self.proj_matrix.parameters()) \
            if self.proj_matrix is not None else []
        params += list(self.input_projs.parameters())
        return params

    def get_lm_dense_parameters(self):
        """ get the language model related parameters

        Returns
        -------
        list of Tensors: the language model parameters.
        """
        return self.lm_models.parameters()

    def prepare(self, g):
        # If there is no trainable nodes, freeze Bert layer.
        if self.num_train == 0:
            self.freeze(g)

    def freeze(self, g):
        """ Generate Bert caching if needed
        """
        # The lm_emb_cache is used in following cases:
        # 1) We don't need to fine-tune Bert, i.e., train_nodes == 0.
        #    In this case, we only generate bert lm_emb_cache once before model training.
        #
        # 2) GNN warnup when lm_freeze_epochs > 0 (controlled by trainer)
        #    We generate the bert emb_cache before model training.
        #    In the first lm_freeze_epochs epochs, the number of trainable text
        #    nodes are set to 0 and the lm_emb_cache is not refreshed.
        #
        # 3) if train_nodes > 0, no emb_cache is used unless Case 2.
        update_bert_cache(g,
                          self.lm_models_info,
                          self.lm_models,
                          self.lm_emb_cache,
                          self.lm_infer_batch_size)
        self.use_cache = True

    def unfreeze(self):
        """ Disable Bert caching
        """
        if self.num_train != 0:
            self.use_cache = False

    def require_cache_embed(self):
        """ Whether to cache the embeddings for inference.

        Returns
        -------
        Bool : return True to cache the embeddings for inference.
        """
        return True

    #pylint: disable=keyword-arg-before-vararg
    def forward(self, input_feats, input_nodes):
        """Forward computation

        Parameters
        ----------
        input_feats: dict
            input features
        input_nodes: dict
            input node ids

        Returns
        -------
        a dict of Tensor: the node embeddings.
        """
        assert isinstance(input_feats, dict), 'The input features should be in a dict.'
        assert isinstance(input_nodes, dict), 'The input node IDs should be in a dict.'

        # Compute language model features first
        lm_feats = lm_model_forward(input_nodes,
            self.lm_emb_cache,
            self.lm_models,
            self.lm_models_info,
            use_cache=len(self.lm_emb_cache) > 0 and self.use_cache)

        for ntype, lm_feat in lm_feats.items():
            # move lm_feat to the right device
            # we assume input_feats has already been moved to that device.
            lm_feat = lm_feat.to(next(self.parameters()).device)
            if ntype in input_feats:
                input_feats[ntype] = th.cat((input_feats[ntype].float(), lm_feat), dim=-1)
            else:
                input_feats[ntype] = lm_feat

        return super(GSLMNodeEncoderInputLayer, self).forward(input_feats, input_nodes)
