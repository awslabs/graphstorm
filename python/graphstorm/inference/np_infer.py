"""
    Copyright 2023 Contributors

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

    Inferrer wrapper for node classification and regression.
"""
import logging
import time

import dgl

from ..dataloading import GSgnnRealtimeInferNodeDataLoader
from ..dataloading.dataset import (prepare_batch_input,
                                   prepare_blocks_edge_feats)
from ..model import do_full_graph_inference
from ..model.node_gnn import (node_mini_batch_gnn_predict,
                              node_mini_batch_predict)
from ..model.utils import (NodeIDShuffler, save_node_prediction_results,
                           save_shuffled_node_embeddings)
from ..utils import barrier, get_rank, sys_tracker
from .graphstorm_infer import GSInferrer


class GSgnnNodePredictionInferrer(GSInferrer):
    """ Inferrer for node prediction tasks.

    ``GSgnnNodePredictionInferrer`` defines the ``infer()`` method that performs three works:

    * Generate node embeddings and save to disk.
    * Compute inference results for nodes with target node type.
    * (Optional) Evaluate the model performance on a test set if given.

    Parameters
    ----------
    model: GSgnnNodeModelBase
        The GNN model for node prediction, which could be a model class inherited from the
        ``GSgnnNodeModelBase``, or a model class that inherits both the ``GSgnnModelBase``
        and the ``GSgnnNodeModelInterface`` class.
    """

    def infer(self, loader, save_embed_path, save_prediction_path=None,
              use_mini_batch_infer=False,
              node_id_mapping_file=None,
              return_proba=True,
              save_embed_format="pytorch"):
        """ Do inference.

        Parameters
        ----------
        loader : GSNodeDataLoader
            Node dataloader for node prediction task.
        save_embed_path : str
            The path where the GNN embeddings will be saved.
        save_prediction_path : str
            The path where the prediction results will be saved. If is None, will not
            save the predictions.
            Default: None.
        use_mini_batch_infer: bool
            Whether to use mini-batch for inference. Default: False.
        node_id_mapping_file: str
            Path to the file storing node id mapping generated by the
            graph partition algorithm. If is None, will not do node ID
            mapping.
            Default: None.
        return_proba : bool
            Whether to return the predicted results, or only return the argmaxed ones in
            classification models.
        save_embed_format : str
            Specify the data format of saved embeddings. Currently only support
            PyTorch Tensor.
            Default: "pytorch".
        """
        do_eval = self.evaluator is not None
        if do_eval:
            assert loader.label_field is not None, \
                "A label field must be provided for node classification " \
                "or regression inference when evaluation is required."

        sys_tracker.check('start inferencing')
        self._model.eval()
        # get the list of ntypes for inference.
        infer_ntypes = list(loader.target_nidx.keys())

        if use_mini_batch_infer:
            res = node_mini_batch_gnn_predict(self._model, loader, return_proba,
                                              return_label=do_eval)
            preds = res[0]
            embs = res[1]
            labels = res[2] if do_eval else None
        else:
            embs = do_full_graph_inference(self._model, loader.data, fanout=loader.fanout,
                                           task_tracker=self.task_tracker)
            res = node_mini_batch_predict(self._model, embs, loader, return_proba,
                                          return_label=do_eval)
            preds = res[0]
            labels = res[1] if do_eval else None

            if save_embed_path is not None:
                # Only embeddings of the target nodes will be saved.
                embs = {ntype: embs[ntype][loader.target_nidx[ntype]] \
                        for ntype in infer_ntypes}
            else:
                # release embs
                del embs
        sys_tracker.check('finish compute embeddings')

        # do evaluation first
        if do_eval:
            # iterate all the target ntypes
            for ntype in infer_ntypes:
                pred = preds[ntype]
                label = labels[ntype]
                test_start = time.time()
                val_score, test_score = self.evaluator.evaluate(pred, pred, label, label, 0)
                sys_tracker.check('run evaluation')
                if get_rank() == 0:
                    self.log_print_metrics(val_score=val_score,
                                        test_score=test_score,
                                        dur_eval=time.time() - test_start,
                                        total_steps=0)

        nid_shuffler = None
        g = loader.data.g
        if save_embed_path is not None:
            # We are going to save the node embeddings of loader.target_nidx[ntype]
            nid_shuffler = NodeIDShuffler(g, node_id_mapping_file, infer_ntypes) \
                if node_id_mapping_file else None
            shuffled_embs = {}
            for ntype in infer_ntypes:
                if get_rank() == 0:
                    logging.info("save embeddings pf %s to %s", ntype, save_embed_path)

                # only save embeddings of target_nidx
                assert ntype in embs, \
                    f"{ntype} is not in the set of evaluation ntypes {infer_ntypes}"
                emb_nids = loader.target_nidx[ntype]

                if node_id_mapping_file is not None:
                    emb_nids = nid_shuffler.shuffle_nids(ntype, emb_nids)
                shuffled_embs[ntype] = (embs[ntype], emb_nids)
            save_shuffled_node_embeddings(shuffled_embs, save_embed_path, save_embed_format)

            barrier()
            sys_tracker.check('save embeddings')

        if save_prediction_path is not None:
            # save_embed_path may be None. In that case, we need to init nid_shuffler
            if nid_shuffler is None:
                nid_shuffler = NodeIDShuffler(g, node_id_mapping_file, list(preds.keys())) \
                    if node_id_mapping_file else None
            shuffled_preds = {}
            for ntype, pred in preds.items():
                assert ntype in infer_ntypes, \
                    f"{ntype} is not in the set of evaluation ntypes {infer_ntypes}"

                pred_nids = loader.target_nidx[ntype]
                if node_id_mapping_file is not None:
                    pred_nids = nid_shuffler.shuffle_nids(ntype, pred_nids)
                shuffled_preds[ntype] = (pred, pred_nids)

            save_node_prediction_results(shuffled_preds, save_prediction_path)
        barrier()
        sys_tracker.check('save predictions')


class GSGnnNodePredictionRealtimeInferrer(GSInferrer):
    """ Inferrer for real-time node prediction tasks on SageMaker endpoints.

    .. versionadded:: 0.5
        Add `GSGnnNodePredictionRealtimeInferrer` class to support real-time node-level
        inference.

    The real-time inferrer has three major differences from the node prediction offline inferrer:

    1. setting and using a DGLGraph, instead of DistGrahp, as the source to extract features.
    2. using DGL dataloader, instead of GS dataloaders that rely on DGL distributed graphs.
    3. no evaluation section as there is no label on real-time inference.

    """
    def infer(self,
              g,
              dataloader,
              infer_ntypes,
              nfeat_fields,
              efeat_fields=None,
              return_proba=True):
        """
        ``GSGnnNodePredictionRealtimeInferrer`` defines the ``infer()`` method that performs
        three tasks:

        1. Extract one batch using the given dataloader;
        2. Prepare input node and edge features;
        3. Compute inference results for nodes with target node type and return the results.

        Parameters
        ----------
        g: DGLGraph
            The inference graph data in the format of a DGL heterograph. For built-in inference
            pipeline, this graph should be constructed by using methods in the
            `gconstruct.construct_payload_graph.py` file.
        dataloader: GSgnnRealtimeInferNodeDataLoader
            A GSgnnRealtimeInferNodeDataLoader class for node prediction inference.
        infer_ntypes: list of string or a string
            The list of the target node types. Or a single string of the target node type.
        nfeat_fields: dict of {str: list}
            The node feature fields in the format of a dict, whose keys are the node type names, and
            values are lists of feature names.
        efeat_fields: dict of {tuple: list}
            The edge feature fields in the format of a dict, whose keys are the edge type name
             tuples, and values are lists of feature names. Default is None.
        return_proba: boolean
            If return probability of model predictions. Default is True.

        Returns
        -------
        predictions: dict
            The inference results in the format of {str: tensor}.
        """
        assert isinstance(g, dgl.DGLGraph), ('The input graph of '
            '\"GSGnnNodePredictionRealtimeInferrer\" must be an instance of dgl.DGLGraph, '
            f'but got {type(g)}.')
        assert isinstance(dataloader, GSgnnRealtimeInferNodeDataLoader), ('The given dataloader '
            'should be a GSgnnRealtimeInferNodeDataLoader instance or its extensions, but '
            f'got {dataloader}.')
        assert isinstance(infer_ntypes, (list, str)), ('The value of \"infer_ntypes\" '
            f'should be either a list of strings or a single string, but got {infer_ntypes}.')
        if isinstance(infer_ntypes, str):
            infer_ntypes = [infer_ntypes]

        # set model to be in the evaluation mode
        self._model.eval()
        # extract one mini-batch blocks using the given dataloader
        assert len(dataloader) == len(infer_ntypes), ('Real-time inference do single batch '
            'computation for each inference node type, but got the number of mini batch: '
            f'{len(dataloader)} for {len(infer_ntypes)} inference node types.')
        input_nodes, _, blocks = next(iter(dataloader))

        # setup device according to the inferrer's device property
        input_nodes = {ntype: nids.to(self.device) for ntype, nids in input_nodes.items()}
        blocks = [block.to(self.device) for block in blocks]
        g = g.to(self.device)

        # extract node and edge features of the sampled blocks
        # TODO (Jian), handle FeatGroup if the node feature fields are FeatGroups
        #      instead of a list of strings
        n_h = prepare_batch_input(g, input_nodes, feat_field=nfeat_fields,
                                  dev=self.device)
        if efeat_fields:
            e_hs = prepare_blocks_edge_feats(g, blocks, efeat_fields, device=self.device)
        else:
            e_hs = prepare_blocks_edge_feats(g, blocks, None, device=self.device)

        # do predict on the blocks
        logits, _ = self._model.predict(blocks, n_h, e_hs, input_nodes,
                                        return_proba=return_proba)

        # post processing to extract predictions on inference node types
        predictions = {}
        for ntype in infer_ntypes:
            assert ntype in logits, \
                f"{ntype} is not in the set of prediction ntypes {list(logits.keys())}."
            predictions[ntype] = logits[ntype].cpu().detach().numpy()

        return predictions
