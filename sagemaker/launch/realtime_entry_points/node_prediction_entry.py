"""
    Copyright Contributors

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

    SageMaker entry point file for GraphStorm node prediction realtime inference
"""

import json
import logging
import os
import traceback
from argparse import Namespace
from datetime import datetime as dt

import numpy as np
import torch as th

import graphstorm as gs
from graphstorm.utils import setup_device, get_device
from graphstorm.gconstruct import (ERROR_CODE, GRAPH, MSG, NODE_MAPPING,
                                   STATUS, process_json_payload_graph)
from graphstorm.dataloading import GSgnnRealtimeInferNodeDataLoader
from graphstorm.inference import GSGnnNodePredictionRealtimeInferrer
from graphstorm.sagemaker import GSRealTimeInferenceResponseMessage as res_msg

# Set seed to ensure prediction results constantly
th.manual_seed(12345678)
np.random.seed(12345678)

# Global variables to be initialized during model loading
CONFIG_JSON = None
GS_CONFIG = None
# Global variables as keys
DATA = 'data'
TARGETS = 'targets'
RESULTS = 'results'


# ================== SageMaker real-time entry point functions ================== #
def model_fn(model_dir):
    """ Load GraphStorm trained model artifacts.

    GraphStorm model artifacts include three major files:
    1. The trained GraphStorm model. By default, it will be a `model.bin` file.
    2. The model configuration YAML file, which should be the one generated by each training job.
    3. The graph configuration JSON file, which should be the one generated by a gconstruct or
       GSProcessing job.
    These components should be packed in a tar file that SageMaker will download and unzip to the
    given `model_dir`, which has the following structure,

    - model_dir
        |- model.bin
        |- GRAPHSTORM_RUNTIME_UPDATED_TRAINING_CONFIG.yaml
        |- data_transform_new.json
        |- code
            |- node_prediction_entry.py

    Parameters
    ----------
    model_dir: str
        The SageMaker model directory. In theory, it should be "/opt/ml/model/".

    Returns
    -------
    model: GraphStorm model
        A GraphStorm model loaded and recreated from model artifacts.

    Note: Because these functions, `model_fn`, `input_fn`, `predict_fn`, and `output_fn`, are
          called by the endpoint separately, variables cannot be passed among them directly. So,
          this function also sets the two global variables, CONFIG_JSON and MODEL_YAML, for other
          functions.
    """
    logging.info('-- START model loading... ')
    s_t = dt.now()

    # find the name of artifact file names, assuming there is only one type file packed
    model_file = None
    yaml_file = None
    json_file = None
    files = os.listdir(model_dir)

    # keep the file name or extension check logic here for easy customization as users may use
    # different artifact names or extensions from the default settings
    for file in files:
        if file == 'model.bin':
            model_file = file
        if file.endswith('.yaml'):
            yaml_file = file
        elif file.endswith('.json'):
            json_file = file
        else:
            continue
    # check if required artifacts exist
    assert model_file is not None, f'Missing model file, e.g., \"model.bin\", in the tar file.'
    assert yaml_file is not None, f'Missing model configuration YAML file in the tar file.' 
    assert json_file is not None, f'Missing graph configuration JSON file in the tar file.'

    # load and recreate the trained model using the gsf built-in function
    try:
        model, config_json, gs_config = gs.restore_builtin_model_from_artifacts(model_dir,
                                                                                json_file,
                                                                                yaml_file)
        global CONFIG_JSON, GS_CONFIG
        CONFIG_JSON = config_json
        GS_CONFIG = gs_config
    except Exception as e:
        model = None
        logging.error('Fail to restore trained GraphStorm model. Details:\n %s', e)
        # This will be endpoint backend error, so not use the response class
        raise Exception('Fail to restore trained GraphStorm model. Details: %s', e)

    e_t = dt.now()
    diff_t = int((e_t - s_t).microseconds / 1000)
    logging.info(f'--model_fn: used {diff_t} ms ...')

    logging.debug(model)
    logging.debug(CONFIG_JSON)
    logging.debug(GS_CONFIG)

    return model

def input_fn(request_body, request_content_type='application/json'):
    """ Preprocessing request_body that is in JSON format.
    
    #TODO: add the API specification url here:

    According to GraphStorm real-time inference API specification, the payload is like:

    {
        "version": "gs-realtime-v0.1",
        "gml_task": "node_classification",
        "graph": {
            "nodes": [
                {
                    "node_type": "author",
                    "features": {
                        "feat": [
                            0.011269339360296726,
                            ......
                        ]
                    },
                    "node_id": "a4444"
                },
                {
                    "node_type": "author",
                    "features": {
                        "feat": [
                            -0.0032965524587780237,
                            .....
                        ]
                    },
                    "node_id": "s39"
                }
            ],
            "edges": [
                {
                    "edge_type": [
                        "author",
                        "writing",
                        "paper"
                    ],
                    "features": {},
                    "src_node_id": "p4463",
                    "dest_node_id": "p4463"
                },
                ......
            ]
        },
        "targets": [
            {
                "node_type": "paper",
                "node_id": "p4463"
            },
            or 
            {
                "edge_type": [
                        "paper",
                        "citing",
                        "paper"
                    ]
                "src_node_id": "p3551",
                "dest_node_id": "p3551"
            }
        ]
    }

    Parameters
    ----------
    request_body: JSON object
        The JSON object in the request. The JSON object contains the subgraph for inference.
    request_content_type: str
        String to indicate what is the format of the payload. For GraphStorm built-in real-time
        input function, the format should be 'application/json'.

    Return
    -------
    dgl_graph: DGLGraph
        A DGL graph for inference. For GraphStorm built-in inference pipeline, the graph is a
        `DGLGraph` instance.
    target_dict: 
    """

    logging.info('-- START processing input data... ')
    s_t = dt.now()

    logging.debug(request_body)

    try:
        payload_data = json.loads(request_body)
    except Exception as e:
        res = res_msg.json_format_error(error=e)
        return res

    # TODO(Jian), build a unified payload content sanity checking  method under gconstruct package
    # to be shared by all entry point files

    # the version object will be used later to keep backward compatibilty for early versions
    version = payload_data.get('version', None)

    gml_task = payload_data.get('gml_task', None)
    targets = payload_data.get('targets', None)

    # 1. check if the payload is for a NC task
    if gml_task is None or (gml_task not in ['node_classification', 'node_regression']):
        track = f'This endpoint is for node prediction, but got {gml_task} task from the payload.'
        res = res_msg.model_mismatch_error(track=track)
        return res

    # 2. check if the targets field is provided
    if targets is None or len(targets)==0:
        res = res_msg.missing_required_field(field='targets')
        return res
    # check if target has node_type and node_id. Will check values in id mapping
    for target in targets:
        if isinstance(target, dict):
            if 'node_type' not in target or 'node_id' not in target:
                res = res_msg.json_format_error(error=('The Element of \"targets\" field should ' \
                    'be a dictionary that has both \"node_type\" and \"node_id\" keys, but ' \
                    f'got {target}.'))
                return res
        else:
            res = res_msg.json_format_error(error=('The Element of \"targets\" field should ' \
                f'be a dictionary, but got {target}'))
            return res

    # processing payload to generate a DGL graph
    global CONFIG_JSON
    g_resp = process_json_payload_graph(payload_data, CONFIG_JSON)

    if g_resp[STATUS] == 400:
        track = f'Error code: {g_resp[ERROR_CODE]}, Message: {g_resp[MSG]}.'
        res = res_msg.graph_construction_failure(track=track)
        return res

    # succeeded
    if g_resp[STATUS] == 200:
        dgl_graph = g_resp[GRAPH]
        raw_node_id_maps = g_resp[NODE_MAPPING]

    # mapping the targets, a list of node objects, to new graph node IDs after dgl graph
    # construction for less overall data processing time
    target_mapping_dict = {}
    for target in targets:
        target_ntype = target['node_type']
        target_nid = target['node_id']

        if target_ntype in target_mapping_dict:
            orig_target_nids = target_mapping_dict[target_ntype][0]
            orig_target_nids.append(target_nid)
            graph_target_nids = target_mapping_dict[target_ntype][1]
            # target id is not in the subgraph
            if raw_node_id_maps[target_ntype].get(target_nid, None) is None:
                res = res_msg.mismatch_target_nid(target_nid)
                return res
            else:
                graph_target_nid = raw_node_id_maps[target_ntype][target_nid]
                graph_target_nids.append(graph_target_nid)
        else:
            orig_target_nids = []
            orig_target_nids.append(target_nid)
            graph_target_nids = []
            # target id is not in the subgraph
            if raw_node_id_maps[target_ntype].get(target_nid, None) is None:
                res = res_msg.mismatch_target_nid(target_nid)
                return res
            else:
                graph_target_nid = raw_node_id_maps[target_ntype][target_nid]
                graph_target_nids.append(graph_target_nid)
                target_mapping_dict[target_ntype] = ([target_nid], [graph_target_nid])

    data = {DATA: dgl_graph, TARGETS: target_mapping_dict}
    res = res_msg.success(data=data)

    e_t = dt.now()
    diff_t = int((e_t - s_t).microseconds / 1000)
    logging.info(f'--input_fn: used {diff_t} ms ...')

    return res

def predict_fn(input_data, model):
    """ Make prediction on the given subgraph for the given targets with the loaded model.
    
    Parameters
    ----------
    input_data: dict
        The input data is a GSRealTimeInferenceResponseMessage, where 'status_code': 200,
        'message': 'something', 'data': {'data': dgl_graph, 'targets': target_dict}}.
        The target_dict is a dictionary whose keys are target node types, and keys are tuples
        where the 1st element is a list of the original node IDs, and the 2nd element is a list
        of the new DGL graph integer IDs. The input_data comes from the input_fn functioin.
    model: GraphStorm model
        A GraphStorm model loaded from the model_fn() function.
    """
    logging.info('-- START prediction... ')
    s_t = dt.now()

    res = {}

    # Handle payload errors from the input_fn
    if input_data.status_code != 200:
        return input_data.to_dict()

    # extract the data
    dgl_graph = input_data.data[DATA]
    target_dict = input_data.data[TARGETS]

    # extract the DGL graph nids from target ID mapping dict
    target_nids = {}
    for ntype, (_, dgl_nids) in target_dict.items():
        target_nids[ntype] = dgl_nids

    # Use load model configuration to intialize an inferrer
    global GS_CONFIG
    gs_config = GS_CONFIG

    try:
        # setup device
        setup_device(0)
        device = get_device()

        # initialize a GS real-time inferrer
        inferrer = GSGnnNodePredictionRealtimeInferrer(model)
        inferrer.setup_device(device)
        # initialize a GS real-time dataLoader
        dataloader = GSgnnRealtimeInferNodeDataLoader(dgl_graph,
                                                      target_nids,
                                                      gs_config.num_layers)
        predictions = inferrer.infer(dgl_graph, dataloader, list(target_dict.keys()),
                                     gs_config.node_feat_name, 
                                     gs_config.edge_feat_name)
        # Build prediction response
        pred_list = []
        for ntype, preds in predictions.items():
            (orig_nids, dgl_nids) = target_dict[ntype]
            preds_in_orig = preds[dgl_nids].cpu().numpy().tolist()
            for orig_nid, pred_in_orig in zip(orig_nids, preds_in_orig):
                pred_res = {
                    'node_type': ntype,
                    'node_id': orig_nid,
                    'prediction': pred_in_orig
                }
                pred_list.append(pred_res)
    except Exception as e:
        logging.error(traceback.format_exc())
        res = res_msg.internal_server_error(detail=e)
        return res.to_dict()

    res = res_msg.success(data={RESULTS: pred_list})

    e_t = dt.now()
    diff_t = int((e_t - s_t).microseconds / 1000)
    logging.info(f'--predict_fn: used {diff_t} ms ...')

    return res.to_dict()
