"""
    Copyright Contributors

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

    SageMaker entry point file for GraphStorm node prediction realtime inference
"""

import json
import logging
import os
import traceback
from argparse import Namespace
from datetime import datetime as dt

import numpy as np
import torch as th

import graphstorm as gs
from graphstorm.utils import setup_device, get_device
from graphstorm.config import COMBINED_CONFIG_FILENAME
from graphstorm.gconstruct import (PAYLOAD_PROCESSING_STATUS,
                                   PAYLOAD_PROCESSING_RETURN_MSG,
                                   PAYLOAD_PROCESSING_ERROR_CODE,
                                   PAYLOAD_GRAPH,
                                   PAYLOAD_GRAPH_NODE_MAPPING,
                                   UPDATED_CONFIGURATION_FILENAME,
                                   process_json_payload_graph)
from graphstorm.dataloading import GSgnnRealtimeInferNodeDataLoader
from graphstorm.inference import GSGnnNodePredictionRealtimeInferrer
from graphstorm.sagemaker import GSRealTimeInferenceResponseMessage as RTResponseMsg

# Set seed to ensure prediction results to be constant
th.manual_seed(12345678)
np.random.seed(12345678)

DEFAULT_GS_MODEL_FILE_NAME = 'model.bin'

# set some constants as response keys
RESPONSE_DATA = 'data'
PREDICTION_TARGETS = 'targets'
PREDICTION_RESULTS = 'results'


# ================== SageMaker real-time entry point functions ================== #
def model_fn(model_dir):
    """ Load GraphStorm trained model artifacts.

    GraphStorm model artifacts include three major files:
    1. The trained GraphStorm model. By default, it will be a `model.bin` file.
    2. The model configuration YAML file, which should be the one generated by each training job.
    3. The graph configuration JSON file, which should be the one generated by a gconstruct or
       GSProcessing job.
    These components should be packed in a tar file that SageMaker will download and unzip to the
    given `model_dir`, which has the following structure,

    - model_dir
        |- model.bin # Binary model artifact
        |- GRAPHSTORM_RUNTIME_UPDATED_TRAINING_CONFIG.yaml # Train YAML config, updated with runtime parameters
        |- data_transform_new.json # GConstruct configuration JSON used during graph processing
        |- code
            |- node_prediction_entry.py # Entry point for task

    Parameters
    ----------
    model_dir: str
        The SageMaker model directory where SageMake unzips the tar file provided in endpoint
        deployment.

    Returns
    -------
    A tuple of three elements, including:
    model: GraphStorm model
        A GraphStorm model rebuilt from model artifacts.
    config_json:
        A JSON object loaded from the given graph configuration JSON file.
    gs_config: GSConfig
        An instance of GSConfig object built from the given model configuration YAML file.
    """
    logging.info('-- START model loading... ')

    # find the name of artifact file names, assuming there is only one type file packed
    model_file = None
    yaml_file = None
    json_file = None
    files = os.listdir(model_dir)

    # keep the file name or extension check logic here for easy customization as users may use
    # different artifact names or extensions from the default settings
    if DEFAULT_GS_MODEL_FILE_NAME in files:
        model_file = DEFAULT_GS_MODEL_FILE_NAME
    if COMBINED_CONFIG_FILENAME in files:
        yaml_file = COMBINED_CONFIG_FILENAME
    if UPDATED_CONFIGURATION_FILENAME in files:
        json_file = UPDATED_CONFIGURATION_FILENAME

    # release the control of file names
    for file in files:
        if yaml_file is None and file.endswith('.yaml'):
            yaml_file = file
        if json_file is None and file.endswith('.json'):
            json_file = file

    # check if required artifacts exist
    assert model_file is not None, f'Missing model file, e.g., \"model.bin\", in the tar file.'
    assert yaml_file is not None, f'Missing model configuration YAML file in the tar file.' 
    assert json_file is not None, f'Missing graph configuration JSON file in the tar file.'

    # load and recreate the trained model using the gsf built-in function
    try:
        model, gconstruct_config_dict, gs_train_config = gs.restore_builtin_model_from_artifacts(model_dir,
                                                                                json_file,
                                                                                yaml_file)
    except Exception as e:
        model = None
        logging.error('Fail to restore trained GraphStorm model. Details:\n %s', e)
        # This will be endpoint backend error, so not use the response class
        raise Exception('Fail to restore trained GraphStorm model. Details: %s', e)

    logging.debug(model)
    logging.debug(config_json)
    logging.debug(gs_config)

    return (model, gconstruct_config_dict, gs_train_config)

def transform_fn(model,
                 request_body,
                 request_content_type,
                 response_content_type='application/json'):
    """ An end-to-end function to handle one request from input to output.

    #TODO: add the API specification url here:

    According to GraphStorm real-time inference API specification, the payload is like:

    {
        "version": "gs-realtime-v0.1",
        "gml_task": "node_classification",
        "graph": {
            "nodes": [
                {
                    "node_type": "author",
                    "features": {
                        "feat": [
                            0.011269339360296726,
                            ......
                        ]
                    },
                    "node_id": "a4444"
                },
                {
                    "node_type": "author",
                    "features": {
                        "feat": [
                            -0.0032965524587780237,
                            .....
                        ]
                    },
                    "node_id": "s39"
                }
            ],
            "edges": [
                {
                    "edge_type": [
                        "author",
                        "writing",
                        "paper"
                    ],
                    "features": {},
                    "src_node_id": "p4463",
                    "dest_node_id": "p4463"
                },
                ......
            ]
        },
        "targets": [
            {
                "node_type": "paper",
                "node_id": "p4463"
            },
            or 
            {
                "edge_type": [
                        "paper",
                        "citing",
                        "paper"
                    ]
                "src_node_id": "p3551",
                "dest_node_id": "p3551"
            }
        ]
    }

    Parameters
    ----------
    model: tuple of three elements
        The output of the model_fn, including a model object, a json object, and a GSConfig object.
    request_body: JSON object
        The JSON object in the request. The JSON object contains the subgraph for inference.
    request_content_type: str
        A string to indicate what is the format of the payload. For GraphStorm built-in real-time
        input function, the format should be 'application/json'.
    response_content_type: str
        A string to indicate what is the format of the response. Default is 'application/json'.

    Returns
    -------
    res: JSON format of GSRealTimeInferenceResponseMessage
        The JSON representations of an instance of GSRealTimeInferenceResponseMessage.
    response_content_type: str
        A string to indicate what is the format of the response.
    """
    model_obj, config_json, gs_config = model
    
    logging.debug(request_body)

    if request_content_type != 'application/json':
        res = RTResponseMsg.json_format_error(error=f'Unsupported content type: {request_content_type}')
        return res.to_json(), response_content_type
    try:
        payload_data = json.loads(request_body)
    except Exception as e:
        res = RTResponseMsg.json_format_error(error=e)
        return res.to_json(), response_content_type

    # TODO(Jian), build a unified payload content sanity checking  method under gconstruct package
    # to be shared by all entry point files

    # the version object will be used later to keep backward compatibilty for early versions
    version = payload_data.get('version', None)

    gml_task = payload_data.get('gml_task', None)
    targets = payload_data.get('targets', None)

    # 1. check if the payload is for a node prediction task
    if gml_task is None or (gml_task not in ['node_classification', 'node_regression']):
        track = (f'This endpoint is for node prediction task, but got {gml_task} task from ' \
                 'the payload. Supported task types include [\"node_classification\", ' \
                 '\"node_regression\"]')
        res = RTResponseMsg.task_mismatch_error(track=track)
        return res.to_json(), response_content_type

    # 2. check if the targets field is provided
    if targets is None or len(targets)==0:
        res = RTResponseMsg.missing_required_field(field='targets')
        return res.to_json(), response_content_type

    # 3. check if target has node_type and node_id. Will check values in id mapping
    for target in targets:
        if isinstance(target, dict):
            if 'node_type' not in target or 'node_id' not in target:
                res = RTResponseMsg.json_format_error(error=('The Element of \"targets\" field should ' \
                    'be a dictionary that has both \"node_type\" and \"node_id\" keys, but ' \
                    f'got {target}.'))
                return res.to_json(), response_content_type
        else:
            res = RTResponseMsg.json_format_error(error=('The Element of \"targets\" field should ' \
                f'be a dictionary, but got {target}'))
            return res.to_json(), response_content_type

    # processing payload to generate a DGL graph
    g_resp = process_json_payload_graph(payload_data, config_json)

    # generation failed
    if g_resp[PAYLOAD_PROCESSING_STATUS] == 400:
        track = (f'Error code: {g_resp[PAYLOAD_PROCESSING_ERROR_CODE]}, ' \
                 f'Message: {g_resp[PAYLOAD_PROCESSING_RETURN_MSG]}.')
        res = RTResponseMsg.graph_construction_failure(track=track)
        return res.to_json(), response_content_type

    # generation succeeded
    if g_resp[PAYLOAD_PROCESSING_STATUS] == 200:
        dgl_graph = g_resp[PAYLOAD_GRAPH]
        raw_node_id_maps = g_resp[PAYLOAD_GRAPH_NODE_MAPPING]

    # 4. check if node or edge feature names match with model configurations
    if gs_config.node_feat_name is not None:
        for ntype, feat_list in gs_config.node_feat_name.items():
            # it is possible that some node types are not sampled 
            if ntype not in dgl_graph.ntypes:
                continue

            for feat in feat_list:
                if feat not in dgl_graph.nodes[ntype].data:
                    res = RTResponseMsg.missing_feature(entity_type='node', 
                                                        entity_name=ntype,
                                                        feat_name=feat)
                    return res.to_json(), response_content_type

    if gs_config.edge_feat_name is not None:
        for etype, feat_list in gs_config.edge_feat_name.items():
            # it is possible that some edge types are not sampled 
            if etype not in dgl_graph.etypes:
                continue

            for feat in feat_list:
                if feat not in dgl_graph.edges[etype].data:
                    res = RTResponseMsg.missing_feature(entity_type='edge', 
                                                        entity_name=etype,
                                                        feat_name=feat)
                    return res.to_json(), response_content_type


    # mapping the targets, a list of node objects, to new graph node IDs after dgl graph
    # construction for less overall data processing time
    target_mapping_dict = {}
    for target in targets:
        target_ntype = target['node_type']
        target_nid = target['node_id']

        if target_ntype in target_mapping_dict:
            graph_target_nids = target_mapping_dict[target_ntype][1]
            # target id is not in the subgraph
            if raw_node_id_maps[target_ntype].get(target_nid, None) is None:
                res = RTResponseMsg.mismatch_target_nid(target_nid)
                return res.to_json(), response_content_type
            else:
                graph_target_nid = raw_node_id_maps[target_ntype][target_nid]
                graph_target_nids.append(graph_target_nid)
        else:
            graph_target_nids = []
            # target id is not in the subgraph
            if raw_node_id_maps[target_ntype].get(target_nid, None) is None:
                res = RTResponseMsg.mismatch_target_nid(target_nid)
                return res.to_json(), response_content_type
            else:
                graph_target_nid = raw_node_id_maps[target_ntype][target_nid]
                graph_target_nids.append(graph_target_nid)
                target_mapping_dict[target_ntype] = ([target_nid], [graph_target_nid])

    # extract the DGL graph nids from target ID mapping dict
    target_nids = {}
    for ntype, (_, dgl_nids) in target_mapping_dict.items():
        target_nids[ntype] = dgl_nids

    try:
        # setup device
        setup_device(0)
        device = get_device()

        # initialize a GS real-time inferrer
        inferrer = GSGnnNodePredictionRealtimeInferrer(model_obj)
        inferrer.setup_device(device)
        # initialize a GS real-time dataLoader
        dataloader = GSgnnRealtimeInferNodeDataLoader(dgl_graph,
                                                      target_nids,
                                                      gs_config.num_layers)
        predictions = inferrer.infer(dgl_graph, dataloader, list(target_mapping_dict.keys()),
                                     gs_config.node_feat_name, 
                                     gs_config.edge_feat_name)
        # Build prediction response
        pred_list = []
        for ntype, preds in predictions.items():
            (orig_nids, _) = target_mapping_dict[ntype]
            # the dataloader ensures that the order of predictions is same as the order of nids
            for orig_nid, pred in zip(orig_nids, preds):
                pred_res = {
                    'node_type': ntype,
                    'node_id': orig_nid,
                    'prediction': pred.tolist()
                }
                pred_list.append(pred_res)
    except Exception as e:
        logging.error(traceback.format_exc())
        res = RTResponseMsg.internal_server_error(detail=e)
        return res.to_json(), response_content_type

    res = RTResponseMsg.success(data={PREDICTION_RESULTS: pred_list})

    return res.to_json(), response_content_type
