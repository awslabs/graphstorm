"""
    Copyright Contributors

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

    SageMaker entry point file for SageMaker realtime inference endpoint
"""

import json
import logging
import os
import traceback
from argparse import Namespace
from datetime import datetime as dt

import numpy as np
import torch as th
from dgl.dataloading import DataLoader, MultiLayerFullNeighborSampler

import graphstorm as gs
from graphstorm.config import GSConfig
from graphstorm.dataloading.metadata import (GSMetadataDglDistGraph,
                                             load_metadata_from_json)
from graphstorm.gconstruct import (ERROR_CODE, GRAPH, MSG, NODE_MAPPING,
                                   STATUS, process_json_payload_graph)
from graphstorm.inference import GSGnnNodePredictionRealtimeInferrer
from graphstorm.sagemaker import GSRealTimeInferenceResponseMessage as res_msg

# Set seed to ensure prediction results constantly
th.manual_seed(6553865)
np.random.seed(6553865)

# Global variables to be initialized during model loading
CONFIG_JSON = None
MODEL_YAML = None
# Global variables as keys
DATA = 'data'
TARGETS = 'targets'
RESULTS = 'results'


# ================== SageMaker real-time entry point functions ================== #
def model_fn(model_dir):
    """ Load GraphStorm trained model artifacts.

    GraphStorm model artifacts include three major files:
    1. The trained GraphStorm model. By default, it will be a `model.bin` file.
    2. The model configuration YAML file, which should be the one generated by each training job.
    3. The graph configuration JSON file, which should be the one generated by the gconstruct or
       GSProcessing operation.
    These components should be packed in a tar file that SageMaker will download and unzip to the
    given `model_dir`, which has the following structure,

    - model_dir
        |- model.bin
        |- acm_nc.yaml
        |- new_acm_config.json
        |- code
            |- node_prediction_entry.py

    Parameters
    ----------
    model_dir: str
        The SageMaker model directory. In theory, it should be "/opt/ml/model/".

    Returns
    -------
    model: GraphStorm model
        A GraphStorm model loaded and recreated from model artifacts.

    Note: Because these functions, `model_fn`, `input_fn`, `predict_fn`, and `output_fn`, are
          called by the endpoint separately, variables cannot be passed among them directly. So,
          this function also sets the two global variables, CONFIG_JSON and MODEL_YAML, for other
          functions.
    """
    logging.info('-- START model loading... ')
    s_t = dt.now()

    # find the name of artifact file names, assuming there is only one type file packed
    model_file = None
    yaml_file = None
    json_file = None
    files = os.listdir(model_dir)
    for file in files:
        if file.endswith('.bin'):
            model_file = file
        if file.endswith('.yaml'):
            yaml_file = file
        elif file.endswith('.json'):
            json_file = file
        else:
            continue

    # check if required artifacts exist
    assert model_file is not None, f'Missing model file, e.g., \"model.bin\", in the tar file.'
    assert yaml_file is not None, f'Missing model configuration YAML file in the tar file.' 
    assert json_file is not None, f'Missing graph configuration JSON file in the tar file.'

    # load and recreate the trained model using the gsf built-in function
    model, config_json, model_yaml = gs.restore_builtin_node_model4realtime(model_dir,
                                                                            json_file,
                                                                            yaml_file)
    global CONFIG_JSON, MODEL_YAML
    CONFIG_JSON = config_json
    MODEL_YAML = model_yaml

    e_t = dt.now()
    diff_t = int((e_t - s_t).microseconds / 1000)
    logging.info(f'--model_fn: used {diff_t} ms ...')

    logging.debug(model)
    logging.debug(CONFIG_JSON)
    logging.debug(MODEL_YAML)

    return model

def input_fn(request_body, request_content_type='application/json'):
    """ Preprocessing request_body that is in JSON format.
    
    #TODO: add the API specification url here:

    According to GraphStorm real-time inference API specification, the payload is like:

    {
        "version": "gs-realtime-v0.1",
        "gml_task": "node_classification",
        "graph": {
            "nodes": [
                {
                    "node_type": "author",
                    "features": {
                        "feat": [
                            0.011269339360296726,
                            ......
                        ]
                    },
                    "node_id": "a4444"
                },
                {
                    "node_type": "author",
                    "features": {
                        "feat": [
                            -0.0032965524587780237,
                            .....
                        ]
                    },
                    "node_id": "s39"
                }
            ],
            "edges": [
                {
                    "edge_type": [
                        "author",
                        "writing",
                        "paper"
                    ],
                    "features": {},
                    "src_node_id": "p4463",
                    "dest_node_id": "p4463"
                },
                ......
            ]
        },
        "targets": [
            {
                "node_type": "paper",
                "node_id": "p4463"
            },
            or 
            {
                "edge_type": [
                        "paper",
                        "citing",
                        "paper"
                    ]
                "src_node_id": "p3551",
                "dest_node_id": "p3551"
            }
        ]
    }

    Parameters
    ----------
    request_body: JSON object
        The JSON object in the request. The JSON object contains the subgraph for inference.
    request_content_type: str
        String to indicate what is the format of the payload. For GraphStorm built-in real-time
        input function, the format should be 'application/json'.

    Return
    -------
    dgl_graph: DGLGraph
        A DGL graph for inference. For GraphStorm built-in inference pipeline, the graph is a
        `DGLGraph` instance.
    target_dict: 
    """

    logging.info('-- START processing input data... ')
    s_t = dt.now()

    logging.debug(request_body)

    payload_data = json.loads(request_body)

    result = {}

    # general checks of the payload, while other checks will be in the payload processing
    version = payload_data.get('version', None)
    gml_task = payload_data.get('gml_task', None)
    targets = payload_data.get('targets', None)

    # 1. check if the payload is for a NC task
    if gml_task is None or not (gml_task == 'node_classification' or gml_task == 'node_regression'):
        track = f'This endpoint is for node prediction, but got {gml_task} task from the payload.'
        res = res_msg.model_mismatch_error(track=track)
        return res

    # 2. check if the targets field is provided
    if targets is None or len(targets)==0:
        res = res_msg.missing_required_field(field='targets')
        return res

    # processing payload to generate a DGL graph
    global CONFIG_JSON
    g_resp = process_json_payload_graph(payload_data, CONFIG_JSON)

    if g_resp[STATUS] == 400:
        track = f'Error code: {g_resp[ERROR_CODE]}, Message: {g_resp[MSG]}.'
        res = res_msg.graph_construction_failure(track=track)
        return res
    elif g_resp[STATUS] == 200:   # succeeded
        dgl_graph = g_resp[GRAPH]
        raw_node_id_maps = g_resp[NODE_MAPPING]

    # mapping the targets, a list of node objects, to new graph node IDs
    target_dict = {}
    for target in targets:
        target_ntype = target['node_type']
        target_nid = target['node_id']

        if target_ntype in target_dict:
            orig_target_nids = target_dict[target_ntype][0]
            orig_target_nids.append(target_nid)
            graph_target_nids = target_dict[target_ntype][1]
            # target id is not in the subgraph
            if raw_node_id_maps[target_ntype].get(target_nid, None) is None:
                res = res_msg.mismatch_target_nid(target_nid)
                return res
            else:
                graph_target_nid = raw_node_id_maps[target_ntype][target_nid]
                graph_target_nids.append(graph_target_nid)
        else:
            orig_target_nids = []
            orig_target_nids.append(target_nid)
            graph_target_nids = []
            # target id is not in the subgraph
            if raw_node_id_maps[target_ntype].get(target_nid, None) is None:
                res = res_msg.mismatch_target_nid(target_nid)
                return res
            else:
                graph_target_nid = raw_node_id_maps[target_ntype][target_nid]
                graph_target_nids.append(graph_target_nid)
                target_dict[target_ntype] = (orig_target_nids, graph_target_nids)

    data = {DATA: dgl_graph, TARGETS: target_dict}
    res = res_msg.success(data=data)

    e_t = dt.now()
    diff_t = int((e_t - s_t).microseconds / 1000)
    logging.info(f'--input_fn: used {diff_t} ms ...')

    return res

def predict_fn(input_data, model):
    """ Make prediction on the given subgraph for the given targets with the loaded model.
    
    Parameters
    ----------
    input_data: dict
        The input data is a GSRealTimeInferenceResponseMessage, where 'status_code': 200,
        'message': 'something', 'data': {'data': dgl_graph, 'targets': target_dict}}.
        The target_dict is a dictionary whose keys are target node types, and keys are tuples
        where the 1st element is a list of the original node IDs, and the 2nd element is a list
        of the new DGL graph integer IDs. The input_data comes from the input_fn functioin.
    model: GraphStorm model
        A GraphStorm model loaded from the model_fn() function.
    """
    logging.info('-- START prediction... ')
    s_t = dt.now()

    res = {}
    # Handle payload errors from the input_fn
    
    if input_data[STATUS] != 200:
        res[STATUS] = input_data[STATUS]
        res[MSG] = input_data[MSG]
        res[RESULTS] = {}
        return res

    # extract the data
    dgl_graph = input_data[DATA]
    target_dict = input_data[TARGETS]

    # sample input graph to build blocks
    target_nids = {}
    batch_size = 0
    for ntype, (_, dgl_nids) in target_dict.items():
        target_nids[ntype] = dgl_nids
        if len(dgl_nids) > batch_size:
            batch_size = len(dgl_nids)

    # Use load model configuration to intialize an inferrer
    global MODEL_YAML
    model_config = MODEL_YAML

    try:
        inferrer = GSGnnNodePredictionRealtimeInferrer(model)
        # Initialize a DGL Sampler and DataLoader for the inferrer
        sampler = MultiLayerFullNeighborSampler(model_config.num_layers)
        dataloader = DataLoader(dgl_graph, target_nids, sampler,
                                batch_size=batch_size, shuffle=False,
                                drop_last=False)
        predictions = inferrer.infer(dgl_graph, dataloader, list(target_dict.keys()),
                                     model_config.node_feat_name, 
                                     model_config.edge_feat_name)
        # Build prediction response
        pred_list = []
        for ntype, preds in predictions.items():
            (orig_nids, dgl_nids) = target_dict[ntype]
            preds_in_orig = preds[dgl_nids].tolist()
            for orig_nid, pred_in_orig in zip(orig_nids, preds_in_orig):
                pred_res = {
                    'node_type': ntype,
                    'node_id': orig_nid,
                    'prediction': pred_in_orig
                }

                pred_list.append(pred_res)
    except Exception as e:
        logging.error(traceback.format_exc())
        res[STATUS] = 500
        res[MSG] = 'Generic server error. Please contact with your administrators ' + \
                   f'for this error: {traceback.format_exc()}'
        res[RESULTS] = {}
        return res

    res = {}
    res[STATUS] = 200
    res[MSG] = 'The request is successfully proceeded. '
    res[RESULTS] = pred_list

    e_t = dt.now()
    diff_t = int((e_t - s_t).microseconds / 1000)
    logging.info(f'--predict_fn: used {diff_t} ms ...')

    return res