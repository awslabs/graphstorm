"""
    Copyright 2025 Contributors

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

    Utility functions for inference
"""

import logging
import os
import re
import json
import shutil
import tarfile
from argparse import ArgumentTypeError
from urllib.parse import urlparse

import boto3
from botocore.errorfactory import ClientError
from sagemaker.s3 import S3Uploader


def wrap_model_artifacts(path_to_model, path_to_yaml, path_to_json, path_to_entry,
                         output_path, output_tarfile_name='model'):
    """ A utility function to zip model artifacts into a tar package

    According to SageMaker's specification of the `Model Directory Structure
    https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#model-directory-structure`_,
    this function will put the entry point file into a sub-folder, named `code`, and then zip the
    `model.bin`, the `**.yaml`, the `**.json` files, and the `code` sub-folder into a tar package
    with the name specified by the output_tarfile_name argument, and save it to the `output_path`.

    This function will check if given files exist. Missing files will trigger erros.
    
    Parameters
    ----------
    path_to_model: str
        The path of GraphStorm's `model.bin` file.
    path_to_yaml: str
        The path of the YAML file (training/inference hyperparameter configuration file) generated
        by GraphStorm during model training.
    path_to_json: str
        The path of JSON file (graph schema and processing configuration file) generated by
        GraphStorm gconstruct or GSProcessing.
    path_to_entry: str
        The path of the entry point file for a specific task. The file will be put into a
        sub-folder, named 'code'.
    output_path: str
        The folder where the output tar archive will be saved. If not provided, will raise an
        error.
    output_tarfile_name: str
        The name of the tar archive. Default is `model`.

    """
    # check if files exist
    assert os.path.exists(path_to_model), f'The model file, {path_to_model}, does not exist.'
    assert os.path.isfile(path_to_model), f'The model file, {path_to_model}, should be a file \
                                            path, but got a folder.'
    assert os.path.exists(path_to_yaml), f'The model configuration YAML file, {path_to_yaml}, \
                                           does not exist.'
    assert os.path.isfile(path_to_yaml), f'The model configuration YAML file, {path_to_yaml}, \
                                           should be a file path, but got a folder.'
    assert os.path.exists(path_to_json), f'The graph metadata JSON file, {path_to_json}, does \
                                           not exist.'
    assert os.path.isfile(path_to_json), f'The graph metadata JSON file, {path_to_json}, should \
                                           be a file path, but got a folder.'
    # TODO(JianZhang): If entry point file requires modules out of GraphStorm, we need to check
    #                  and support a list of strings (multiple files) and a folder (multiple files
    #                  in a folder).
    assert os.path.exists(path_to_entry), f'The SageMaker entry point file, {path_to_entry}, does \
                                           not exist.'
    assert os.path.isfile(path_to_entry), f'The SageMaker entry point file, {path_to_entry}, \
                                           should be a file path, but got a folder.'

    # create the output folder if not exist.
    if os.path.exists(output_path):
        assert os.path.isdir(output_path), 'Output path should be a folder name, but got a ' + \
                                           f'file {output_path}.'
    else:
        os.makedirs(output_path, exist_ok=True)
    os.makedirs(os.path.join(output_path, 'code'), exist_ok=True)

    # copy the artifacts files to output_path
    shutil.copy(path_to_entry, os.path.join(output_path, 'code', os.path.basename(path_to_entry)))
    if output_path != os.path.dirname(path_to_model):
        shutil.copy(path_to_model, os.path.join(output_path, os.path.basename(path_to_model)))
    if output_path != os.path.dirname(path_to_yaml):
        shutil.copy(path_to_yaml, os.path.join(output_path, os.path.basename(path_to_yaml)))
    if output_path != os.path.dirname(path_to_json):
        shutil.copy(path_to_json, os.path.join(output_path, os.path.basename(path_to_json)))

    output_file = os.path.join(output_path, output_tarfile_name + '.tar.gz')
    with tarfile.open(output_file, 'w:gz') as tar:
        tar.add(output_path, arcname='')

    return output_file


def parse_s3_uri(s3_uri):
    """ Parse the given S3 uri and checkif it is a valid S3 uri.

    Parameters:
    -----------
    s3_uri: str
        The given S3 uri string. Should start with "https://" or "s3://". If not, will raise
        an assertion error.
    
    Returns:
    --------
    bucket_name: str
        The name of S3 bucket in the uri.
    key: str
        The S3 object in the uri.
    """
    parsed_uri = urlparse(s3_uri)
    assert parsed_uri.scheme in ['s3', 'https'], (f'Incorrect S3 uri was given {s3_uri}')

    bucket_name = parsed_uri.netloc
    key = parsed_uri.path.lstrip('/')
    return bucket_name, key


def extract_ecr_region(ecr_uri):
    """Extracts the region string from an ECR URI
    
    ECR URIs are formatted as ``<account_id>.dkr.ecr.<region>.amazonaws.com``. The region string
    contains letters, digits, and hyphens only, and is between '.ecr.' and '.amazonaws.com'.
    
    Parameters
    ----------
    ecr_uri: str
        The URI of an ECR Docker image.

    Returns
    -------
    region: str
        The region string extraced from the given URI of an ECR Docker image, or None if no such
        region string.
    """
    pattern = re.compile(r'\.ecr\.([a-z0-9-]+)\.amazonaws\.com')
    is_match = re.search(pattern, ecr_uri)
    if is_match:
        region = is_match.group(1)
    else:
        region = None

    return region


def check_tarfile_s3_object(s3_url):
    """ Check the object in the given S3 url
    1. if the object exists;
    2. if the object ends with ".tar.gz".

    Parameters:
    -----------
    s3_url: str
        The given S3 url string. The object in the url should be an object ending with `.tar.gz`.
        If not, will raise an assertion error.

    Returns:
    --------
    bool: 
        True if the S3 object exists and ends with ``.tar.gz``, False otherwise.
    """
    bucket_name, key = parse_s3_uri(s3_url)
    s3_object = key.split('/')[-1]
    assert s3_object.endswith('.tar.gz'), (f'The S3 object, {s3_url}, is not a compressed ' + \
                                            'tar file.')

    # create a boto3 S3 client
    s3_client = boto3.client('s3')

    # check if exists
    try:
        s3_client.head_object(Bucket=bucket_name, Key=key)
        return True
    except ClientError:
        return False


def upload_data_to_s3(s3_path, data_path, sagemaker_session):
    """ Upload data into S3

    Parameters
    ----------
    s3_path: str
        S3 uri to upload the data
    data_path: str
        Local data path.
    sagemaker_session: sagemaker.session.Session
        sagemaker_session to run upload
    
    Returns
    -------
    ret: str
        The S3 url of the uploaded data.
    """
    try:
        ret = S3Uploader.upload(data_path, s3_path, sagemaker_session=sagemaker_session)
    except Exception as err: # pylint: disable=broad-except
        logging.error("Can not upload data into %s", s3_path)
        raise RuntimeError(f"Can not upload data into {s3_path}. {err}")
    return ret


def check_name_format(name_str):
    r""" Check if given name follow AWS naming format
    
    The name should follow AWS' naming regular expression: ^[a-zA-Z0-9]([\-a-zA-Z0-9]*[a-zA-Z0-9])$
    It means the string must start with a letter or digit. In the middle, it could be one or more
    hyphons, letters, or digits. And the string must end with a letter or digit.
    
    Parameters:
    -----------
    name_str: str
        The name string to be checked.
    
    Returns:
    --------
    str
        The same name string as input if it match the regular expression. Otherwise will
        raise a ValueError.

    """
    pattern = re.compile(r"^[a-zA-Z0-9]([\-a-zA-Z0-9]*[a-zA-Z0-9])$")
    if not re.match(pattern, name_str):
        raise ArgumentTypeError(f'Value {name_str} failed to satisfy regular ' + \
                                'expression pattern: ' + \
                                r'^[a-zA-Z0-9]([\-a-zA-Z0-9]*[a-zA-Z0-9])$.')
    return name_str


def has_tokenize_transformation(graph_config):
    """ A simple sanity check of tokenize_hf transformation in graph construction JSON file

    This function only check if there is a tokenize_hf feature transformation. For a
    comprehensive solution, please use the
    `graphstorm.dataloading.metadata.config_json_sanity_check` function.
    Because we design this sagemaker package to be independent to the core code of graphstorm,
    this function does not import the config_json_sanity_check function, but implements a
    simple and quick check for the tokenize transformation.
    
    TODO(Jian): remove this function once GraphStorm supports tokenize transformation in real-time
        inference pipeline.

    Parameters
    ----------
    graph_config: dict or str
        A dict from a graph construciton JSON file, or a string from json.dumps() function.

    Return
    ------
    has_tokenize_transform: bool
        A boolean value. If the JSON oject contains at least one tokenize transformation, return
        True, otherwise False.
    """
    # convert to string if the input is a dict
    if isinstance(graph_config, dict):
        json_str = json.dumps(graph_config)
    else:
        json_str = graph_config

    # for GCons: "transform": {"name": "tokenize_hf", ...}
    gcons_pattern = r'"transform"\s*:\s*{[^}]*"name"\s*:\s*"tokenize_hf"'
    
    # for GSProcessing: "transformation": {..., "kwargs": {"action": "tokenize_hf", ...}, ...}
    gsproc_pattern = \
        r'"transformation"\s*:\s*{[^}]*"kwargs"\s*:\s*{[^}]*"action"\s*:\s*"tokenize_hf"'
    
    # Check both patterns
    gcons_match = re.search(gcons_pattern, json_str)
    gsproc_match = re.search(gsproc_pattern, json_str)
    
    return gcons_match is not None or gsproc_match is not None