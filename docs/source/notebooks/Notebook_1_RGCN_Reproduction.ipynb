{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Reproduce RGCN Model with APIs (for review)\n",
    "\n",
    "This notebook is the first in a series that demonstrates how to use GraphStorm's APIs to create users' own graph machine learning setup by leveraging GraphStorm's easy-to-use and great scalability features. All of these notebooks are designed to run on GraphStorm's Standalone mode, i.e., in a single Linux machine with CPUs and GPUs. \n",
    "\n",
    "In this notebook, we willl reproduce the GraphStorm RGCN model with the nessessary APIs and use it to conduct a node classification task on the ACM dataset. By playing with this notebook, users will be able to get familiar with these APIs.\n",
    "\n",
    "### Prerequsites\n",
    "\n",
    "- GraphStorm installed using pip. Please find [more details on installation of GraphStorm](https://graphstorm.readthedocs.io/en/latest/install/env-setup.html#setup-graphstorm-with-pip-packages).\n",
    "- ACM data created in the [Notebook 0: Data Prepare](https://graphstorm.readthedocs.io/en/latest/notebooks/Notebook_0_Data_Prepare.html), and is stored in the `./acm_gs_1p/` folder.\n",
    "- Installation of supporting libraries, e.g., matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import supporting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup log level\n",
    "import logging\n",
    "logging.basicConfig(level=20)\n",
    "\n",
    "# Import GraphStorm APIs\n",
    "import graphstorm as gs\n",
    "from graphstorm.trainer import GSgnnNodePredictionTrainer\n",
    "from graphstorm.dataloading import GSgnnNodeTrainData, GSgnnNodeDataLoader, GSgnnNodeInferData\n",
    "from graphstorm.model import (GSgnnNodeModel,\n",
    "                              GSNodeEncoderInputLayer,\n",
    "                              RelationalGCNEncoder,\n",
    "                              EntityClassifier,\n",
    "                              ClassifyLossFunc)\n",
    "from graphstorm.inference import GSgnnNodePredictionInferrer\n",
    "from graphstorm.eval import GSgnnAccEvaluator\n",
    "from graphstorm.tracker import GSSageMakerTaskTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 0. Initialize the GraphStorm Standalone Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs.initialize(ip_config=None, backend='gloo')       # TODO: set logging level\n",
    "device = gs.utils.setup_device(0)\n",
    "\n",
    "# gs.initialize(**, device=)\n",
    "# device = gs.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup Graph Data Information\n",
    "\n",
    "First, let's setup the information of the ACM graph data for GraphStorm model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acm_graph_config = './acm_gs_1p/acm.json'\n",
    "graph_name = 'acm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup GraphStorm Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:part 0, train: 9999, val: 1249, test: 1249\n"
     ]
    }
   ],
   "source": [
    "# create a GraphStorm Dataset for the ACM graph data \n",
    "train_data = GSgnnNodeTrainData(graph_name=graph_name,         # need to remove this argument and get graph name from partion json file\n",
    "                              part_config=acm_graph_config,\n",
    "                              train_ntypes=['paper'],            # TODO: node type/s list for training\n",
    "                              eval_ntypes=['paper'],             # TODO: node type/s list for evaluation and testing\n",
    "                              node_feat_field={'author':['feat'], 'paper':['feat'],'subject':['feat']}, # TODO: paste the logic of handline input here?\n",
    "                              label_field='label')             # TODO: diction of name of the label field, \n",
    "\n",
    "# TODO: why not just have a GSgnnNodeData?? \n",
    "# Not do it in this release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup data loaders for training, validation, and test\n",
    "\n",
    "# TODO: define fanout as a variable\n",
    "fanout = [5,5]\n",
    "\n",
    "# TODO: one line to build three dataloaders\n",
    "train_dataloader = GSgnnNodeDataLoader(dataset=train_data,\n",
    "                                       target_idx=train_data.train_idxs,\n",
    "                                       fanout=[5,5],\n",
    "                                       batch_size=64,\n",
    "                                       device=device,                    # TODO: remove this argument, but get it from gs.get_device()\n",
    "                                       train_task=True)\n",
    "val_dataloader = GSgnnNodeDataLoader(dataset=train_data,\n",
    "                                     target_idx=train_data.val_idxs,\n",
    "                                     fanout=[5,5],\n",
    "                                     batch_size=256,\n",
    "                                     device=device,\n",
    "                                     train_task=False)\n",
    "# test_dataloader = GSgnnNodeDataLoader(dataset=train_data,\n",
    "#                                       target_idx=train_data.test_idxs,\n",
    "#                                       fanout=[5,5],\n",
    "#                                       batch_size=256,\n",
    "#                                       device=device,\n",
    "#                                       train_task=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reproduce the GraphStorm RGCN Model for Node Classification \n",
    "\n",
    "Next, we use a set of GraphStorm APIs to reproduce the built-in RGCN model.\n",
    "\n",
    "A GraphStorm model should contain the following components: \n",
    "- Input encoder for nodes (and optionally edges): process and project input features and embeddings into a certain dimension;\n",
    "- GNN encoder: performs message-passing on projected node/edge inputs;\n",
    "- Decoder: specific for tasks on the graph.\n",
    "\n",
    "We can see the following codes set up a `GSgnnNodeModel` model composed of `GSNodeEncoderInputLayer`, `RelationalGCNEncoder`, `EntityClassifier` step-by-step. One can also replace individual components/layers with a custom model for development purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: build GSgnnRGCNNodeClassificationModel() ...\n",
    "\n",
    "# create a GraphStorm model for node tasks\n",
    "model = GSgnnNodeModel(alpha_l2norm=0.)    # set an alpha_l2norm default value\n",
    "\n",
    "# set an input layer encoder\n",
    "# TODO: replace feat_size, with train_data.get_feat_size()\n",
    "encoder = GSNodeEncoderInputLayer(g=train_data.g,\n",
    "                                  feat_size={'author':256, 'paper':256, 'subject':256}, \n",
    "                                  embed_size=64)\n",
    "model.set_node_input_encoder(encoder)\n",
    "\n",
    "# set a GNN encoder\n",
    "gnn_encoder = RelationalGCNEncoder(g=train_data.g,\n",
    "                                   h_dim=64,\n",
    "                                   out_dim=128,\n",
    "                                   num_hidden_layers=len(fanout)-1)    # MUST be len(fanout)-1 !!\n",
    "model.set_gnn_encoder(gnn_encoder)\n",
    "\n",
    "# set a decoder specific to node-classification task\n",
    "decoder = EntityClassifier(in_dim=128,\n",
    "                           num_classes=14,\n",
    "                           multilabel=False)\n",
    "model.set_decoder(decoder)\n",
    "\n",
    "# classification loss function\n",
    "model.set_loss_func(ClassifyLossFunc(multilabel=False))\n",
    "\n",
    "# initialize model's optimizer\n",
    "model.init_optimizer(lr=0.001,                           # 1. Can we let model to init the optimizer automatically??\n",
    "                     sparse_optimizer_lr=0.01,           # 2. TODO:  have default settings for the sparse lr and WD.\n",
    "                     weight_decay=0)\n",
    "\n",
    "# (Optional) uncomment to display the model architecture\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup GraphStorm Training Pipeline\n",
    "\n",
    "GraphStorm uses its Trainers to train the RGCN model. It handles:\n",
    "1. model training/evaluation loops\n",
    "2. saving and recording best performed models \n",
    "3. early-stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a GraphStorm node task trainer for the RGCN model\n",
    "trainer = GSgnnNodePredictionTrainer(model)   # TODO: set device as argument of trainers\n",
    "\n",
    "# setup device for the trainer\n",
    "trainer.setup_device(device=device)          # TODO: remove, \n",
    "\n",
    "# setup evaluator for the trainer:\n",
    "# evaluator = GSgnnAccEvaluator(eval_frequency=100,\n",
    "#                               eval_metric=['accuracy'],\n",
    "#                               multilabel=False)\n",
    "\n",
    "# setup a task tracker to output running information\n",
    "task_tracker = GSSageMakerTaskTracker(log_report_frequency=evaluator.eval_frequency)     # TODO: set a default task tracker.\n",
    "trainer.setup_task_tracker(task_tracker)\n",
    "\n",
    "# trainer.setup_evaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Part 0 | Epoch 00000 | Batch 000 | Loss: 1.0089 | Time: 0.0309\n",
      "INFO:root:Part 0 | Epoch 00000 | Batch 020 | Loss: 0.8512 | Time: 0.0320\n",
      "INFO:root:Part 0 | Epoch 00000 | Batch 040 | Loss: 0.8564 | Time: 0.0336\n",
      "INFO:root:Part 0 | Epoch 00000 | Batch 060 | Loss: 0.8211 | Time: 0.0325\n",
      "INFO:root:Part 0 | Epoch 00000 | Batch 080 | Loss: 1.2406 | Time: 0.0344\n",
      "INFO:root:Step 100 | Train loss: 0.8107\n",
      "INFO:root:Part 0 | Epoch 00000 | Batch 100 | Loss: 0.9378 | Time: 0.0331\n",
      "INFO:root:Part 0 | Epoch 00000 | Batch 120 | Loss: 1.1321 | Time: 0.0345\n",
      "INFO:root:Part 0 | Epoch 00000 | Batch 140 | Loss: 1.3479 | Time: 0.0297\n",
      "INFO:root:Epoch 0 take 5.157 seconds\n",
      "INFO:root:successfully save the model to nc_model/epoch-0\n",
      "INFO:root:Time on save model: 0.002 seconds\n",
      "INFO:root:Part 0 | Epoch 00001 | Batch 000 | Loss: 0.7831 | Time: 0.0282\n",
      "INFO:root:Part 0 | Epoch 00001 | Batch 020 | Loss: 1.0633 | Time: 0.0353\n",
      "INFO:root:Part 0 | Epoch 00001 | Batch 040 | Loss: 1.1163 | Time: 0.0323\n",
      "INFO:root:Step 200 | Train loss: 1.0735\n",
      "INFO:root:Part 0 | Epoch 00001 | Batch 060 | Loss: 0.8277 | Time: 0.0349\n",
      "INFO:root:Part 0 | Epoch 00001 | Batch 080 | Loss: 0.9312 | Time: 0.0336\n",
      "INFO:root:Part 0 | Epoch 00001 | Batch 100 | Loss: 1.1532 | Time: 0.0328\n",
      "INFO:root:Part 0 | Epoch 00001 | Batch 120 | Loss: 0.8488 | Time: 0.0324\n",
      "INFO:root:Part 0 | Epoch 00001 | Batch 140 | Loss: 1.0632 | Time: 0.0282\n",
      "INFO:root:Step 300 | Train loss: 1.0363\n",
      "INFO:root:Epoch 1 take 5.121 seconds\n",
      "INFO:root:successfully save the model to nc_model/epoch-1\n",
      "INFO:root:Time on save model: 0.002 seconds\n",
      "INFO:root:Part 0 | Epoch 00002 | Batch 000 | Loss: 0.8956 | Time: 0.0282\n",
      "INFO:root:Part 0 | Epoch 00002 | Batch 020 | Loss: 0.7935 | Time: 0.0353\n",
      "INFO:root:Part 0 | Epoch 00002 | Batch 040 | Loss: 0.9575 | Time: 0.0317\n",
      "INFO:root:Part 0 | Epoch 00002 | Batch 060 | Loss: 0.8919 | Time: 0.0327\n",
      "INFO:root:Part 0 | Epoch 00002 | Batch 080 | Loss: 0.6752 | Time: 0.0326\n",
      "INFO:root:Step 400 | Train loss: 0.9076\n",
      "INFO:root:Part 0 | Epoch 00002 | Batch 100 | Loss: 0.9341 | Time: 0.0333\n",
      "INFO:root:Part 0 | Epoch 00002 | Batch 120 | Loss: 0.8368 | Time: 0.0342\n",
      "INFO:root:Part 0 | Epoch 00002 | Batch 140 | Loss: 0.7462 | Time: 0.0364\n",
      "INFO:root:Epoch 2 take 5.248 seconds\n",
      "INFO:root:successfully save the model to nc_model/epoch-2\n",
      "INFO:root:Time on save model: 0.002 seconds\n",
      "INFO:root:Part 0 | Epoch 00003 | Batch 000 | Loss: 0.7165 | Time: 0.0281\n",
      "INFO:root:Part 0 | Epoch 00003 | Batch 020 | Loss: 0.8369 | Time: 0.0322\n",
      "INFO:root:Step 500 | Train loss: 1.0544\n",
      "INFO:root:Part 0 | Epoch 00003 | Batch 040 | Loss: 0.8516 | Time: 0.0320\n",
      "INFO:root:Part 0 | Epoch 00003 | Batch 060 | Loss: 0.8575 | Time: 0.0355\n",
      "INFO:root:Part 0 | Epoch 00003 | Batch 080 | Loss: 0.7423 | Time: 0.0345\n",
      "INFO:root:Part 0 | Epoch 00003 | Batch 100 | Loss: 0.8670 | Time: 0.0329\n",
      "INFO:root:Part 0 | Epoch 00003 | Batch 120 | Loss: 0.6500 | Time: 0.0328\n",
      "INFO:root:Step 600 | Train loss: 0.8513\n",
      "INFO:root:Part 0 | Epoch 00003 | Batch 140 | Loss: 0.7855 | Time: 0.0335\n",
      "INFO:root:Epoch 3 take 5.156 seconds\n",
      "INFO:root:successfully save the model to nc_model/epoch-3\n",
      "INFO:root:Time on save model: 0.002 seconds\n",
      "INFO:root:Part 0 | Epoch 00004 | Batch 000 | Loss: 0.6825 | Time: 0.0306\n",
      "INFO:root:Part 0 | Epoch 00004 | Batch 020 | Loss: 0.6750 | Time: 0.0342\n",
      "INFO:root:Part 0 | Epoch 00004 | Batch 040 | Loss: 0.9210 | Time: 0.0329\n",
      "INFO:root:Part 0 | Epoch 00004 | Batch 060 | Loss: 0.7162 | Time: 0.0280\n",
      "INFO:root:Step 700 | Train loss: 0.6954\n",
      "INFO:root:Part 0 | Epoch 00004 | Batch 080 | Loss: 0.6618 | Time: 0.0323\n",
      "INFO:root:Part 0 | Epoch 00004 | Batch 100 | Loss: 0.9696 | Time: 0.0326\n",
      "INFO:root:Part 0 | Epoch 00004 | Batch 120 | Loss: 0.7855 | Time: 0.0346\n",
      "INFO:root:Part 0 | Epoch 00004 | Batch 140 | Loss: 0.8014 | Time: 0.0349\n",
      "INFO:root:Epoch 4 take 5.228 seconds\n",
      "INFO:root:successfully save the model to nc_model/epoch-4\n",
      "INFO:root:Time on save model: 0.002 seconds\n",
      "INFO:root:Part 0 | Epoch 00005 | Batch 000 | Loss: 0.6076 | Time: 0.0276\n",
      "INFO:root:Step 800 | Train loss: 0.6605\n",
      "INFO:root:Part 0 | Epoch 00005 | Batch 020 | Loss: 0.7602 | Time: 0.0349\n",
      "INFO:root:Part 0 | Epoch 00005 | Batch 040 | Loss: 0.6843 | Time: 0.0328\n",
      "INFO:root:Part 0 | Epoch 00005 | Batch 060 | Loss: 0.5683 | Time: 0.0327\n",
      "INFO:root:Part 0 | Epoch 00005 | Batch 080 | Loss: 0.5494 | Time: 0.0349\n",
      "INFO:root:Part 0 | Epoch 00005 | Batch 100 | Loss: 0.6770 | Time: 0.0343\n",
      "INFO:root:Step 900 | Train loss: 0.6153\n",
      "INFO:root:Part 0 | Epoch 00005 | Batch 120 | Loss: 0.7942 | Time: 0.0345\n",
      "INFO:root:Part 0 | Epoch 00005 | Batch 140 | Loss: 0.8129 | Time: 0.0344\n",
      "INFO:root:Epoch 5 take 5.254 seconds\n",
      "INFO:root:successfully save the model to nc_model/epoch-5\n",
      "INFO:root:Time on save model: 0.002 seconds\n",
      "INFO:root:Part 0 | Epoch 00006 | Batch 000 | Loss: 0.7098 | Time: 0.0264\n",
      "INFO:root:Part 0 | Epoch 00006 | Batch 020 | Loss: 0.5076 | Time: 0.0340\n",
      "INFO:root:Part 0 | Epoch 00006 | Batch 040 | Loss: 0.7289 | Time: 0.0326\n",
      "INFO:root:Step 1000 | Train loss: 0.7748\n",
      "INFO:root:Part 0 | Epoch 00006 | Batch 060 | Loss: 0.5180 | Time: 0.0334\n",
      "INFO:root:Part 0 | Epoch 00006 | Batch 080 | Loss: 0.6556 | Time: 0.0316\n",
      "INFO:root:Part 0 | Epoch 00006 | Batch 100 | Loss: 0.7853 | Time: 0.0320\n",
      "INFO:root:Part 0 | Epoch 00006 | Batch 120 | Loss: 0.5171 | Time: 0.0323\n",
      "INFO:root:Part 0 | Epoch 00006 | Batch 140 | Loss: 0.5987 | Time: 0.0329\n",
      "INFO:root:Epoch 6 take 5.101 seconds\n",
      "INFO:root:successfully save the model to nc_model/epoch-6\n",
      "INFO:root:Time on save model: 0.002 seconds\n",
      "INFO:root:Step 1100 | Train loss: 0.6410\n",
      "INFO:root:Part 0 | Epoch 00007 | Batch 000 | Loss: 0.6410 | Time: 0.0337\n",
      "INFO:root:Part 0 | Epoch 00007 | Batch 020 | Loss: 0.4972 | Time: 0.0319\n",
      "INFO:root:Part 0 | Epoch 00007 | Batch 040 | Loss: 0.8013 | Time: 0.0334\n",
      "INFO:root:Part 0 | Epoch 00007 | Batch 060 | Loss: 0.3858 | Time: 0.0325\n",
      "INFO:root:Part 0 | Epoch 00007 | Batch 080 | Loss: 0.5199 | Time: 0.0326\n",
      "INFO:root:Step 1200 | Train loss: 0.5301\n",
      "INFO:root:Part 0 | Epoch 00007 | Batch 100 | Loss: 0.5301 | Time: 0.0350\n",
      "INFO:root:Part 0 | Epoch 00007 | Batch 120 | Loss: 0.6742 | Time: 0.0351\n",
      "INFO:root:Part 0 | Epoch 00007 | Batch 140 | Loss: 0.6946 | Time: 0.0350\n",
      "INFO:root:Epoch 7 take 5.184 seconds\n",
      "INFO:root:successfully save the model to nc_model/epoch-7\n",
      "INFO:root:Time on save model: 0.002 seconds\n",
      "INFO:root:Part 0 | Epoch 00008 | Batch 000 | Loss: 0.5220 | Time: 0.0295\n",
      "INFO:root:Part 0 | Epoch 00008 | Batch 020 | Loss: 0.4425 | Time: 0.0351\n",
      "INFO:root:Part 0 | Epoch 00008 | Batch 040 | Loss: 0.5816 | Time: 0.0358\n",
      "INFO:root:Step 1300 | Train loss: 0.5419\n",
      "INFO:root:Part 0 | Epoch 00008 | Batch 060 | Loss: 0.4747 | Time: 0.0332\n",
      "INFO:root:Part 0 | Epoch 00008 | Batch 080 | Loss: 0.4879 | Time: 0.0304\n",
      "INFO:root:Part 0 | Epoch 00008 | Batch 100 | Loss: 0.5112 | Time: 0.0351\n",
      "INFO:root:Part 0 | Epoch 00008 | Batch 120 | Loss: 0.5414 | Time: 0.0322\n",
      "INFO:root:Part 0 | Epoch 00008 | Batch 140 | Loss: 0.6581 | Time: 0.0327\n",
      "INFO:root:Step 1400 | Train loss: 0.7550\n",
      "INFO:root:Epoch 8 take 5.148 seconds\n",
      "INFO:root:successfully save the model to nc_model/epoch-8\n",
      "INFO:root:Time on save model: 0.002 seconds\n",
      "INFO:root:Part 0 | Epoch 00009 | Batch 000 | Loss: 0.4115 | Time: 0.0324\n",
      "INFO:root:Part 0 | Epoch 00009 | Batch 020 | Loss: 0.4650 | Time: 0.0348\n",
      "INFO:root:Part 0 | Epoch 00009 | Batch 040 | Loss: 0.5970 | Time: 0.0327\n",
      "INFO:root:Part 0 | Epoch 00009 | Batch 060 | Loss: 0.7418 | Time: 0.0329\n",
      "INFO:root:Part 0 | Epoch 00009 | Batch 080 | Loss: 0.5833 | Time: 0.0346\n",
      "INFO:root:Step 1500 | Train loss: 0.5451\n",
      "INFO:root:Part 0 | Epoch 00009 | Batch 100 | Loss: 0.4759 | Time: 0.0285\n",
      "INFO:root:Part 0 | Epoch 00009 | Batch 120 | Loss: 0.6045 | Time: 0.0284\n",
      "INFO:root:Part 0 | Epoch 00009 | Batch 140 | Loss: 0.5613 | Time: 0.0369\n",
      "INFO:root:Epoch 9 take 5.230 seconds\n",
      "INFO:root:successfully save the model to nc_model/epoch-9\n",
      "INFO:root:Time on save model: 0.002 seconds\n",
      "INFO:root:Peak RAM Mem alloc: 569.1953 MB\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the trainer using fit() function\n",
    "trainer.fit(train_loader=train_dataloader,\n",
    "            # val_loader=val_dataloader,\n",
    "            # test_loader=test_dataloader,\n",
    "            num_epochs=10,\n",
    "            save_model_path='nc_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualize Model Performance History\n",
    "\n",
    "Next, we examine the model performance on the validation and testing sets over the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# extract evaluation history of metrics from the trainer's evaluator:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m val_metrics, test_metrics \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_metric, test_metric \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m:\n\u001b[1;32m      4\u001b[0m     val_metrics\u001b[38;5;241m.\u001b[39mappend(val_metric[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m     test_metrics\u001b[38;5;241m.\u001b[39mappend(test_metric[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "# extract evaluation history of metrics from the trainer's evaluator:\n",
    "val_metrics, test_metrics = [], []\n",
    "for val_metric, test_metric in trainer.evaluator.history:\n",
    "    val_metrics.append(val_metric['accuracy'])\n",
    "    test_metrics.append(test_metric['accuracy'])\n",
    "\n",
    "# plot the performance curves\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(val_metrics, label='val')\n",
    "ax.plot(test_metrics, label='test')\n",
    "ax.set(xlabel='Epoch', ylabel='Accuracy')\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Inference with the Trained Model\n",
    "\n",
    "GraphStorm automatically save the best performaned model in the given `save_model_path` argument. We can first find out what is the best model and its path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path: nc_model/epoch-9\n"
     ]
    }
   ],
   "source": [
    "# after training, the best model is saved to disk:\n",
    "best_model_path = trainer.get_best_model_path()\n",
    "print('Best model path:', best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:successfully load the model from nc_model/epoch-9\n",
      "INFO:root:Time on load model: 0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "# we can restore the model from the saved path using the model's restore_model() function.\n",
    "model.restore_model(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for inference, we use the same ACM graph\n",
    "infer_data = GSgnnNodeInferData(graph_name=graph_name,\n",
    "                                part_config=acm_graph_config,\n",
    "                                eval_ntypes='paper',\n",
    "                                node_feat_field={'author':['feat'], 'paper':['feat'],'subject':['feat']},\n",
    "                                label_field='label')\n",
    "\n",
    "# Setup dataloader for the inference dataset\n",
    "infer_dataloader = GSgnnNodeDataLoader(dataset=infer_data,\n",
    "                                       target_idx=infer_data.test_idxs,\n",
    "                                       fanout=[50,50],\n",
    "                                       batch_size=100,\n",
    "                                       device=device,\n",
    "                                       train_task=False)\n",
    "\n",
    "# Create an Inferrer object\n",
    "infer = GSgnnNodePredictionInferrer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:save embeddings pf paper to infer/embeddings\n",
      "INFO:root:Writing GNN embeddings to infer/embeddings in pytorch format.\n"
     ]
    }
   ],
   "source": [
    "# Run inference on the inference dataset\n",
    "infer.infer(infer_dataloader,\n",
    "            save_embed_path='infer/embeddings',\n",
    "            save_prediction_path='infer/predictions',\n",
    "            use_mini_batch_infer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 640K\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 626K Jan 26 20:54 embed-00000.pt\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  11K Jan 26 20:54 embed_nids-00000.pt\n",
      "total 84K\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 70K Jan 26 20:54 predict-00000.pt\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 11K Jan 26 20:54 predict_nids-00000.pt\n"
     ]
    }
   ],
   "source": [
    "# The GNN embeddings and predictions on the inference graph are saved to the folder named after the target_ntype\n",
    "!ls -lh infer/embeddings/paper\n",
    "!ls -lh infer/predictions/paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsf",
   "language": "python",
   "name": "gsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
