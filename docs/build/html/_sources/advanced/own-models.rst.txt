.. _use-own-models:

Use Your Own Models
======================
Currently GraphStorm has two built-in GNN models, i.e., the RGCN and the RGAT model, for heterogenous graphs. If users want to further explore different GNN models and leverage the GraphStorm's ease-of-use and scalability, you can create your own GNN models according to the GraphStorm's customer model APIs. This tutorial will explain in detail how to do this, with a runnable `example <https://github.com/awslabs/graphstorm/tree/main/examples/customized_models/HGT>`_ that customizes the HGT DGL model implementation.

.. _use-own-models-prerequisites:

Prerequisites
---------------
Before following GraphStorm's customized model APIs, please make sure your GNN models meet the prerequisites.

Use DGL to implement your GNN models
.....................................
The GraphStorm Framework relies on the `DGL library <https://www.dgl.ai/>`_ to implement and run GNN models. Particularly, the GraphStorm's scalability comes from the DGL's distributed libraries. For this reason, your GNN models should be implemented with the DGL Library. You can learn how to do this via the DGL's `User Guide <https://docs.dgl.ai/guide/index.html>`_. In addition, there are many `GNN model examples <https://github.com/dmlc/dgl/tree/master/examples>`_ implemented by the DGL community. Please explore these materials to check if there is any model that may meet your requirements.

Modify you GNN models to use mini-batch training/inference
..........................................................
Many existing GNN models were implemented for running on popular academic graphs, which, compared to enterprise-level graphs, are relatively small and lack node/edge features. Therefore, implementors use the full-graph training/inference mode, i.e., feed the entire graph along with its node/edge features into GNN models in one epoch. When dealing with large graphs, this mode will fail due to either the limits of the GPUs' memory, or the slow speed if using CPUs.

In order to tackle large graphs, we can change GNN models to perform stochastic mini-batch training, where we do not have to fit the graphs and their features of all the nodes/edges into GPUs. You can learn how to modify GNN models into mini-batch training/inference mode via the `DGL User Guide Chapter 6 <https://docs.dgl.ai/en/1.0.x/guide/minibatch.html>`_. For examples of the different implementations between full-graph mode and mini-batch mode, please look for DGL model examples, in which mini-batch mode files normally have a file name ended with "_mb" string, like the `RGCN model <https://github.com/dmlc/dgl/blob/master/examples/pytorch/rgcn-hetero/entity_classify_mb.py>`_, or file names including "dist" string, like the `GraphSage distributed model <https://github.com/dmlc/dgl/blob/master/examples/pytorch/graphsage/dist/train_dist.py#L26>`_.

Learn how to run GraphStorm in a Docker environment
......................................................
Currently GraphStorm runs on Docker environment. The rest of the tutorial assumes execution wthin the GraphStorm Docker container. Please refer to the first two sections in the :ref:`Environment Setup<setup>` to learn how to run GraphStorm in a Docker environment, and set up your environment.

Modifications required for customer models
---------------------------------------------------------------
Step 1: Convert your graph data into required format
.....................................................
Users can follow the :ref:`User Your Own Graph Data <use-own-data>` tutorial to prepare your graph data for GraphStorm.

Step 2: Modify your GNN model to use the GraphStorm APIs
.........................................................
To plug GNN models into GraphStorm, you need to use the GraphStorm model APIs. The key model APIs are the class `GSgnnNodeModelBase <https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/model/node_gnn.py#L76>`_, `GSgnnEdgeModelBase <https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/model/edge_gnn.py#L80>`_, and `GSgnnLinkPredictionModelBase <https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/model/lp_gnn.py#L58>`_. Your GNN models should inherit one of the three classes depending on your task.

Here we use the `DGL HGT example model <https://github.com/dmlc/dgl/blob/master/examples/pytorch/hgt/model.py>`_ to demonstrate how to modify the GNN models.

.. code-block:: python

    class HGT(nn.Module):
        def __init__(self, ......):
            super(HTG, self).__init__()
            ......

        def forward(self, G, out_key):
            h = {}
            for ntype in G.ntypes:
                n_id = self.node_dict[ntype]
                h[ntype] = F.gelu(self.adapt_ws[n_id](G.nodes[ntype].data["inp"]))
            for i in range(self.n_layers):
                h = self.gcs[i](G, h)
            return self.out(h[out_key])

The original HGT model implement uses full-graph training and inference mode. Its ``forward()`` function takes a DGL graph, `G` and the to-be predicted node type, `out_key` as input arguments.

As the :ref:`Prerequisites <use-own-models-prerequisites>` required, we revise this model to use mini-batch training and inference mode as shown below.

.. code-block:: python

    class HGT_mb(nn.Module):
        def __init__(self, ......)
            super(HGT_mb, self).__init__()
            ......

        def forward(self, mfgs, n_feats_dict, out_ntype):
            h = {}
            for ntype in mfgs[0].ntypes:
                if self.adapt_ws[ntype] is None:
                    n_id = self.node_dict[ntype]
                    emb_id = self.ntype_id_map[n_id]
                    n_embed = self.ntype_embed(torch.Tensor([emb_id] * mfgs[0].num_nodes(ntype)).long().to(self.device))
                else:
                    n_embed = self.adapt_ws[ntype](n_feats_dict[ntype])
                h[ntype] = F.gelu(n_embed)

            for i in range(self.n_layers):
                h = self.gcs[i](mfgs[i], h)

            return self.out(h[out_ntype])

To make this dummy model work in GraphStorm, we need replace the PyTorch ``nn.Module`` with GraphStorm's ``GSgnnNodeModelBase`` and implement required functions.

The ``GSgnnNodeModelBase`` class, which is also a PyTorch Module, has three required functions that users' own GNN model need to implement, including ``forward(self, blocks, node_feats, edge_feats, labels, input_nodes)``, ``predict(self, blocks, node_feats, edge_feats, input_nodes)``, and ``create_optimizer(self)``.

The ``forward()`` function is similar to the PyTorch Module's ``forward()`` function except that its input arguments **MUST** include blocks and labels, node features, edge features, or input_nodes are optional only if your GNN model needs them. Unlike common cases where forward function returns logits computed by models, the return value of ``GSgnnNodeModelBase``'s ``forward()`` should be a loss value, which the GraphStorm will use to perform backward operations. Because of this change, you need to include a loss function within your GNN models, instead of computing loss outside. Following these requirements, our dummy model have a few more lines added as shown below.

.. code-block:: python

    import torch
    import dgl.nn as dglnn
    import torch.nn as nn
    import torch.nn.function as F
    from graphstorm import model as gsmodel


    class dummyGnnModel(gsmodel.GSgnnNodeModelBase):
        def __init__(self, in_dim, hid_dim, out_dim):
            super().__init__()
            self.conv1 = dglnn.HeteroSAGEConv(in_dim, hid_dim)
            self.conv2 = dglnn.HeteroSAGEConv(hid_dim, out_dim)
            self._loss_fn = gsmodel.ClassifyLossFunc(multilabel=False)             # define a loss function

        def forward(self, blocks, node_feats, target_ntype, labels):
            h = self.conv1(blocks[0], node_feats)
            h = {ntype: F.relu(h[n_emb] for ntype, n_emb in h.items())
            h = self.conv2(blocks[1], h)
            loss = self._loss_fn(h[target_ntype], labels)                            # compute loss value
            return loss

You may notice that the GraphStorm already provides common loss functions for classification, regression and link prediction, which can be easily imported and used in your model. But you are free to use any PyTorch loss functions or even your own loss function. In the above codes, we also add the prediction node type and labels to compute the loss.

The ``predict()`` function is for inference and it will not be used for backward. Its input arguments are similar to the forward() function, but no need for labels. The ``predict()`` function will return two values. The first is the prediction results, not the logits. The second is the model embeddings, which could be used for some specific purposes (this return value is uncommon for some users, and we are working on the fix this confusion). With these requirements, the ``predict()`` function of the dummy model is like the code below.

.. code-block:: python

    def predict(self, blocks, node_feats, target_ntype):
            h = self.conv1(blocks[0], node_feats)
            h = {ntype: F.relu(h[n_emb] for ntype, n_emb in h.items())
            h = self.conv2(blocks[1], h)
            return h[target_ntype].argmax(dim=1), h[target_ntype]                       # return two values: one is the predict results, 
                                                                                    # while another is the computed node representations, which can be saved.

The ``create_optimizer()`` function is for users to define its own optimizer. You can put the optimizer definition from the training flow in here, like the code below

.. code-block:: python

    def create_optimizer(self, lr=0.001):
        return torch.optim.Adam(self.parameters(), lr=lr)

There are other optional functions in the `GSgnnNodeModelBase <https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/model/node_gnn.py#L76>`_ class, including ``restore_model(self, restore_model_path)`` and ``save_model(self, model_path)``, which are used for restore and save models. If you want to save or restore models, implement these two functions too.

Step 3. Modify the training/inference flow with the GraphStorm APIs
....................................................................
With the modified GNN models ready, the next step is to modify the training/inference loop by replacing datasets and dataloaders with the GraphStorm's dataloading classes.

In general, the dummy model could use the following flow to be trained and evaluated as suggested by the `DGL User Guide <https://docs.dgl.ai/guide/training.html>`_.

.. code-block:: python

    from dgl.dataloading import NeighborSampler, DataLoader

    sampler = NeighborSampler([20,20] * 2)                                                  # Define neighbor sampler
    train_dataloader = DataLoader(graph, {target_ntype: train_index}, sampler                 # Define DGL dataloader
                                batch_size=64)
    dummy_model = dummyGnnModel(in_dim=16, hid_dim=64, out_dim=n_labels)                    # Initialize the dummy model
    opt = torch.optim.Adam(model.parameters(), lr=0.001)                                    # Define an optimizer

    for epoch in range(10):
        model.train()
        for i, (input_nodes, output_nodes, blocks) in enumerate(train_dataloader):
            node_features = extract_node_feature(input_nodes)                               # Only extract features for input nodes in this blocks
            labels = extract_labels(output_nodes[target_ntype])                               # Only extract labels for output nodes in this blocks
            logits = dummy_model(blocks, node_features)                                     # forward propagation by using all nodes
            loss = F.cross_entropy(logits[target_ntype], labels)                              # compute loss
            # compute validation accuracy ......
            loss.backward()

The GraphStorm training flow is similar with a few modifications.

Start training process with GraphStorm's iniatilization
```````````````````````````````````````````````````````````
Any GraphStorm training process **MUST** start with a proper initialization. You can use the following codes at the beginning of training flow.

.. code-block:: python

    import graphstorm as gs
    ......

    def main(args):
        gs.initialize(ip_config=ip_config, backend="gloo")

the ``ip_config`` argument specifies a ip configuration file, which contains the IP addresses of machines in a GraphStorm distributed cluster. You can find its description at the :ref:`Launch Training<launch-training>` section of the :ref:`Quick Start Tutorial <quick-start>`.

Replace DGL DataLoader with the GraphStorm's dataset and dataloader
`````````````````````````````````````````````````````````````````````
Because the GraphStorm uses distributed graphs, we need to first load the partitioned graph, which is created in the **Step 1**, with the `GSgnnNodeTrainData <https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/dataloading/dataset.py#L469>`_ class (for edge tasks, the GraphStorm also provides `GSgnnEdgeTrainData <https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/dataloading/dataset.py#L216>`_). The ``GSgnnNodeTrainData`` could be created as shown in the codes below.

.. code-block:: python

    graph_data = GSgnnNodeTrainData(graph_name, part_config, train_ntypes=target_ntype, eval_ntypes=None,
                                    label_field=label_field, node_feat_field=node_feat_field, edge_feat_field=None)

Arguments of this class includes the partition configuration JSON file path, which are the outputs of the **Step 1**. The ``graph_name`` can be found in the JSON file.

The other values, the ``train_ntypes``, the ``label_field``, the ``node_feat_field`` and ``edge_feat_field``, should be consistent with the values in the raw data :ref:`input configuration JSON <input-config>` defined in the **Step 1**. The ``train_ntypes`` is the ``node_type`` that has ``labels`` specified. The ``label_fields`` is the value specified in ``label_col`` of the ``train_ntype``. The ``node_feat_field`` and the ``edge_feat_field`` are two dictionaries, whose keys are the values of ``node_type``, and values are the values of ``feature_name``.

Then we can put this dataset into GraphStorm's `GSgnnNodeDataLoader <https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/dataloading/dataloading.py#L544>`_, which is like:

.. code-block:: python

    train_dataloader = GSgnnNodeDataLoader(graph_data, graph_data.train_idxs, fanout=fanout,
                                       batch_size=64, device=device, train_task=True)
    val_dataloader   = GSgnnNodeDataLoader(graph_data, graph_data.val_idxs, fanout=fanout,
                                        batch_size=64, device=device, train_task=False)
    test_dataloader  = GSgnnNodeDataLoader(graph_data, graph_data.test_idxs, fanout=fanout,
                                        batch_size=64, device=device, train_task=False)

The GraphStorm provides a set of dataloaders for different GML tasks. Here we deal with a node task, hence using the node dataloader, which takes the graph data created above as the first argument. The second argument is the label index that the GraphStorm dataset extracted from the graph as indicated in the target nodes' ``train_mask``, ``val_mask``, and ``test_mask``, which are automatically generated by GraphStorm graph construction tool with the specified ``split_pct`` field. The ``GSgnnNodeTrainData`` automatically extract these indexes out and set its properties so that you can directly use them like ``graph_data.train_idxs`` and ``graph_data.val_idxs``, and ``graph_data.test_idxs``. The rest of arguments are similar to the common training flow, except that we set the ``train_task`` to be ``False`` for the evaluation and test dataloader.

Use GraphStorm's model trainer to wrap your model and attach evaluator and task tracker to it
````````````````````````````````````````````````````````````````````````````````````````````````
Unlike the common flow, GraphStorm wraps GNN models with different trainers just like other frameworks, e.g. scikit-learn. GraphStorm provides node prediction, edge prediction, and link prediction trainers. Creation of them is easy. For the dummy model, after create it as the ``dummy_model = dummyGnnModel(in_dim=16, hid_dim=64, out_dim=n_labels)``, we can use the `GSgnnNodePredictionTrainer <https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/trainer/np_trainer.py#L29>`_ class to wrap it like:

.. code-block:: python

    trainer = GSgnnNodePredictionTrainer(dummy_model, rank=gs.get_rank())

The ``GSgnnNodePredictionTrainer`` takes a GNN model as the first argument. The seconde argument is for using different GPUs.

The GraphStorm trainers can have evaluators and task trackers associated. The following codes show how to do this.

.. code-block:: python

    evaluator = GSgnnAccEvaluator(config.eval_frequency,
                              config.eval_metric,
                              config.multilabel,
                              config.use_early_stop,
                              config.early_stop_burnin_rounds,
                              config.early_stop_rounds,
                              config.early_stop_strategy)
    trainer.setup_evaluator(evaluator)

    tracker = GSSageMakerTaskTracker(config, gs.get_rank())       # Optional: set up a task tracker to show the progress of training.
    trainer.setup_task_tracker(tracker)

GraphStorm's `evaluators <https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/eval/evaluator.py>`_ could help to compute the required evaluation metrics, such as ``accuracy``, ``f1``, ``mrr``, and etc. Users can select the proper evaluator and use the trainer's ``setup_evaluator()`` method to attach them. GraphStorm's `task trackers <https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/tracker/graphstorm_tracker.py>`_ serve as log collectors, which is used to show the process information.

Use trainer's ``fit()`` function to run training
``````````````````````````````````````````````````
Once all trainers, evaluators, and task trackers set, the last step is to use the trainer's ``fit()`` function to run training, validating, and testing on the three sets like the code below.

.. code-block:: python

    trainer.fit(train_loader=train_dataloader, num_epochs=num_epochs, val_loader=eval_dataloader,
                test_loader=test_dataloader, save_model_path=save_model_path, mini_batch_infer=True)

The ``fit()`` function wraps dataloaders, number of epochs, to replace the common "**for loops**" as seen in the above training flow. The ``fit()`` function also takes additional arguments, such as ``save_model_path``, and ``save_model_requency`` to save different model artifacts. **BUT** before set these arguments, you need to implement the ``restore_model(self, restore_model_path)`` and ``save_model(self, model_path)`` functions in the **Step 2**.

Step 4. Setup GraphStorm configuration YAML file
.....................................................................
GraphStorm has a set of configurations that control the various perspectives of the model training and inference process. You can find the details of these configurations in the GraphStorm :ref:`Configuration page <configurations-run>`. These configurations could be either passed as input arguments or set in a YAML format file. Below is an example of the YAML file.

.. code-block:: yaml

    ---
    version: 1.0
    gsf:
        basic:
            backend: gloo
            ip_config: ip_list.txt
            part_config: /data/acm_nc/acm.json
            alpha_l2norm: 0.
        gnn:
            fanout: "50,50"
            num_layers: 2
            hidden_size: 256
        input:
            restore_model_path: null
        output:
            save_model_path: /data/outputs
            save_embeds_path: /data/outputs
            save_prediction_path: /data/outputs
        hyperparam:
            dropout: 0.
            lr: 0.0001
            num_epochs: 200
            batch_size: 1024
            eval_batch_size: 1024
        node_classification:
            target_ntype: "paper"
            label_field: "label"
            multilabel: false
            num_classes: 14

Users can use an argument to read in this YAML file, and construct a ``GSConfig`` object like the below codes. And then use the GSConfig instance, e.g., ``config``, to provide arguments that the GraphStorm supports.

.. code-block:: python

    from graphstorm.config import GSConfig
    ......
    argparser.add_argument("--yaml-config-file", type=str, required=True, help="The GraphStorm YAML configuration file path.")
    args = argparser.parse_args()
    config = GSConfig(args)

For users' own configurations, you still can pass them as input argument of the training script, and extract them from the ``args`` object.

Step 5. One more thing: the unused weights error
...................................................
Uncommonly seen in the full-graph training or mini-batch training on a single GPU, the unused weights error could frequently occur when start to train models on multiple GPUs in parallel. PyTorch distributed framework's inner mechanism cause this problem. One easy way to solve this error is to add a regularization to all trainable parameters into the loss computation like the codes blow.

.. code-block:: python

        pred_loss = self._loss_fn(h[self.target_ntype], labels[self.target_ntype])
        # L2 regularization of trainable parameters
        reg_loss = torch.tensor(0.).to(pred_loss.device)
        for d_para in self.parameters():
            reg_loss += d_para.square().sum()
        
        reg_loss = self.alpha_l2norm * reg_loss

        total_loss = pred_loss + reg_loss

You can add a coefficient, like the ``alpha_l2norm``, to control the influence of the regularization.

Put Everything Together and Run them
-------------------------------------
With all required modifications ready, let's put everything of the dummy model together in a Python file, e.g, ``dummy_nc.py``. We can put the Python file and the related artifacts, including the YAML file, e.g., ``acm_nc.yaml``, and the ``ip_list.txt`` file in a folder, e.g. ``/dummy_model/``. And then use the GraphStorm's launch script to run this dummy model.

.. code-block:: python

    python3 ~/dgl/tools/launch.py \
            --workspace /dummy_model \
            --part_config /data/dummy_nc/dummy_data.json \
            --ip_config ip_list.txt \
            --num_trainers 4 \
            --num_servers 1 \
            --num_samplers 0 \
            --ssh_port 2222 \
            "python3 dummy_nc.py --yaml-config-file dummy_nc.yaml \
                                 --part-config dummy_data.json \ 
                                 --ip-config ip_list.txt \
                                 --node-feat-name paper:feat-author:feat-subject:feat"

The argument value of ``--part_config`` is the JSON file coming from the :ref:`outputs <output-graph-construction>` of the Step 1.

The full dummy model example
.............................
Here come the full dummy model example for reference. Please note that this code is **NOT** runnable but for demonstration purpose. 

.. note:: To try runnable example, please check the `GraphStorm examples <https://github.com/awslabs/graphstorm/tree/main/examples/customized_models/HGT>`_.

.. code-block:: python

    import torch
    import dgl.nn as dglnn
    import torch.nn as nn
    import torch.nn.function as F
    import graphstorm as gs
    from graphstorm import model as gsmodel
    from graphstorm.trainer import GSgnnNodePredictionTrainer
    from graphstorm.inference import GSgnnNodePredictionInfer
    from graphstorm.dataloading import GSgnnNodeTrainData, GSgnnNodeInferData
    from graphstorm.dataloading import GSgnnNodeDataLoader
    from graphstorm.eval import GSgnnAccEvaluator
    from graphstorm.tracker import GSSageMakerTaskTracker


    class dummyGnnModel(gsmodel.GSgnnNodeModelBase):
        def __init__(self, in_dim, hid_dim, out_dim, target_ntype):
            super().__init__()
            self.conv1 = dglnn.HeteroSAGEConv(in_dim, hid_dim)
            self.conv2 = dglnn.HeteroSAGEConv(hid_dim, out_dim)
            self.target_ntype = target_ntype
            self._loss_fn = gsmodel.ClassifyLossFunc(multilabel=False)             # define a loss function

        def forward(self, blocks, node_feats, labels):
            h = self.conv1(blocks[0], node_feats)
            h = {ntype: F.relu(h[n_emb] for ntype, n_emb in h.items())
            h = self.conv2(blocks[1], h)

            pred_loss = self._loss_fn(h[self.target_ntype], labels)
            # L2 regularization of trainable parameters
            reg_loss = torch.tensor(0.).to(pred_loss.device)
            for d_para in self.parameters():
                reg_loss += d_para.square().sum()
                reg_loss = self.alpha_l2norm * reg_loss

            total_loss = pred_loss + reg_loss
            return total_loss

        def predict(self, blocks, node_feats, target_ntype):
            h = self.conv1(blocks[0], node_feats)
            h = {ntype: F.relu(h[n_emb] for ntype, n_emb in h.items())
            h = self.conv2(blocks[1], h)
            return h[target_ntype].argmax(dim=1), h[target_ntype]                       # return two values

        def create_optimizer(self, lr=0.001):
            return torch.optim.Adam(self.parameters(), lr=lr)

        def restore_model(self, restore_model_path):
            pass

        def save_model(self, model_path):
            pass

    def main(args):
        gs.initialize(ip_config=args.ip_config, backend="gloo")
        config = GSConfig(args)

        # Process node_feat_field to define GraphStorm dataset
        node_feat_fields = {}
        node_feat_types = args.node_feat_name.split('-')
        for node_feat_type in node_feat_types:
            node_type, feat_names = node_feat_type.split(':')
            node_feat_fields[node_type] = feat_names.split(',')

        # Define the GraphStorm training dataset
        train_data = GSgnnNodeTrainData(config.graph_name,
                                        config.part_config,
                                        train_ntypes=config.target_ntype,
                                        node_feat_field=node_feat_fields,
                                        label_field=config.label_field)
        nfeat_dims = {}
        for ntype, _ in node_dict.items():
            if train_data.g.nodes[ntype].data.get('feat') is not None:
                nfeat_dims[ntype] = train_data.g.nodes[ntype].data['feat'].shape[-1]
            else:
                nfeat_dims[ntype] = 0

        model = dummyModel(nfeat_dims, config.hidden_size, config.num_classes, config.target_ntype)

        # Create a trainer for the node classification task.
        trainer = GSgnnNodePredictionTrainer(model, gs.get_rank(), topk_model_to_save=1)
        trainer.setup_cuda(dev_id=gs.get_rank())
        device = 'cuda:%d' % trainer.dev_id

        # Define the GraphStorm train dataloader
        dataloader = GSgnnNodeDataLoader(train_data, train_data.train_idxs, fanout=config.fanout,
                                        batch_size=config.batch_size, device=device, train_task=True)

        # Optional: Define the evaluation dataloader
        eval_dataloader = GSgnnNodeDataLoader(train_data, train_data.val_idxs,fanout=config.fanout,
                                            batch_size=config.eval_batch_size, device=device,
                                            train_task=False)
        
        # Optional: Define the evaluation dataloader
        test_dataloader = GSgnnNodeDataLoader(train_data, train_data.test_idxs,fanout=config.fanout,
                                            batch_size=config.eval_batch_size, device=device,
                                            train_task=False)

        # Optional: set up a evaluator
        evaluator = GSgnnAccEvaluator(config.eval_frequency,
                                    config.eval_metric,
                                    config.multilabel,
                                    config.use_early_stop,
                                    config.early_stop_burnin_rounds,
                                    config.early_stop_rounds,
                                    config.early_stop_strategy)
        trainer.setup_evaluator(evaluator)
        # Optional: set up a task tracker to show the progress of training.
        tracker = GSSageMakerTaskTracker(config, gs.get_rank())
        trainer.setup_task_tracker(tracker)

        # Start the training process.
        trainer.fit(train_loader=dataloader, n_epochs=config.num_epochs,
                    val_loader=eval_dataloader, 
                    test_loader=test_dataloader,
                    save_model_path=config.save_model_path,
                    mini_batch_infer=True)
        
        # After training, get the best model from the trainer.
        best_model = trainer.get_best_model()


        # Create a dataset for inference.
        infer_data = GSgnnNodeInferData(config.graph_name, config.part_config,
                                        eval_ntypes=config.target_ntype,
                                        node_feat_field=node_feat_fields,
                                        label_field=config.label_field)

        # Create an inference for a node task.
        infer = GSgnnNodePredictionInfer(best_model, gs.get_rank())
        infer.setup_cuda(dev_id=gs.get_rank())
        infer.setup_evaluator(evaluator)
        infer.setup_task_tracker(tracker)
        dataloader = GSgnnNodeDataLoader(infer_data, infer_data.test_idxs,
                                        fanout=config.fanout, batch_size=100, device=device,
                                        train_task=False)

        # Run inference on the inference dataset and save the GNN embeddings in the specified path.
        infer.infer(dataloader, save_embed_path=config.save_embed_path, mini_batch_infer=True)

    if __name__ == '__main__':
        argparser = argparse.ArgumentParser("Training HGT model with the GraphStorm Framework")
        argparser.add_argument("--yaml-config-file", type=str, required=True,
                            help="The GraphStorm YAML configuration file path.")
        argparser.add_argument("--ip-config", type=str, required=True,
                            help="The IP config file for the cluster.")
        argparser.add_argument("--node-feat-name", type=str, required=True,
                            help="The name of the node features. \
                                    Format is nodetype1:featname1,featname2-nodetype2:featname1,...")
        argparser.add_argument("--local_rank", type=int,
                            help="The rank for trainers. MUST have this argument for using DGL launch command!!")
        args = argparser.parse_args()
        
        print(args)
        main(args)