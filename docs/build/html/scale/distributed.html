<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Use GraphStorm in a Distributed Cluster &mdash; GraphStorm 0.3 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Use GraphStorm on SageMaker" href="sagemaker.html" />
    <link rel="prev" title="Running distributed jobs on Amazon SageMaker" href="../gs-processing/usage/amazon-sagemaker.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            GraphStorm
          </a>
              <div class="version">
                0.3
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/env-setup.html">Environment Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/quick-start.html">Standalone Mode Quick Start Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/own-data.html">Use Your Own Data Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration/index.html">GraphStorm Configurations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gs-processing/gs-processing-getting-started.html">GraphStorm Processing Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gs-processing/usage/example.html">GraphStorm Processing Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gs-processing/usage/distributed-processing-setup.html">GraphStorm Processing setup for Amazon SageMaker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gs-processing/usage/amazon-sagemaker.html">Running distributed jobs on Amazon SageMaker</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Use GraphStorm in a Distributed Cluster</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#create-a-graphstorm-cluster">Create a GraphStorm Cluster</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#setup-the-instance-of-a-cluster">Setup the instance of a cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#setup-of-a-shared-file-system-for-the-cluster">Setup of a shared file system for the cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-graphstorm-container-by-mounting-the-nfs-folder">Create GraphStorm container by mounting the NFS folder</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#setup-the-ip-address-file-and-check-port-status">Setup the IP address file and check port status</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#collect-the-ip-list">Collect the IP list</a></li>
<li class="toctree-l3"><a class="reference internal" href="#check-port">Check port</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#partition-a-graph">Partition a Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="#launch-training-on-one-container">Launch Training on One Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="#train-a-large-graph-ogbn-papers100m">Train a Large Graph (OGBN-Papers100M)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">Create a GraphStorm Cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#process-and-partition-a-graph">Process and Partition a Graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distribute-partitioned-graphs-and-configurations-to-all-instances">Distribute Partitioned Graphs and Configurations to all Instances</a></li>
<li class="toctree-l3"><a class="reference internal" href="#launch-training-in-one-container">Launch Training in One Container</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sagemaker.html">Use GraphStorm on SageMaker</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Topics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/own-models.html">Use Your Own Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/language-models.html">Use Text as Node Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/advanced-usages.html">GraphStorm Advanced Usages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/advanced-wholegraph.html">Use WholeGraph in GraphStorm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.html">graphstorm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.dataloading.html">graphstorm.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.eval.html">graphstorm.eval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.inference.html">graphstorm.inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.model.html">graphstorm.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.trainer.html">graphstorm.trainer</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">GraphStorm</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Use GraphStorm in a Distributed Cluster</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/scale/distributed.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="use-graphstorm-in-a-distributed-cluster">
<span id="distributed-cluster"></span><h1>Use GraphStorm in a Distributed Cluster<a class="headerlink" href="#use-graphstorm-in-a-distributed-cluster" title="Permalink to this heading"></a></h1>
<p>GraphStorm can scale to the enterprise-level graphs in the distributed mode by using a cluster of instances. To leverage this capacity, there are four steps to follow:</p>
<ul class="simple">
<li><p>Create a cluster with instances each of which can run GraphStorm Docker container.</p></li>
<li><p>Set up the IP address file and check port 2222 status.</p></li>
<li><p>Partition large graphs into distributed format.</p></li>
<li><p>Launch the training command within one instance’ container.</p></li>
</ul>
<p>The first section of this tutorial uses the <a class="reference external" href="https://ogb.stanford.edu/docs/nodeprop/#ogbn-mag">OGB-MAG</a> as an example data to demonstrate how to use GraphStorm to train an RGCN model (a built-in model) in a cluster with three EC2 instances. The OGB-MAG data is large enough to demonstrate the scalability of GraphStorm, and also small enough to complete training in short time.</p>
<section id="create-a-graphstorm-cluster">
<h2>Create a GraphStorm Cluster<a class="headerlink" href="#create-a-graphstorm-cluster" title="Permalink to this heading"></a></h2>
<section id="setup-the-instance-of-a-cluster">
<h3>Setup the instance of a cluster<a class="headerlink" href="#setup-the-instance-of-a-cluster" title="Permalink to this heading"></a></h3>
<p>A cluster contains several GPU installed instances each of which can run GraphStorm Docker container. For each instance, please follow the <a class="reference internal" href="../install/env-setup.html#setup-docker"><span class="std std-ref">Environment Setup</span></a> description to setup GraphStorm Docker container environment. This tutorial uses three EC2 instances in the cluster.</p>
</section>
<section id="setup-of-a-shared-file-system-for-the-cluster">
<h3>Setup of a shared file system for the cluster<a class="headerlink" href="#setup-of-a-shared-file-system-for-the-cluster" title="Permalink to this heading"></a></h3>
<p>A cluster requires a shared file system, such as NFS or EFS, mounted to each instance in the cluster, in which all GraphStorm containers in the cluster can share data files, and save model artifacts and prediction results.</p>
<p><a class="reference external" href="https://github.com/dmlc/dgl/tree/master/examples/pytorch/graphsage/dist#step-0-setup-a-distributed-file-system">Here</a> is the instruction of setting up NFS for a cluster provided by DGL. As the steps of setup of an NFS could be various for different systems, we suggest users to look for additional information about NFS setting. Here are some available resources: <a class="reference external" href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-22-04">NFS tutorial</a> by DigitalOcean, <a class="reference external" href="https://ubuntu.com/server/docs/service-nfs">NFS document</a> for Ubuntu, <a class="reference external" href="https://www.linode.com/docs/guides/using-an-nfs-server-on-ubuntu2004/">NFS guide</a> by Linode, <a class="reference external" href="https://www.tecmint.com/how-to-setup-nfs-server-in-linux/">NFS tutorial</a> at Tecmint, and <a class="reference external" href="https://www.howtoforge.com/how-to-install-nfs-server-and-client-on-ubuntu-22-04/">NFS guide</a> by HowtoForge.</p>
<p>For an AWS EC2 cluster, users can also use EFS as the shared file system. Please follow 1) <a class="reference external" href="https://docs.aws.amazon.com/efs/latest/ug/gs-step-two-create-efs-resources.html">the instruction of creating EFS</a>; 2) <a class="reference external" href="https://docs.aws.amazon.com/efs/latest/ug/installing-amazon-efs-utils.html">the instruction of installing an EFS client</a>; and 3) <a class="reference external" href="https://docs.aws.amazon.com/efs/latest/ug/efs-mount-helper.html">the instructions of mounting the EFS filesystem</a> to set up EFS.</p>
<p>After setting up a shared file system, we can keep all partitioned graph data in the shared folder. Then mount the data folder to the <code class="docutils literal notranslate"><span class="pre">/path_to_data/</span></code> of each instances in the cluster so that all GraphStorm containers in the cluster can access these partitioned graph data easily.</p>
</section>
<section id="create-graphstorm-container-by-mounting-the-nfs-folder">
<h3>Create GraphStorm container by mounting the NFS folder<a class="headerlink" href="#create-graphstorm-container-by-mounting-the-nfs-folder" title="Permalink to this heading"></a></h3>
<p>In each instance, use the following command to start a GraphStorm Docker container and run it as a backend daemon.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>nvidia-docker<span class="w"> </span>run<span class="w"> </span>-v<span class="w"> </span>/path_to_data/:/data<span class="w"> </span><span class="se">\</span>
<span class="w">                  </span>-v<span class="w"> </span>/dev/shm:/dev/shm<span class="w"> </span><span class="se">\</span>
<span class="w">                  </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">                  </span>-d<span class="w"> </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span>graphstorm:local
</pre></div>
</div>
<p>This command mount the shared <code class="docutils literal notranslate"><span class="pre">/path_to_data/</span></code> folder to each container’s <code class="docutils literal notranslate"><span class="pre">/data/</span></code> folder by which GraphStorm codes can access graph data and save training and inference outcomes.</p>
</section>
</section>
<section id="setup-the-ip-address-file-and-check-port-status">
<h2>Setup the IP address file and check port status<a class="headerlink" href="#setup-the-ip-address-file-and-check-port-status" title="Permalink to this heading"></a></h2>
<section id="collect-the-ip-list">
<h3>Collect the IP list<a class="headerlink" href="#collect-the-ip-list" title="Permalink to this heading"></a></h3>
<p>The GraphStorm Docker containers use SSH on port <code class="docutils literal notranslate"><span class="pre">2222</span></code> to communicate with each other. Users need to collect all IP addresses of the three instances and put them into a text file, e.g., <code class="docutils literal notranslate"><span class="pre">/data/ip_list.txt</span></code>, which is like:</p>
<figure class="align-center">
<img alt="../_images/distributed_ips.png" src="../_images/distributed_ips.png" />
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If possible, use <strong>private IP addresses</strong>, insteand of public IP addresses. Public IP addresses may have additional port constraints, which cause communication issues.</p>
</div>
<p>Put this file into container’s <code class="docutils literal notranslate"><span class="pre">/data/</span></code> folder.</p>
</section>
<section id="check-port">
<h3>Check port<a class="headerlink" href="#check-port" title="Permalink to this heading"></a></h3>
<p>The GraphStorm Docker container uses port <code class="docutils literal notranslate"><span class="pre">2222</span></code> to <strong>ssh</strong> to containers running on other machines without passwords. Please make sure all host instances do not use this port.</p>
<p>Users also need to make sure the port <code class="docutils literal notranslate"><span class="pre">2222</span></code> is open for <strong>ssh</strong> commands.</p>
<p>Pick one instance and run the following command to connect to the GraphStorm Docker container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span><span class="nb">test</span><span class="w"> </span>/bin/bash
</pre></div>
</div>
<p>In the container environment, users can check the connectivity with the command <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">&lt;ip-in-the-cluster&gt;</span> <span class="pre">-o</span> <span class="pre">StrictHostKeyChecking=no</span> <span class="pre">-p</span> <span class="pre">2222</span></code>. Please replace the <code class="docutils literal notranslate"><span class="pre">&lt;ip-in-the-cluster&gt;</span></code> with the real IP address from the <code class="docutils literal notranslate"><span class="pre">ip_list.txt</span></code> file above, e.g.,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span><span class="m">172</span>.38.12.143<span class="w"> </span>-o<span class="w"> </span><span class="nv">StrictHostKeyChecking</span><span class="o">=</span>no<span class="w"> </span>-p<span class="w"> </span><span class="m">2222</span>
</pre></div>
</div>
<p>If succeeds, you should login to the container in the <code class="docutils literal notranslate"><span class="pre">&lt;ip-in-the-cluster&gt;</span></code> instance.</p>
<p>If not, please make sure there is no limitation of port 2222.
You may also have to exchange the keys from inside the GraphStorm Docker containers to allow their communication. For that, copy the keys from the <code class="docutils literal notranslate"><span class="pre">/root/.ssh/id_rsa.pub</span></code> from this container to <code class="docutils literal notranslate"><span class="pre">/root/.ssh/authorized_keys</span></code> in containers on every machine in your cluster.</p>
<p>For distributed training, users also need to make sure ports under 65536 is open for DistDGL to use.</p>
</section>
</section>
<section id="partition-a-graph">
<span id="id3"></span><h2>Partition a Graph<a class="headerlink" href="#partition-a-graph" title="Permalink to this heading"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All commands below should be run in a GraphStorm Docker container. Please refer to the <a class="reference internal" href="../install/env-setup.html#setup-docker"><span class="std std-ref">GraphStorm Docker environment setup</span></a> to prepare your environment.</p>
</div>
<p>Now we can download and process the OGBN-MAG data with the command below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>/graphstorm/tools/gen_mag_dataset.py<span class="w"> </span>--savepath<span class="w"> </span>/data/ogbn-mag-lp/<span class="w"> </span>--edge-pct<span class="w"> </span><span class="m">0</span>.2
</pre></div>
</div>
<p>Because we use three GraphStorm instances in the cluster for model training, this command splits the MAG data into three partitions by specifying the <code class="docutils literal notranslate"><span class="pre">--num-parts</span></code> argument to <code class="docutils literal notranslate"><span class="pre">3</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>/graphstorm/tools/partition_graph_lp.py<span class="w"> </span>--dataset<span class="w"> </span>ogbn-mag<span class="w"> </span><span class="se">\</span>
<span class="w">                                                </span>--filepath<span class="w"> </span>/data/ogbn-mag-lp/<span class="w"> </span><span class="se">\</span>
<span class="w">                                                </span>--num-parts<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">                                                </span>--balance-train<span class="w"> </span><span class="se">\</span>
<span class="w">                                                </span>--balance-edges<span class="w"> </span><span class="se">\</span>
<span class="w">                                                </span>--num-trainers-per-machine<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">                                                </span>--target-etypes<span class="w"> </span>author,writes,paper<span class="w"> </span><span class="se">\</span>
<span class="w">                                                </span>--output<span class="w"> </span>/data/ogbn_mag_lp_3p
</pre></div>
</div>
<p>After this command completes successfully, the partitioned OGBN-MAG graph is stored in the <code class="docutils literal notranslate"><span class="pre">/data/ogbn_mag_lp_3p</span></code> folder whose structure is like the diagram below. Because the <code class="docutils literal notranslate"><span class="pre">/data/</span></code> folder is a shared filesystem, all instances in the cluster can access these files.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/data/ogbn_mag_lp_3p
ogbn-mag.json
node_mapping.pt
edge_mapping.pt
<span class="p">|</span>-<span class="w"> </span>part0
<span class="w">    </span>edge_feat.dgl
<span class="w">    </span>graph.dgl
<span class="w">    </span>node_feat.dgl
<span class="p">|</span>-<span class="w"> </span>part1
<span class="w">    </span>edge_feat.dgl
<span class="w">    </span>graph.dgl
<span class="w">    </span>node_feat.dgl
<span class="p">|</span>-<span class="w"> </span>part2
<span class="w">    </span>edge_feat.dgl
<span class="w">    </span>graph.dgl
<span class="w">    </span>node_feat.dgl
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The two mapping files, <code class="docutils literal notranslate"><span class="pre">node_mapping.pt</span></code> and <code class="docutils literal notranslate"><span class="pre">edge_mapping.pt</span></code>, are used to record the mapping between the ogriginal node and edge ids in the raw data files and the ids of nodes and edges in the constructed graph. They are important for mapping the training and inference outputs. Therefore, DO NOT move or delete them.</p>
</div>
</section>
<section id="launch-training-on-one-container">
<h2>Launch Training on One Container<a class="headerlink" href="#launch-training-on-one-container" title="Permalink to this heading"></a></h2>
<p>When graph partition data is ready, it is easy to launch a distributed training job. Pick a GraphStorm container, e.g. the container with IP address <code class="docutils literal notranslate"><span class="pre">172.37.11.221</span></code>, and run the following command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>graphstorm.run.gs_link_prediction<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--workspace<span class="w"> </span>/data/ogbn-mag-lp/<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--num-trainers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--num-servers<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--num-samplers<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--part-config<span class="w"> </span>/data/ogbn_mag_lp_3p/ogbn-mag.json<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--ip-config<span class="w"> </span>/data/ip_list.txt<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--ssh-port<span class="w"> </span><span class="m">2222</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--cf<span class="w"> </span>/graphstorm/training_scripts/gsgnn_lp/mag_lp.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--node-feat-name<span class="w"> </span>paper:feat<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--save-model-path<span class="w"> </span>/data/ogbn-mag-lp/models/
</pre></div>
</div>
<p>That’s it! The command will initialize the training in all three GraphStorm containers, each of which will take a partition of the MAG graph and conduct link prediction traing collaborately.</p>
</section>
<section id="train-a-large-graph-ogbn-papers100m">
<h2>Train a Large Graph (OGBN-Papers100M)<a class="headerlink" href="#train-a-large-graph-ogbn-papers100m" title="Permalink to this heading"></a></h2>
<p>The previous sections demonstrates GraphStorm’s distributed capability for a quick start. This section will use GraphStorm to train a large Graph data, i.e., <a class="reference external" href="https://ogb.stanford.edu/docs/nodeprop/#ogbn-papers100M">OGBN-Papers100M</a>,  that can hardly train an RGCN model on it in a single machine. The steps of training this large graph is nearly the same as the above section, and only need a few additional operations.</p>
<section id="id4">
<h3>Create a GraphStorm Cluster<a class="headerlink" href="#id4" title="Permalink to this heading"></a></h3>
<p>In addition to the three GraphStorm instance created in the OGBN-MAG tutorial, to download and partition the OGBN-Papers100M graph, we need a new instance that has large memory, e.g., &gt;800GB. In this tutorial we use an AWS r6a.32xlarge instance, which has 1TB memory. For the instance, please follow the <a class="reference internal" href="../install/env-setup.html#setup"><span class="std std-ref">Environment Setup</span></a> description to setup GraphStorm Docker container environment. Once building the GraphStorm Docker image in this instance, use the following command to start a GraphStorm Docker container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-v<span class="w"> </span>/path_to_data/:/data<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>-v<span class="w"> </span>/dev/shm:/dev/shm<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>-d<span class="w"> </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span>graphstorm:local
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Use the “<strong>docker</strong>”, instead of “nvidia-docker” command to create the GraphStorm container because the new r6a.32xlarge instance does not have GPUs installed.</p></li>
<li><p>Make sure there is at least 300GB free space in the /path_to_data/ folder. It is better to use the shared file system folder so that the partitioned graph data can be easily shared to the GraphStorm cluster.</p></li>
</ul>
</div>
</section>
<section id="process-and-partition-a-graph">
<h3>Process and Partition a Graph<a class="headerlink" href="#process-and-partition-a-graph" title="Permalink to this heading"></a></h3>
<p>Run the below command to download and partition the OGBN-Papers100M data for a node classification task, which will predict the category of a paper. Because the ogbn-papers100M is one of GraphStorm’s built-in datasets, we do not specify some arguments, such as <code class="docutils literal notranslate"><span class="pre">target-ntype</span></code>, <code class="docutils literal notranslate"><span class="pre">nlabel-field</span></code>, and <code class="docutils literal notranslate"><span class="pre">ntask-type</span></code>, which have been automatically handled by GraphStorm’s <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/data/ogbn_datasets.py">ogbn_datasets.py</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>/graphstorm/tools/partition_graph.py<span class="w"> </span>--dataset<span class="w"> </span>ogbn-papers100M<span class="w"> </span><span class="se">\</span>
<span class="w">                                            </span>--filepath<span class="w"> </span>/data<span class="w"> </span><span class="se">\</span>
<span class="w">                                            </span>--num-parts<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">                                            </span>--train-pct<span class="w"> </span><span class="m">0</span>.1<span class="w"> </span><span class="se">\</span>
<span class="w">                                            </span>--balance-train<span class="w"> </span><span class="se">\</span>
<span class="w">                                            </span>--balance-edges<span class="w"> </span><span class="se">\</span>
<span class="w">                                            </span>--output<span class="w"> </span>/data/ogbn_papers100M_3p<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>Given the size of OGBN-Papers100M, the download and partition process could run more than 5 hours and consume around 700GB memory in peak. After the command completes, the partitioned OGBN-Papers100M graphs are stored in the <code class="docutils literal notranslate"><span class="pre">/data/ogbn_papers100M_3p</span></code> folder whose structure is the same as the OGBN-MAG’s.</p>
</section>
<section id="distribute-partitioned-graphs-and-configurations-to-all-instances">
<h3>Distribute Partitioned Graphs and Configurations to all Instances<a class="headerlink" href="#distribute-partitioned-graphs-and-configurations-to-all-instances" title="Permalink to this heading"></a></h3>
<p>In this step, users need to copy these partitioned files to the shared file system of the GraphStorm cluster. And the IP list file creation and 2222 port open operations are identical to the above OGBN-MAG section.</p>
<p>For the OGBN-Papers100M data, we use a YAML file, <code class="docutils literal notranslate"><span class="pre">ogbn_papers100M_nc_p3.yaml</span></code>, that has the contents below.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="nt">gsf</span><span class="p">:</span>
<span class="nt">basic</span><span class="p">:</span>
<span class="w">    </span><span class="nt">model_encoder_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rgcn</span>
<span class="w">    </span><span class="nt">backend</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gloo</span>
<span class="w">    </span><span class="nt">verbose</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">no_validation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">evaluation_frequency</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">500</span>
<span class="nt">gnn</span><span class="p">:</span>
<span class="w">    </span><span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="w">    </span><span class="nt">hidden_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
<span class="w">    </span><span class="nt">mini_batch_infer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">input</span><span class="p">:</span>
<span class="w">    </span><span class="nt">restore_model_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">output</span><span class="p">:</span>
<span class="w">    </span><span class="nt">save_model_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">save_embed_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">hyperparam</span><span class="p">:</span>
<span class="w">    </span><span class="nt">dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.</span>
<span class="w">    </span><span class="nt">lr</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.001</span>
<span class="w">    </span><span class="nt">num_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">    </span><span class="nt">fanout</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;5,10,15&quot;</span>
<span class="w">    </span><span class="nt">eval_fanout</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;5,10,15&quot;</span>
<span class="w">    </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
<span class="w">    </span><span class="nt">eval_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
<span class="w">    </span><span class="nt">wd_l2norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">rgcn</span><span class="p">:</span>
<span class="w">    </span><span class="nt">num_bases</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span>
<span class="w">    </span><span class="nt">use_self_loop</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">lp_decoder_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">dot_product</span>
<span class="w">    </span><span class="nt">sparse_optimizer-lr</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-2</span>
<span class="w">    </span><span class="nt">use_node_embeddings</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">node_classification</span><span class="p">:</span>
<span class="w">    </span><span class="nt">target_ntype</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;node&quot;</span>
<span class="w">    </span><span class="nt">label_field</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;labels&quot;</span>
<span class="w">    </span><span class="nt">num_classes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">172</span>
</pre></div>
</div>
</section>
<section id="launch-training-in-one-container">
<h3>Launch Training in One Container<a class="headerlink" href="#launch-training-in-one-container" title="Permalink to this heading"></a></h3>
<p>Launch the training for the OGBN-Papers100M is similar as the OGBN-MAG data. Pick a GraphStorm container, e.g. the container with IP address <code class="docutils literal notranslate"><span class="pre">172.37.11.221</span></code>, and run the following command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>graphstorm.run.gs_node_classification<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--workspace<span class="w"> </span>/data/<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--num-trainers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--num-servers<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--num-samplers<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--part-config<span class="w"> </span>/data/ogbn_papers100M_3p/ogbn-papers100M.json<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--ip-config<span class="w"> </span>/data/ip_list.txt<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--ssh-port<span class="w"> </span><span class="m">2222</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--graph-format<span class="w"> </span>csc,coo<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--cf<span class="w"> </span>/data/ogbn_papers100M_nc_p3.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--node-feat-name<span class="w"> </span>feat
</pre></div>
</div>
<p>Due to the size of Papers100M graph, it will take around six minutes for all GraphStorm containers in the cluster to load corresponding partitions before the training starts.</p>
<p>Given a cluster with three AWS g4dn.12xlarge instances, each of which has 48 Intel Xeon vCPUs, four Nvidia T4 GPUs, and 192GB memory, it takes around 45 minutes to train one epoch with the given configurations.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../gs-processing/usage/amazon-sagemaker.html" class="btn btn-neutral float-left" title="Running distributed jobs on Amazon SageMaker" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sagemaker.html" class="btn btn-neutral float-right" title="Use GraphStorm on SageMaker" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, AGML team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>