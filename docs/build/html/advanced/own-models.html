<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Use Your Own Models &mdash; GraphStorm 0.2 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Use Text as Node Features" href="language-models.html" />
    <link rel="prev" title="Use GraphStorm on SageMaker" href="../scale/sagemaker.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            GraphStorm
          </a>
              <div class="version">
                0.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/env-setup.html">Environment Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/quick-start.html">Standalone Mode Quick Start Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/own-data.html">Use Your Own Data Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration/index.html">GraphStorm Configurations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Scale to Giant Graphs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../scale/distributed.html">Use GraphStorm in a Distributed Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../scale/sagemaker.html">Use GraphStorm on SageMaker</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Topics</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Use Your Own Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#use-dgl-to-implement-your-gnn-models">Use DGL to implement your GNN models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#modify-you-gnn-models-to-use-mini-batch-training-inference">Modify you GNN models to use mini-batch training/inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#learn-how-to-run-graphstorm-in-a-docker-environment">Learn how to run GraphStorm in a Docker environment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#modifications-required-for-customer-models">Modifications required for customer models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-convert-your-graph-data-into-required-format">Step 1: Convert your graph data into required format</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-modify-your-gnn-model-to-use-the-graphstorm-apis">Step 2: Modify your GNN model to use the GraphStorm APIs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-modify-the-training-inference-flow-with-the-graphstorm-apis">Step 3. Modify the training/inference flow with the GraphStorm APIs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#start-training-process-with-graphstorm-s-iniatilization">Start training process with GraphStorm’s iniatilization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#replace-dgl-dataloader-with-the-graphstorm-s-dataset-and-dataloader">Replace DGL DataLoader with the GraphStorm’s dataset and dataloader</a></li>
<li class="toctree-l4"><a class="reference internal" href="#use-graphstorm-s-model-trainer-to-wrap-your-model-and-attach-evaluator-and-task-tracker-to-it">Use GraphStorm’s model trainer to wrap your model and attach evaluator and task tracker to it</a></li>
<li class="toctree-l4"><a class="reference internal" href="#use-trainer-s-fit-function-to-run-training">Use trainer’s <code class="docutils literal notranslate"><span class="pre">fit()</span></code> function to run training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-handle-the-unused-weights-error">Step 4. Handle the unused weights error</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-5-add-a-few-additional-arguments-for-the-python-main-function">Step 5. Add a few additional arguments for the Python main function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-6-setup-graphstorm-configuration-yaml-file">Step 6. Setup GraphStorm configuration YAML file</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#put-everything-together-and-run-them">Put Everything Together and Run them</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="language-models.html">Use Text as Node Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced-usages.html">GraphStorm Advanced Usages</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced-usages.html#multiple-target-node-types-training">Multiple Target Node Types Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.html">graphstorm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.dataloading.html">graphstorm.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.eval.html">graphstorm.eval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.inference.html">graphstorm.inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.model.html">graphstorm.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/graphstorm.trainer.html">graphstorm.trainer</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">GraphStorm</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Use Your Own Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/advanced/own-models.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="use-your-own-models">
<span id="use-own-models"></span><h1>Use Your Own Models<a class="headerlink" href="#use-your-own-models" title="Permalink to this heading"></a></h1>
<p>Currently GraphStorm has two built-in GNN models, i.e., the RGCN and the RGAT model. If users want to further explore different GNN models and leverage the GraphStorm’s ease-of-use and scalability, you can create your own GNN models according to the GraphStorm’s customer model APIs. This tutorial will explain in detail how to do this with a runnable <a class="reference external" href="https://github.com/awslabs/graphstorm/tree/main/examples/customized_models/HGT">example</a> that customizes the HGT model implementation.</p>
<section id="prerequisites">
<span id="use-own-models-prerequisites"></span><h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading"></a></h2>
<p>Before following GraphStorm’s customized model APIs, please make sure your GNN models meet the prerequisites.</p>
<section id="use-dgl-to-implement-your-gnn-models">
<span id="use-own-models-prerequisites-1"></span><h3>Use DGL to implement your GNN models<a class="headerlink" href="#use-dgl-to-implement-your-gnn-models" title="Permalink to this heading"></a></h3>
<p>The GraphStorm Framework relies on the <a class="reference external" href="https://www.dgl.ai/">DGL library</a> to implement and run GNN models. Particularly, the GraphStorm’s scalability comes from the DGL’s distributed libraries. For this reason, your GNN models should be implemented with the DGL Library. You can learn how to do this via the DGL’s <a class="reference external" href="https://docs.dgl.ai/guide/index.html">User Guide</a>. In addition, there are many <a class="reference external" href="https://github.com/dmlc/dgl/tree/master/examples">GNN model examples</a> implemented by the DGL community. Please explore these materials to check if there is any model that may meet your requirements.</p>
</section>
<section id="modify-you-gnn-models-to-use-mini-batch-training-inference">
<span id="use-own-models-prerequisites-2"></span><h3>Modify you GNN models to use mini-batch training/inference<a class="headerlink" href="#modify-you-gnn-models-to-use-mini-batch-training-inference" title="Permalink to this heading"></a></h3>
<p>Many existing GNN models were implemented for running on popular academic graphs, which, compared to enterprise-level graphs, are relatively small and lack node/edge features. Therefore, implementors use the full-graph training/inference mode, i.e., feed the entire graph along with its node/edge features into GNN models in one epoch. When dealing with large graphs, this mode will fail due to either the limits of the GPUs’ memory, or the slow speed if using CPUs.</p>
<p>In order to tackle large graphs, we can change GNN models to perform stochastic mini-batch training. You can learn how to modify GNN models into mini-batch training/inference mode via the <a class="reference external" href="https://docs.dgl.ai/en/1.0.x/guide/minibatch.html">DGL User Guide Chapter 6</a>. For examples of the different implementations between full-graph mode and mini-batch mode, please look for DGL model examples, in which mini-batch mode files normally have a file name ended with <cite>_mb`</cite> string, like the <a class="reference external" href="https://github.com/dmlc/dgl/blob/master/examples/pytorch/rgcn-hetero/entity_classify_mb.py">RGCN model</a>, or file names including <a class="reference external" href="https://github.com/dmlc/dgl/blob/master/examples/pytorch/graphsage/dist/train_dist.py#L26">dist string, like the `GraphSage distributed model</a>.</p>
</section>
<section id="learn-how-to-run-graphstorm-in-a-docker-environment">
<span id="use-own-models-prerequisites-3"></span><h3>Learn how to run GraphStorm in a Docker environment<a class="headerlink" href="#learn-how-to-run-graphstorm-in-a-docker-environment" title="Permalink to this heading"></a></h3>
<p>Currently GraphStorm runs on Docker environment. The rest of the tutorial assumes execution within the GraphStorm Docker container. Please refer to the first two sections in the <a class="reference internal" href="../install/env-setup.html#setup"><span class="std std-ref">Environment Setup</span></a> to learn how to run GraphStorm in a Docker environment, and set up your environment.</p>
</section>
</section>
<section id="modifications-required-for-customer-models">
<h2>Modifications required for customer models<a class="headerlink" href="#modifications-required-for-customer-models" title="Permalink to this heading"></a></h2>
<section id="step-1-convert-your-graph-data-into-required-format">
<span id="step-1"></span><h3>Step 1: Convert your graph data into required format<a class="headerlink" href="#step-1-convert-your-graph-data-into-required-format" title="Permalink to this heading"></a></h3>
<p>Users can follow the <a class="reference internal" href="../tutorials/own-data.html#use-own-data"><span class="std std-ref">User Your Own Graph Data</span></a> tutorial to prepare your graph data for GraphStorm.</p>
</section>
<section id="step-2-modify-your-gnn-model-to-use-the-graphstorm-apis">
<span id="step-2"></span><h3>Step 2: Modify your GNN model to use the GraphStorm APIs<a class="headerlink" href="#step-2-modify-your-gnn-model-to-use-the-graphstorm-apis" title="Permalink to this heading"></a></h3>
<p>To plug your GNN models into GraphStorm, you need to use the GraphStorm model APIs. The key model APIs are the class <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/model/node_gnn.py#L76">GSgnnNodeModelBase</a>, <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/model/edge_gnn.py#L80">GSgnnEdgeModelBase</a>, and <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/model/lp_gnn.py#L58">GSgnnLinkPredictionModelBase</a>. Your GNN models should inherit one of the three classes depending on your task.</p>
<p>Here we use the <a class="reference external" href="https://github.com/dmlc/dgl/blob/master/examples/pytorch/hgt/model.py">DGL HGT example model</a> to demonstrate how to modify the GNN models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">HGT</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">......</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HTG</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="o">......</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">out_key</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">ntype</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">ntypes</span><span class="p">:</span>
            <span class="n">n_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_dict</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span>
            <span class="n">h</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adapt_ws</span><span class="p">[</span><span class="n">n_id</span><span class="p">](</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;inp&quot;</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gcs</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">G</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">out_key</span><span class="p">])</span>
</pre></div>
</div>
<p>The original HGT model implement uses full-graph training and inference mode. Its <code class="docutils literal notranslate"><span class="pre">forward()</span></code> function takes a DGL graph, <code class="docutils literal notranslate"><span class="pre">G</span></code>, and the to-be predicted node type, <code class="docutils literal notranslate"><span class="pre">out_key</span></code>, as input arguments.</p>
<p>As the <a class="reference internal" href="#use-own-models-prerequisites-2"><span class="std std-ref">Prerequisites</span></a> required, we first revise this model to use mini-batch training and inference mode as shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">HGT_mb</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">......</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HGT_mb</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="o">......</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">n_feats_dict</span><span class="p">,</span> <span class="n">out_ntype</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">ntype</span> <span class="ow">in</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ntypes</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapt_ws</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">n_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_dict</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span>
                <span class="n">emb_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntype_id_map</span><span class="p">[</span><span class="n">n_id</span><span class="p">]</span>
                <span class="n">n_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntype_embed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">emb_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(</span><span class="n">ntype</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">n_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapt_ws</span><span class="p">[</span><span class="n">ntype</span><span class="p">](</span><span class="n">n_feats_dict</span><span class="p">[</span><span class="n">ntype</span><span class="p">])</span>
            <span class="n">h</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">n_embed</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gcs</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">h</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">out_ntype</span><span class="p">])</span>
</pre></div>
</div>
<p>The new <code class="docutils literal notranslate"><span class="pre">HGT_mb</span></code> model’s <code class="docutils literal notranslate"><span class="pre">forward()</span></code> function takes mini-batch blocks, <code class="docutils literal notranslate"><span class="pre">blocks</span></code>, and their corresponding node feature dictionary, <code class="docutils literal notranslate"><span class="pre">n_feats_dict</span></code>, as inputs to replace the original full graph data, <code class="docutils literal notranslate"><span class="pre">G</span></code>.</p>
<p>Then to further make this <code class="docutils literal notranslate"><span class="pre">HGT_mb</span></code> model work in GraphStorm, we need replace the PyTorch <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> with GraphStorm’s <code class="docutils literal notranslate"><span class="pre">GSgnnNodeModelBase</span></code> and implement required functions.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">GSgnnNodeModelBase</span></code> class, which is also a PyTorch Module extension, has three required functions that users’ own GNN model need to implement, including <code class="docutils literal notranslate"><span class="pre">forward(self,</span> <span class="pre">blocks,</span> <span class="pre">node_feats,</span> <span class="pre">edge_feats,</span> <span class="pre">labels,</span> <span class="pre">input_nodes)</span></code>, <code class="docutils literal notranslate"><span class="pre">predict(self,</span> <span class="pre">blocks,</span> <span class="pre">node_feats,</span> <span class="pre">edge_feats,</span> <span class="pre">input_nodes)</span></code>, and <code class="docutils literal notranslate"><span class="pre">create_optimizer(self)</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">GSgnnNodeModelBase</span></code> class’ <code class="docutils literal notranslate"><span class="pre">forward()</span></code> function is similar to the PyTorch Module’s <code class="docutils literal notranslate"><span class="pre">forward()</span></code> function except that its input arguments <strong>MUST</strong> include:</p>
<ul class="simple">
<li><p><strong>blocks</strong>, which are DGL blocks sampled for a mini-batch.</p></li>
<li><p><strong>labels</strong>, which is a dictionary, whose key is the to-be predicted node type, and value is the labels of the to-be predicted nodes in a mini-batch.</p></li>
<li><p><strong>node_feats</strong>, which is a dictionary, whose keys are node types in the graph, and values are the node features associated to.</p></li>
<li><p><strong>edge_feats</strong>. Currently GraphStorm does <strong>NOT</strong> support edge features. So, leave as it is.</p></li>
<li><p><strong>input_nodes</strong>, optional only if your GNN model needs them.</p></li>
</ul>
<p>Unlike common cases where forward function returns logits computed by models, the return value of <code class="docutils literal notranslate"><span class="pre">forward()</span></code> should be a loss value, which GraphStorm will use to perform backward operations. Because of this change, you need to include a loss function within your GNN models, instead of computing loss outside. Following these requirements, our revised model will have a few more lines added as shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">HGT</span><span class="p">(</span><span class="n">gsmodel</span><span class="o">.</span><span class="n">GSgnnNodeModelBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">......</span><span class="p">)</span>

    <span class="c1"># use GraphStorm loss function components</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">gsmodel</span><span class="o">.</span><span class="n">ClassifyLossFunc</span><span class="p">(</span><span class="n">multilabel</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">,</span> <span class="n">edge_feats</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">input_nodes</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">ntype</span> <span class="ow">in</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ntypes</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapt_ws</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_dict</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span>
            <span class="n">emb_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntype_id_map</span><span class="p">[</span><span class="n">n_id</span><span class="p">]</span>
            <span class="n">embeding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntype_embed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">emb_id</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
            <span class="n">n_embed</span> <span class="o">=</span> <span class="n">embeding</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(</span><span class="n">ntype</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapt_ws</span><span class="p">[</span><span class="n">ntype</span><span class="p">](</span><span class="n">node_feats</span><span class="p">[</span><span class="n">ntype</span><span class="p">])</span>
        <span class="n">h</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">n_embed</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gcs</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">h</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ntype</span><span class="p">,</span> <span class="n">emb</span> <span class="ow">in</span> <span class="n">h</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">h</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
    <span class="n">pred_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">target_ntype</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">target_ntype</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">pred_loss</span>
</pre></div>
</div>
<p>You may notice that GraphStorm already provides common loss functions for classification, regression and link prediction, which can be easily imported and used in your model. But you are free to use any PyTorch loss functions or even your own loss function. In the above example, we also change the to-be predicted node type as a class variable, and use it for computing the loss value.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">predict()</span></code> function is for inference and it will not be used for backward. Its input arguments are similar to the forward() function, but no need for labels. The <code class="docutils literal notranslate"><span class="pre">predict()</span></code> will return two values. The first is the prediction results. The second will be the probability if the argument <code class="docutils literal notranslate"><span class="pre">return_proba</span></code> is True, otherwise will return the raw logits, which could be used for some specific purposes. With these requirements, the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> function of the modified HGT model is like the code below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">return_proba</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">ntype</span> <span class="ow">in</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ntypes</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapt_ws</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_dict</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span>
            <span class="n">emb_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntype_id_map</span><span class="p">[</span><span class="n">n_id</span><span class="p">]</span>
            <span class="n">embeding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntype_embed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">emb_id</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
            <span class="n">n_embed</span> <span class="o">=</span> <span class="n">embeding</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(</span><span class="n">ntype</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapt_ws</span><span class="p">[</span><span class="n">ntype</span><span class="p">](</span><span class="n">node_feats</span><span class="p">[</span><span class="n">ntype</span><span class="p">])</span>
        <span class="n">h</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">n_embed</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gcs</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">h</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ntype</span><span class="p">,</span> <span class="n">emb</span> <span class="ow">in</span> <span class="n">h</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">h</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_proba</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">h</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">target_ntype</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">target_ntype</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">h</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">target_ntype</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">h</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">target_ntype</span><span class="p">]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">create_optimizer()</span></code> function is for users to define their own optimizer, like the code below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
<p>There are other two required functions in the <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/model/node_gnn.py#L76">GSgnnNodeModelBase</a> class, including <code class="docutils literal notranslate"><span class="pre">restore_model(self,</span> <span class="pre">restore_model_path)</span></code> and <code class="docutils literal notranslate"><span class="pre">save_model(self,</span> <span class="pre">model_path)</span></code>, which are used to restore and save models. If you want to save or restore models, implement these two functions too. If not, you can just leave it unimplemented as the below code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">restore_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">restore_model_path</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">):</span>
    <span class="k">pass</span>
</pre></div>
</div>
</section>
<section id="step-3-modify-the-training-inference-flow-with-the-graphstorm-apis">
<h3>Step 3. Modify the training/inference flow with the GraphStorm APIs<a class="headerlink" href="#step-3-modify-the-training-inference-flow-with-the-graphstorm-apis" title="Permalink to this heading"></a></h3>
<p>With the modified GNN models ready, the next step is to modify the training/inference loop by replacing datasets and dataloaders with the GraphStorm’s dataloading classes.</p>
<p>The original HGT_mb model uses the <a class="reference external" href="https://docs.dgl.ai/guide/minibatch-node.html#guide-minibatch-node-classification-sampler">DGL Stochastic Trainingon Large Graph Guide</a> method for the training/infernece flow. GraphStorm training/inference flow is similar with a few modifications.</p>
<section id="start-training-process-with-graphstorm-s-iniatilization">
<h4>Start training process with GraphStorm’s iniatilization<a class="headerlink" href="#start-training-process-with-graphstorm-s-iniatilization" title="Permalink to this heading"></a></h4>
<p>Any GraphStorm training process <strong>MUST</strong> start with a proper initialization. You can use the following codes at the beginning of training flow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">graphstorm</span> <span class="k">as</span> <span class="nn">gs</span>
<span class="o">......</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="n">gs</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">ip_config</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ip_config</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;gloo&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>the <code class="docutils literal notranslate"><span class="pre">ip_config</span></code> argument specifies a ip configuration file, which contains the IP addresses of machines in a GraphStorm distributed cluster. You can find its description at the <a class="reference internal" href="../tutorials/quick-start.html#launch-training"><span class="std std-ref">Launch Training</span></a> section of the <a class="reference internal" href="../tutorials/quick-start.html#quick-start-standalone"><span class="std std-ref">Quick Start Tutorial</span></a>.</p>
</section>
<section id="replace-dgl-dataloader-with-the-graphstorm-s-dataset-and-dataloader">
<h4>Replace DGL DataLoader with the GraphStorm’s dataset and dataloader<a class="headerlink" href="#replace-dgl-dataloader-with-the-graphstorm-s-dataset-and-dataloader" title="Permalink to this heading"></a></h4>
<p>Because the GraphStorm uses distributed graphs, we need to first load the partitioned graph, which is created in the <a class="reference internal" href="#step-1"><span class="std std-ref">Step 1</span></a>, with the <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/dataloading/dataset.py#L469">GSgnnNodeTrainData</a> class (for edge tasks, GraphStorm also provides <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/dataloading/dataset.py#L216">GSgnnEdgeTrainData</a>). The <code class="docutils literal notranslate"><span class="pre">GSgnnNodeTrainData</span></code> could be created as shown in the code below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">GSgnnNodeTrainData</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">graph_name</span><span class="p">,</span>
                                <span class="n">config</span><span class="o">.</span><span class="n">part_config</span><span class="p">,</span>
                                <span class="n">train_ntypes</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">target_ntype</span><span class="p">,</span>
                                <span class="n">node_feat_field</span><span class="o">=</span><span class="n">node_feat_fields</span><span class="p">,</span>
                                <span class="n">label_field</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">label_field</span><span class="p">)</span>
</pre></div>
</div>
<p>Arguments of this class include the partition configuration JSON file path, which are the outputs of the <a class="reference internal" href="#step-1"><span class="std std-ref">Step 1</span></a>. The <code class="docutils literal notranslate"><span class="pre">graph_name</span></code> can be found in the JSON file.</p>
<p>The other values, the <code class="docutils literal notranslate"><span class="pre">train_ntypes</span></code>, the <code class="docutils literal notranslate"><span class="pre">label_field</span></code>, and the <code class="docutils literal notranslate"><span class="pre">node_feat_field</span></code>, should be consistent with the values in the raw data <a class="reference internal" href="../tutorials/own-data.html#input-config"><span class="std std-ref">input configuration JSON</span></a> defined in the <a class="reference internal" href="#step-1"><span class="std std-ref">Step 1</span></a>. The <code class="docutils literal notranslate"><span class="pre">train_ntypes</span></code> is the <code class="docutils literal notranslate"><span class="pre">node_type</span></code> that has <code class="docutils literal notranslate"><span class="pre">labels</span></code> specified. The <code class="docutils literal notranslate"><span class="pre">label_fields</span></code> is the value specified in <code class="docutils literal notranslate"><span class="pre">label_col</span></code> of the <code class="docutils literal notranslate"><span class="pre">train_ntype</span></code>. The <code class="docutils literal notranslate"><span class="pre">node_feat_field</span></code> is a dictionary, whose key is the values of <code class="docutils literal notranslate"><span class="pre">node_type</span></code>, and value is the values of <code class="docutils literal notranslate"><span class="pre">feature_name</span></code>.</p>
<p>Then we can put this dataset into GraphStorm’s <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/dataloading/dataloading.py#L544">GSgnnNodeDataLoader</a>, which is like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the GraphStorm train dataloader</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">GSgnnNodeDataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_data</span><span class="o">.</span><span class="n">train_idxs</span><span class="p">,</span> <span class="n">fanout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">fanout</span><span class="p">,</span>
                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">train_task</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Optional: Define the evaluation dataloader</span>
<span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">GSgnnNodeDataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_data</span><span class="o">.</span><span class="n">val_idxs</span><span class="p">,</span><span class="n">fanout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">fanout</span><span class="p">,</span>
                                      <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                      <span class="n">train_task</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Optional: Define the evaluation dataloader</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">GSgnnNodeDataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_data</span><span class="o">.</span><span class="n">test_idxs</span><span class="p">,</span><span class="n">fanout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">fanout</span><span class="p">,</span>
                                      <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                      <span class="n">train_task</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>GraphStorm provides a set of dataloaders for different GML tasks. Here we deal with a node task, hence using the node dataloader, which takes the graph data created above as the first argument. The second argument is the label index that the GraphStorm dataset extracts from the graph as indicated in the target nodes’ <code class="docutils literal notranslate"><span class="pre">train_mask</span></code>, <code class="docutils literal notranslate"><span class="pre">val_mask</span></code>, and <code class="docutils literal notranslate"><span class="pre">test_mask</span></code>, which are automatically generated by GraphStorm graph construction tool with the specified <code class="docutils literal notranslate"><span class="pre">split_pct</span></code> field. The <code class="docutils literal notranslate"><span class="pre">GSgnnNodeTrainData</span></code> automatically extracts these indexes out and set its properties so that you can directly use them like <code class="docutils literal notranslate"><span class="pre">graph_data.train_idxs</span></code> and <code class="docutils literal notranslate"><span class="pre">graph_data.val_idxs</span></code>, and <code class="docutils literal notranslate"><span class="pre">graph_data.test_idxs</span></code>. The rest of arguments are similar to the common training flow, except that we set the <code class="docutils literal notranslate"><span class="pre">train_task</span></code> to be <code class="docutils literal notranslate"><span class="pre">False</span></code> for the evaluation and test dataloader.</p>
</section>
<section id="use-graphstorm-s-model-trainer-to-wrap-your-model-and-attach-evaluator-and-task-tracker-to-it">
<h4>Use GraphStorm’s model trainer to wrap your model and attach evaluator and task tracker to it<a class="headerlink" href="#use-graphstorm-s-model-trainer-to-wrap-your-model-and-attach-evaluator-and-task-tracker-to-it" title="Permalink to this heading"></a></h4>
<p>Unlike the common flow, GraphStorm wraps GNN models with different trainers just like other frameworks, e.g. scikit-learn. GraphStorm provides node prediction, edge prediction, and link prediction trainers. Creation of them is easy.</p>
<p>First we create the modified HGT model like the following code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the HGT model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HGT</span><span class="p">(</span><span class="n">node_dict</span><span class="p">,</span> <span class="n">edge_dict</span><span class="p">,</span>
            <span class="n">n_inp_dict</span><span class="o">=</span><span class="n">nfeat_dims</span><span class="p">,</span>
            <span class="n">n_hid</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">n_out</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">target_ntype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">target_ntype</span><span class="p">,</span>
            <span class="n">use_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">alpha_l2norm</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">alpha_l2norm</span><span class="p">)</span>
</pre></div>
</div>
<p>Then we can use the <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/trainer/np_trainer.py#L29">GSgnnNodePredictionTrainer</a> class to wrap it like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a trainer for the node classification task.</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">GSgnnNodePredictionTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">GSgnnNodePredictionTrainer</span></code> takes a GraphStorm model as the first argument. The seconde argument is for using different GPUs.</p>
<p>The GraphStorm trainers can have evaluators and task trackers associated. The following code shows how to do this.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Optional: set up a evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">GSgnnAccEvaluator</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span><span class="p">,</span>
                              <span class="n">config</span><span class="o">.</span><span class="n">eval_metric</span><span class="p">,</span>
                              <span class="n">config</span><span class="o">.</span><span class="n">multilabel</span><span class="p">,</span>
                              <span class="n">config</span><span class="o">.</span><span class="n">use_early_stop</span><span class="p">,</span>
                              <span class="n">config</span><span class="o">.</span><span class="n">early_stop_burnin_rounds</span><span class="p">,</span>
                              <span class="n">config</span><span class="o">.</span><span class="n">early_stop_rounds</span><span class="p">,</span>
                              <span class="n">config</span><span class="o">.</span><span class="n">early_stop_strategy</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">setup_evaluator</span><span class="p">(</span><span class="n">evaluator</span><span class="p">)</span>
<span class="c1"># Optional: set up a task tracker to show the progress of training.</span>
<span class="n">tracker</span> <span class="o">=</span> <span class="n">GSSageMakerTaskTracker</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">setup_task_tracker</span><span class="p">(</span><span class="n">tracker</span><span class="p">)</span>
</pre></div>
</div>
<p>GraphStorm’s <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/eval/evaluator.py">evaluators</a> could help to compute the required evaluation metrics, such as <code class="docutils literal notranslate"><span class="pre">accuracy</span></code>, <code class="docutils literal notranslate"><span class="pre">f1</span></code>, <code class="docutils literal notranslate"><span class="pre">mrr</span></code>, and etc. Users can select the proper evaluator and use the trainer’s <code class="docutils literal notranslate"><span class="pre">setup_evaluator()</span></code> method to attach them. GraphStorm’s <a class="reference external" href="https://github.com/awslabs/graphstorm/blob/main/python/graphstorm/tracker/graphstorm_tracker.py">task trackers</a> serve as log collectors, which are used to show the process information.</p>
</section>
<section id="use-trainer-s-fit-function-to-run-training">
<h4>Use trainer’s <code class="docutils literal notranslate"><span class="pre">fit()</span></code> function to run training<a class="headerlink" href="#use-trainer-s-fit-function-to-run-training" title="Permalink to this heading"></a></h4>
<p>Once all trainers, evaluators, and task trackers are set, the last step is to use the trainer’s <code class="docutils literal notranslate"><span class="pre">fit()</span></code> function to run training, validating, and testing on the three sets like the code below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start the training process.</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_loader</span><span class="o">=</span><span class="n">dataloader</span><span class="p">,</span>
            <span class="n">num_epochs</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">,</span>
            <span class="n">val_loader</span><span class="o">=</span><span class="n">eval_dataloader</span><span class="p">,</span>
            <span class="n">test_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
            <span class="n">save_model_path</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">save_model_path</span><span class="p">,</span>
            <span class="n">use_mini_batch_infer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">fit()</span></code> function wraps dataloaders, number of epochs, to replace the common “<strong>for loops</strong>” as seen in the common training flow. The <code class="docutils literal notranslate"><span class="pre">fit()</span></code> function also takes additional arguments, such as <code class="docutils literal notranslate"><span class="pre">save_model_path</span></code> to save different model artifacts. <strong>BUT</strong> before set these arguments, you need to implement the <code class="docutils literal notranslate"><span class="pre">restore_model(self,</span> <span class="pre">restore_model_path)</span></code> and <code class="docutils literal notranslate"><span class="pre">save_model(self,</span> <span class="pre">model_path)</span></code> functions in the <a class="reference internal" href="#step-2"><span class="std std-ref">Step 2</span></a>.</p>
</section>
</section>
<section id="step-4-handle-the-unused-weights-error">
<h3>Step 4. Handle the unused weights error<a class="headerlink" href="#step-4-handle-the-unused-weights-error" title="Permalink to this heading"></a></h3>
<p>Uncommonly seen in the full-graph training or mini-batch training on a single GPU, the unused weights error could frequently occur when we start to train models on multiple GPUs in parallel. PyTorch distributed framework’s inner mechanism causes this problem. One easy way to solve this error is to add a regularization to all trainable parameters into the loss computation like the code blow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pred_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">target_ntype</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">target_ntype</span><span class="p">])</span>

<span class="n">reg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">pred_loss</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># L2 regularization of dense parameters</span>
<span class="k">for</span> <span class="n">d_para</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">reg_loss</span> <span class="o">+=</span> <span class="n">d_para</span><span class="o">.</span><span class="n">square</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">reg_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_l2norm</span> <span class="o">*</span> <span class="n">reg_loss</span>

<span class="k">return</span> <span class="n">pred_loss</span> <span class="o">+</span> <span class="n">reg_loss</span>
</pre></div>
</div>
<p>You can add a coefficient, like the <code class="docutils literal notranslate"><span class="pre">alpha_l2norm</span></code>, to control the influence of the regularization.</p>
</section>
<section id="step-5-add-a-few-additional-arguments-for-the-python-main-function">
<h3>Step 5. Add a few additional arguments for the Python main function<a class="headerlink" href="#step-5-add-a-few-additional-arguments-for-the-python-main-function" title="Permalink to this heading"></a></h3>
<p>Because GraphStorm relys on a few arguments to launch training and inference command, including: <code class="docutils literal notranslate"><span class="pre">part-config</span></code>, <code class="docutils literal notranslate"><span class="pre">ip-config</span></code>, <code class="docutils literal notranslate"><span class="pre">verbose</span></code>, and <code class="docutils literal notranslate"><span class="pre">local_rank</span></code>. GraphStorm’s built-in launch scripts have this argument configured already. But for customized models, it is required to add them as arguments of the Python main function although these arguments are not used anywhere in the customized model. A sample code is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">argparser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="s2">&quot;Training HGT model with the GraphStorm Framework&quot;</span><span class="p">)</span>
    <span class="o">......</span>
    <span class="n">argparser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--part-config&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The partition config file. </span><span class="se">\</span>
<span class="s2">                                For customized models, MUST have this argument!!&quot;</span><span class="p">)</span>
    <span class="n">argparser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--ip-config&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The IP config file for the cluster. </span><span class="se">\</span>
<span class="s2">                                For customized models, MUST have this argument!!&quot;</span><span class="p">)</span>
    <span class="n">argparser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--verbose&quot;</span><span class="p">,</span>
                        <span class="nb">type</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;true&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">]),</span>
                        <span class="n">default</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">SUPPRESS</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Print more information. </span><span class="se">\</span>
<span class="s2">                                For customized models, MUST have this argument!!&quot;</span><span class="p">)</span>
    <span class="n">argparser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--local_rank&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The rank of the trainer. </span><span class="se">\</span>
<span class="s2">                                For customized models, MUST have this argument!!&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>PyTorch v2.0 change the argument <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> to <code class="docutils literal notranslate"><span class="pre">local-rank</span></code>. Therefore, if users use PyTorch v2.0 or later version, please change this argument accordingly.</p>
</div>
</section>
<section id="step-6-setup-graphstorm-configuration-yaml-file">
<h3>Step 6. Setup GraphStorm configuration YAML file<a class="headerlink" href="#step-6-setup-graphstorm-configuration-yaml-file" title="Permalink to this heading"></a></h3>
<p>GraphStorm has a set of parameters that control the various perspectives of the model training and inference process. You can find the details of these parameters in the GraphStorm <a class="reference internal" href="../configuration/configuration-run.html#configurations-run"><span class="std std-ref">Training and Inference Configurations</span></a>. These parameters could be either passed as input arguments or set in a YAML format file. Below is an example of the YAML file.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="nt">gsf</span><span class="p">:</span>
<span class="nt">basic</span><span class="p">:</span>
<span class="w">    </span><span class="nt">model_encoder_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rgcn</span>
<span class="w">    </span><span class="nt">backend</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gloo</span>
<span class="w">    </span><span class="nt">verbose</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">alpha_l2norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.</span>
<span class="nt">gnn</span><span class="p">:</span>
<span class="w">    </span><span class="nt">fanout</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;50,50&quot;</span>
<span class="w">    </span><span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">    </span><span class="nt">hidden_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="w">    </span><span class="nt">use_mini_batch_infer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">input</span><span class="p">:</span>
<span class="w">    </span><span class="nt">restore_model_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">output</span><span class="p">:</span>
<span class="w">    </span><span class="nt">topk_model_to_save</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">7</span>
<span class="w">    </span><span class="nt">save_model_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/data/outputs</span>
<span class="w">    </span><span class="nt">save_embeds_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/data/outputs</span>
<span class="w">    </span><span class="nt">save_prediction_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/data/outputs</span>
<span class="nt">hyperparam</span><span class="p">:</span>
<span class="w">    </span><span class="nt">dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.</span>
<span class="w">    </span><span class="nt">lr</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0001</span>
<span class="w">    </span><span class="nt">num_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20</span>
<span class="w">    </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1024</span>
<span class="w">    </span><span class="nt">wd_l2norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">rgcn</span><span class="p">:</span>
<span class="w">    </span><span class="nt">num_bases</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span>
<span class="w">    </span><span class="nt">use_self_loop</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">sparse_optimizer_lr</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-2</span>
<span class="w">    </span><span class="nt">use_node_embeddings</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">node_classification</span><span class="p">:</span>
<span class="w">    </span><span class="nt">target_ntype</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;paper&quot;</span>
<span class="w">    </span><span class="nt">label_field</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;label&quot;</span>
<span class="w">    </span><span class="nt">multilabel</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">num_classes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">14</span>
</pre></div>
</div>
<p>Users can use an argument to read in this YAML file, and construct a <code class="docutils literal notranslate"><span class="pre">GSConfig</span></code> object like the below code. And then use the GSConfig instance, e.g., <code class="docutils literal notranslate"><span class="pre">config</span></code>, to provide arguments that the GraphStorm supports.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphstorm.config</span> <span class="kn">import</span> <span class="n">GSConfig</span>
<span class="o">......</span>
<span class="n">argparser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--yaml-config-file&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The GraphStorm YAML configuration file path.&quot;</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">argparser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">GSConfig</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
<p>For users’ own configurations, you still can pass them as input argument of the training script, and extract them from the <code class="docutils literal notranslate"><span class="pre">args</span></code> object.</p>
</section>
</section>
<section id="put-everything-together-and-run-them">
<h2>Put Everything Together and Run them<a class="headerlink" href="#put-everything-together-and-run-them" title="Permalink to this heading"></a></h2>
<p>With all required modifications ready, let’s put everything of the modified HGT model together in a Python file, e.g, <code class="docutils literal notranslate"><span class="pre">hgt_nc.py</span></code>. We can put the Python file and the related artifacts, including the YAML file, e.g., <code class="docutils literal notranslate"><span class="pre">acm_nc.yaml</span></code>, and the <code class="docutils literal notranslate"><span class="pre">ip_list.txt</span></code> file in a folder, e.g. <code class="docutils literal notranslate"><span class="pre">/hgt_nc/</span></code>. And then use the GraphStorm’s launch script to run this modified HGT model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">graphstorm</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">launch</span> \
        <span class="o">--</span><span class="n">workspace</span> <span class="o">/</span><span class="n">graphstorm</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">customized_models</span><span class="o">/</span><span class="n">HGT</span> \
        <span class="o">--</span><span class="n">part</span><span class="o">-</span><span class="n">config</span> <span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">acm_nc</span><span class="o">/</span><span class="n">acm</span><span class="o">.</span><span class="n">json</span> \
        <span class="o">--</span><span class="n">ip</span><span class="o">-</span><span class="n">config</span> <span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">ip_list</span><span class="o">.</span><span class="n">txt</span> \
        <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">trainers</span> <span class="mi">2</span> \
        <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">servers</span> <span class="mi">1</span> \
        <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">samplers</span> <span class="mi">0</span> \
        <span class="o">--</span><span class="n">ssh</span><span class="o">-</span><span class="n">port</span> <span class="mi">2222</span> \
        <span class="n">hgt_nc</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">yaml</span><span class="o">-</span><span class="n">config</span><span class="o">-</span><span class="n">file</span> <span class="n">acm_nc</span><span class="o">.</span><span class="n">yaml</span> \
                  <span class="o">--</span><span class="n">node</span><span class="o">-</span><span class="n">feat</span> <span class="n">paper</span><span class="p">:</span><span class="n">feat</span><span class="o">-</span><span class="n">author</span><span class="p">:</span><span class="n">feat</span><span class="o">-</span><span class="n">subject</span><span class="p">:</span><span class="n">feat</span> \
                  <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">heads</span> <span class="mi">8</span>
</pre></div>
</div>
<p>The argument value of <code class="docutils literal notranslate"><span class="pre">--part-config</span></code> is the JSON file coming from the <a class="reference internal" href="../tutorials/own-data.html#output-graph-construction"><span class="std std-ref">outputs</span></a> of the <a class="reference internal" href="#step-1"><span class="std std-ref">Step 1</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To try this runnable example, please follow the <a class="reference external" href="https://github.com/awslabs/graphstorm/tree/main/examples/customized_models/HGT">GraphStorm examples readme</a>.</p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../scale/sagemaker.html" class="btn btn-neutral float-left" title="Use GraphStorm on SageMaker" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="language-models.html" class="btn btn-neutral float-right" title="Use Text as Node Features" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, AGML team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>